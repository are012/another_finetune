# pipeline_update.py - ë°ì´í„° ìˆ˜ì§‘ ë° ë²¡í„° DB êµ¬ì¶• (Producer)
"""
ê¸°ì—… ë¶„ì„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ - ìƒì‚°ì (Producer)

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë˜ë©°, ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:
1. ì§€ì •ëœ ê¸°ì—… ëª©ë¡ì— ëŒ€í•´ ìµœì‹  ë‰´ìŠ¤ ë° DART ê³µì‹œ ì •ë³´ ìˆ˜ì§‘
2. ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í• 
3. ë²¡í„°í™”í•˜ì—¬ ë¡œì»¬ ChromaDBì— ì €ì¥
4. íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„±ì„ ìœ„í•œ ë©”íƒ€ë°ì´í„° í¬í•¨

ì‚¬ìš©ë²•:
    python pipeline_update.py
"""

import os
import json
from datetime import datetime, timedelta
from typing import List, Dict, Any
import logging

# LangChain ê´€ë ¨ ì„í¬íŠ¸
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document

# ìš°ë¦¬ì˜ ë°ì´í„° ìˆ˜ì§‘ ëª¨ë“ˆ
import data_collector as dc
import importlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DataPipeline:
    """ë°ì´í„° ìˆ˜ì§‘ ë° ë²¡í„° DB êµ¬ì¶•ì„ ë‹´ë‹¹í•˜ëŠ” ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, google_api_key: str, db_dir: str = "rag_db"):
        """
        ì´ˆê¸°í™”
        
        Args:
            google_api_key (str): Google AI API í‚¤
            db_dir (str): ë²¡í„° DB ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.google_api_key = google_api_key
        self.db_dir = db_dir
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/embedding-001",
            google_api_key=google_api_key
        )
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™” (íŒŒì¸íŠœë‹ì— ì í•©í•œ ì²­í¬ í¬ê¸°)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # íŒŒì¸íŠœë‹ì— ì í•©í•œ í¬ê¸°
            chunk_overlap=100,  # ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ë¥¼ ìœ„í•œ ì˜¤ë²„ë©
            separators=["\n\n", "\n", ".", "!", "?", ";", ":", " ", ""]
        )
        
        # ë¶„ì„ ëŒ€ìƒ ê¸°ì—… ëª©ë¡ (ì‰½ê²Œ í™•ì¥ ê°€ëŠ¥)
        self.target_companies = [
            "ì‚¼ì„±ì „ì",
            "SKí•˜ì´ë‹‰ìŠ¤",
            "NAVER",
            "ì¹´ì¹´ì˜¤",
            "LGì—ë„ˆì§€ì†”ë£¨ì…˜"
        ]
        
        logger.info(f"ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ. DB ë””ë ‰í† ë¦¬: {self.db_dir}")
    
    def collect_company_data(self, company_name: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ê¸°ì—…ì˜ ìµœì‹  ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            company_name (str): ê¸°ì—…ëª…
            
        Returns:
            Dict[str, Any]: ìˆ˜ì§‘ëœ ë°ì´í„°ì™€ ë©”íƒ€ë°ì´í„°
        """
        logger.info(f"ğŸ“Š {company_name} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
        
        collected_data = {
            "company_name": company_name,
            "collection_timestamp": datetime.now().isoformat(),
            "news_data": [],
            "disclosure_data": [],
            "summary_stats": {}
        }
        
        try:
            # 1. ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
            logger.info(f"  ğŸ“° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            news_articles = dc.fetch_naver_news(company_name, display_count=10)
            
            for article in news_articles:
                # ë‰´ìŠ¤ ë°ì´í„°ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
                if company_name in article['title'] or company_name in article['description']:
                    collected_data["news_data"].append({
                        "title": article['title'],
                        "description": article['description'],
                        "source": "naver_news",
                        "company": company_name,
                        "collection_date": datetime.now().isoformat()
                    })
            
            logger.info(f"  âœ… ë‰´ìŠ¤ {len(collected_data['news_data'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
            
            # 2. DART ê³µì‹œ ì •ë³´ ìˆ˜ì§‘ (ì¤‘ìš”ë„ë³„ ë¶„ë¥˜)
            logger.info(f"  ğŸ¢ ê³µì‹œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            try:
                classified_disclosures = dc.get_important_disclosures_by_priority(
                    company_name, days_back=30
                )
                
                if classified_disclosures:
                    for priority, disclosures in classified_disclosures.items():
                        for disclosure in disclosures:
                            collected_data["disclosure_data"].append({
                                "report_name": disclosure.get('report_nm', ''),
                                "reception_date": disclosure.get('rcept_dt', ''),
                                "priority": priority,
                                "source": "dart_api",
                                "company": company_name,
                                "collection_date": datetime.now().isoformat()
                            })
                
                logger.info(f"  âœ… ê³µì‹œ {len(collected_data['disclosure_data'])}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                logger.warning(f"  âš ï¸ ê³µì‹œ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            
            # 3. ìˆ˜ì§‘ í†µê³„
            collected_data["summary_stats"] = {
                "total_news": len(collected_data["news_data"]),
                "total_disclosures": len(collected_data["disclosure_data"]),
                "collection_success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ {company_name} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            collected_data["summary_stats"]["collection_success"] = False
            
        return collected_data
    
    def create_documents(self, company_data: Dict[str, Any]) -> List[Document]:
        """
        ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ LangChain Document í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            company_data (Dict[str, Any]): ìˆ˜ì§‘ëœ ê¸°ì—… ë°ì´í„°
            
        Returns:
            List[Document]: LangChain Document ë¦¬ìŠ¤íŠ¸
        """
        documents = []
        company_name = company_data["company_name"]
        collection_time = company_data["collection_timestamp"]
        
        # 1. ë‰´ìŠ¤ ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜
        for news in company_data["news_data"]:
            content = f"""ì œëª©: {news['title']}
ë‚´ìš©: {news['description']}

ê¸°ì—…: {company_name}
ì¶œì²˜: ë„¤ì´ë²„ ë‰´ìŠ¤
ìˆ˜ì§‘ì¼ì‹œ: {news['collection_date']}"""
            
            metadata = {
                "company": company_name,
                "source": "news",
                "title": news['title'],
                "collection_date": collection_time,
                "data_type": "news_article"
            }
            
            documents.append(Document(page_content=content, metadata=metadata))
        
        # 2. ê³µì‹œ ë°ì´í„°ë¥¼ Documentë¡œ ë³€í™˜
        for disclosure in company_data["disclosure_data"]:
            content = f"""ê³µì‹œëª…: {disclosure['report_name']}
ì ‘ìˆ˜ì¼ì: {disclosure['reception_date']}
ì¤‘ìš”ë„: {disclosure['priority']}

ê¸°ì—…: {company_name}
ì¶œì²˜: DART ê³µì‹œì‹œìŠ¤í…œ
ìˆ˜ì§‘ì¼ì‹œ: {disclosure['collection_date']}"""
            
            metadata = {
                "company": company_name,
                "source": "disclosure",
                "priority": disclosure['priority'],
                "report_name": disclosure['report_name'],
                "collection_date": collection_time,
                "data_type": "disclosure_info"
            }
            
            documents.append(Document(page_content=content, metadata=metadata))
        
        logger.info(f"  ğŸ“„ {company_name}: {len(documents)}ê°œ ë¬¸ì„œ ìƒì„± ì™„ë£Œ")
        return documents
    
    def split_and_store_documents(self, documents: List[Document]) -> int:
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ê³  ë²¡í„° DBì— ì €ì¥
        
        Args:
            documents (List[Document]): ì €ì¥í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            int: ì €ì¥ëœ ì²­í¬ ìˆ˜
        """
        if not documents:
            return 0
        
        # ë¬¸ì„œ ë¶„í• 
        split_docs = self.text_splitter.split_documents(documents)
        logger.info(f"  âœ‚ï¸ ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(split_docs)}ê°œ ì²­í¬ ìƒì„±")
        
        # ChromaDBì— ì €ì¥ (ê¸°ì¡´ DBì— ì¶”ê°€)
        try:
            if os.path.exists(self.db_dir):
                # ê¸°ì¡´ DB ë¡œë“œí•˜ì—¬ ì¶”ê°€
                vectorstore = Chroma(
                    persist_directory=self.db_dir,
                    embedding_function=self.embeddings
                )
                vectorstore.add_documents(split_docs)
            else:
                # ìƒˆ DB ìƒì„±
                vectorstore = Chroma.from_documents(
                    split_docs,
                    embedding=self.embeddings,
                    persist_directory=self.db_dir
                )
            
            # ë³€ê²½ì‚¬í•­ ì €ì¥
            vectorstore.persist()
            logger.info(f"  ğŸ’¾ ë²¡í„° DB ì €ì¥ ì™„ë£Œ: {len(split_docs)}ê°œ ì²­í¬")
            
            return len(split_docs)
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return 0
    
    def run_pipeline(self) -> Dict[str, Any]:
        """
        ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Returns:
            Dict[str, Any]: ì‹¤í–‰ ê²°ê³¼ í†µê³„
        """
        logger.info("ğŸš€ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œì‘")
        
        pipeline_stats = {
            "start_time": datetime.now().isoformat(),
            "companies_processed": [],
            "total_documents": 0,
            "total_chunks": 0,
            "errors": []
        }
        
        # data_collector ëª¨ë“ˆ ìµœì‹  ë²„ì „ìœ¼ë¡œ ë¦¬ë¡œë“œ
        importlib.reload(dc)
        
        # ê° ê¸°ì—…ë³„ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥
        for company in self.target_companies:
            try:
                # 1. ë°ì´í„° ìˆ˜ì§‘
                company_data = self.collect_company_data(company)
                
                if company_data["summary_stats"]["collection_success"]:
                    # 2. Document ìƒì„±
                    documents = self.create_documents(company_data)
                    
                    # 3. ë¶„í•  ë° ì €ì¥
                    chunk_count = self.split_and_store_documents(documents)
                    
                    # í†µê³„ ì—…ë°ì´íŠ¸
                    pipeline_stats["companies_processed"].append({
                        "company": company,
                        "documents": len(documents),
                        "chunks": chunk_count,
                        "success": True
                    })
                    
                    pipeline_stats["total_documents"] += len(documents)
                    pipeline_stats["total_chunks"] += chunk_count
                    
                    logger.info(f"âœ… {company} ì²˜ë¦¬ ì™„ë£Œ - ë¬¸ì„œ: {len(documents)}, ì²­í¬: {chunk_count}")
                    
                else:
                    pipeline_stats["errors"].append(f"{company}: ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
                    
            except Exception as e:
                error_msg = f"{company}: {str(e)}"
                pipeline_stats["errors"].append(error_msg)
                logger.error(f"âŒ {error_msg}")
        
        pipeline_stats["end_time"] = datetime.now().isoformat()
        pipeline_stats["duration_minutes"] = (
            datetime.fromisoformat(pipeline_stats["end_time"]) - 
            datetime.fromisoformat(pipeline_stats["start_time"])
        ).total_seconds() / 60
        
        # ê²°ê³¼ ì €ì¥ (íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ê´€ë¦¬ìš©)
        self.save_pipeline_log(pipeline_stats)
        
        logger.info("ğŸ‰ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
        logger.info(f"  ì²˜ë¦¬ëœ ê¸°ì—…: {len(pipeline_stats['companies_processed'])}ê°œ")
        logger.info(f"  ì´ ë¬¸ì„œ: {pipeline_stats['total_documents']}ê°œ")
        logger.info(f"  ì´ ì²­í¬: {pipeline_stats['total_chunks']}ê°œ")
        logger.info(f"  ì†Œìš”ì‹œê°„: {pipeline_stats['duration_minutes']:.1f}ë¶„")
        
        return pipeline_stats
    
    def save_pipeline_log(self, stats: Dict[str, Any]) -> None:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¡œê·¸ ì €ì¥ (íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ì¶”ì ìš©)"""
        log_file = "pipeline_logs.json"
        
        # ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ
        if os.path.exists(log_file):
            with open(log_file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        else:
            logs = []
        
        # ìƒˆ ë¡œê·¸ ì¶”ê°€
        logs.append(stats)
        
        # ìµœê·¼ 50ê°œ ë¡œê·¸ë§Œ ìœ ì§€
        logs = logs[-50:]
        
        # ì €ì¥
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, ensure_ascii=False, indent=2)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # Google API í‚¤ ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ì§ì ‘ ì…ë ¥)
    GOOGLE_API_KEY = "AIzaSyBwcmK-DKRCI2r8xhHygSfu2GQ-oqK6t_4"  # ì‹¤ì œ í‚¤ë¡œ êµì²´ í•„ìš”
    
    if not GOOGLE_API_KEY or GOOGLE_API_KEY == "YOUR_GOOGLE_API_KEY":
        print("âŒ Google API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        print("GOOGLE_API_KEY ë³€ìˆ˜ì— ì‹¤ì œ API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        return
    
    try:
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        pipeline = DataPipeline(google_api_key=GOOGLE_API_KEY)
        results = pipeline.run_pipeline()
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*60)
        print("ğŸ“Š íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼")
        print("="*60)
        print(f"ì²˜ë¦¬ëœ ê¸°ì—… ìˆ˜: {len(results['companies_processed'])}")
        print(f"ì´ ìƒì„± ë¬¸ì„œ: {results['total_documents']}")
        print(f"ì´ ë²¡í„° ì²­í¬: {results['total_chunks']}")
        print(f"ì†Œìš” ì‹œê°„: {results['duration_minutes']:.1f}ë¶„")
        
        if results['errors']:
            print(f"\nâš ï¸ ì˜¤ë¥˜ ë°œìƒ: {len(results['errors'])}ê±´")
            for error in results['errors']:
                print(f"  - {error}")
        
        print("\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„:")
        print("  1. report_generator.ipynb ë…¸íŠ¸ë¶ ì‹¤í–‰")
        print("  2. ìƒì„±ëœ ë¦¬í¬íŠ¸ë¥¼ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ìœ¼ë¡œ í™œìš©")
        print("  3. ì •ê¸°ì ìœ¼ë¡œ ì´ ìŠ¤í¬ë¦½íŠ¸ ì¬ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ì—…ë°ì´íŠ¸")
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
