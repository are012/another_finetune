{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5097bc8",
   "metadata": {},
   "source": [
    "# 📊 완전 오프라인 RAG 리포트 생성기 (Pure Consumer)\n",
    "\n",
    "## 🎯 시스템 개요 \n",
    "이 노트북은 **100% 오프라인**으로 작동하는 Consumer입니다.\n",
    "외부 API 호출 없이 로컬 벡터 DB에서만 정보를 추출하여 리포트를 생성합니다.\n",
    "\n",
    "### 📋 핵심 특징\n",
    "- **완전 오프라인**: 외부 API 호출 전혀 없음\n",
    "- **순수 RAG**: 로컬 벡터 DB에서만 정보 검색\n",
    "- **템플릿 기반**: 규칙 기반 리포트 생성\n",
    "- **파인튜닝 친화**: 일관된 구조의 고품질 데이터셋 생성\n",
    "- **빠른 처리**: API 대기시간 없음\n",
    "\n",
    "### 🔄 Producer-Consumer 분리\n",
    "- **Producer** (`pipeline_update.py`): 데이터 수집 + API 호출 + 벡터 DB 저장\n",
    "- **Consumer** (이 노트북): 벡터 DB 검색 + 템플릿 기반 리포트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a143aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 임포트 완료 (순수 오프라인 모드)\n",
      "🚫 외부 API 의존성 없음\n"
     ]
    }
   ],
   "source": [
    "# 📦 필수 라이브러리 임포트 (API 관련 제외)\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# 벡터 DB 관련 (임베딩은 저장된 것만 사용)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 노트북 출력용\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료 (순수 오프라인 모드)\")\n",
    "print(\"🚫 외부 API 의존성 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 오프라인 설정 완료\n",
      "📁 벡터 DB 디렉토리: rag_db\n",
      "✅ 벡터 DB 발견\n"
     ]
    }
   ],
   "source": [
    "# 🔧 오프라인 설정\n",
    "DB_DIR = \"rag_db\"  # 벡터 DB 디렉토리\n",
    "GOOGLE_API_KEY = \"\"  # 임베딩 로드용만\n",
    "\n",
    "print(\"✅ 오프라인 설정 완료\")\n",
    "print(f\"📁 벡터 DB 디렉토리: {DB_DIR}\")\n",
    "\n",
    "# DB 존재 여부 확인\n",
    "if os.path.exists(DB_DIR):\n",
    "    print(\"✅ 벡터 DB 발견\")\n",
    "else:\n",
    "    print(\"❌ 벡터 DB를 찾을 수 없습니다. pipeline_update.py를 먼저 실행하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ba8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OfflineReportGenerator 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "class OfflineReportGenerator:\n",
    "    \"\"\"완전 오프라인 리포트 생성기 - 외부 API 호출 없음\"\"\"\n",
    "    \n",
    "    def __init__(self, db_dir: str = \"rag_db\", google_api_key: str = None):\n",
    "        \"\"\"\n",
    "        초기화 - 저장된 벡터 DB만 로드\n",
    "        \n",
    "        Args:\n",
    "            db_dir (str): 벡터 DB 디렉토리\n",
    "            google_api_key (str): 임베딩 로드용 (새로운 임베딩 생성 안함)\n",
    "        \"\"\"\n",
    "        self.db_dir = db_dir\n",
    "        \n",
    "        if not os.path.exists(db_dir):\n",
    "            raise FileNotFoundError(f\"벡터 DB 디렉토리를 찾을 수 없습니다: {db_dir}\")\n",
    "        \n",
    "        # 임베딩 모델 초기화 (기존 임베딩 로드용만)\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=google_api_key\n",
    "        )\n",
    "        \n",
    "        # 벡터 DB 로드 (저장된 데이터만 사용)\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 오프라인 RAG 시스템 초기화 완료\")\n",
    "        print(f\"📊 벡터 DB 컬렉션 수: {self.vectorstore._collection.count()}\")\n",
    "    \n",
    "    def get_available_companies(self) -> List[str]:\n",
    "        \"\"\"DB에서 사용 가능한 기업 목록 조회\"\"\"\n",
    "        try:\n",
    "            results = self.vectorstore.get()\n",
    "            companies = set()\n",
    "            \n",
    "            for metadata in results['metadatas']:\n",
    "                if 'company' in metadata:\n",
    "                    companies.add(metadata['company'])\n",
    "            \n",
    "            return sorted(list(companies))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 기업 목록 조회 실패: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_company_data(self, company_name: str, k: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"특정 기업의 모든 관련 데이터 검색 및 분류\"\"\"\n",
    "        try:\n",
    "            # 기업명으로 검색\n",
    "            results = self.vectorstore.similarity_search(\n",
    "                query=f\"{company_name} 뉴스 공시 분석\",\n",
    "                k=k,\n",
    "                filter={\"company\": company_name}\n",
    "            )\n",
    "            \n",
    "            # 데이터 타입별로 분류\n",
    "            classified_data = {\n",
    "                \"news\": [],\n",
    "                \"disclosures\": [],\n",
    "                \"other\": [],\n",
    "                \"total_count\": len(results)\n",
    "            }\n",
    "            \n",
    "            for doc in results:\n",
    "                source = doc.metadata.get('source', 'other')\n",
    "                if source == 'news':\n",
    "                    classified_data[\"news\"].append(doc)\n",
    "                elif source == 'disclosure':\n",
    "                    classified_data[\"disclosures\"].append(doc)\n",
    "                else:\n",
    "                    classified_data[\"other\"].append(doc)\n",
    "            \n",
    "            return classified_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {company_name} 데이터 검색 실패: {e}\")\n",
    "            return {\"news\": [], \"disclosures\": [], \"other\": [], \"total_count\": 0}\n",
    "    \n",
    "    def extract_key_insights(self, classified_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"검색된 데이터에서 핵심 인사이트 추출 (규칙 기반)\"\"\"\n",
    "        insights = {\n",
    "            \"positive_signals\": [],\n",
    "            \"negative_signals\": [],\n",
    "            \"key_events\": [],\n",
    "            \"disclosure_priorities\": defaultdict(int),\n",
    "            \"recent_news_count\": 0,\n",
    "            \"disclosure_count\": 0\n",
    "        }\n",
    "        \n",
    "        # 긍정/부정 키워드 정의\n",
    "        positive_keywords = ['증가', '상승', '성장', '확대', '투자', '개발', '협력', '계약', '수주', '매출']\n",
    "        negative_keywords = ['감소', '하락', '축소', '손실', '리스크', '우려', '취소', '지연', '문제']\n",
    "        \n",
    "        # 뉴스 분석\n",
    "        for news_doc in classified_data[\"news\"]:\n",
    "            content = news_doc.page_content.lower()\n",
    "            \n",
    "            # 긍정적 신호 탐지\n",
    "            for keyword in positive_keywords:\n",
    "                if keyword in content:\n",
    "                    insights[\"positive_signals\"].append({\n",
    "                        \"keyword\": keyword,\n",
    "                        \"source\": \"뉴스\",\n",
    "                        \"title\": news_doc.metadata.get('title', '')[:50]\n",
    "                    })\n",
    "            \n",
    "            # 부정적 신호 탐지\n",
    "            for keyword in negative_keywords:\n",
    "                if keyword in content:\n",
    "                    insights[\"negative_signals\"].append({\n",
    "                        \"keyword\": keyword,\n",
    "                        \"source\": \"뉴스\",\n",
    "                        \"title\": news_doc.metadata.get('title', '')[:50]\n",
    "                    })\n",
    "        \n",
    "        # 공시 분석\n",
    "        for disclosure_doc in classified_data[\"disclosures\"]:\n",
    "            priority = disclosure_doc.metadata.get('priority', 'unknown')\n",
    "            insights[\"disclosure_priorities\"][priority] += 1\n",
    "            \n",
    "            # 주요 공시 이벤트 추출\n",
    "            report_name = disclosure_doc.metadata.get('report_name', '')\n",
    "            if any(keyword in report_name for keyword in ['분기보고서', '사업보고서', '주요사항보고서']):\n",
    "                insights[\"key_events\"].append({\n",
    "                    \"type\": \"공시\",\n",
    "                    \"event\": report_name,\n",
    "                    \"priority\": priority\n",
    "                })\n",
    "        \n",
    "        insights[\"recent_news_count\"] = len(classified_data[\"news\"])\n",
    "        insights[\"disclosure_count\"] = len(classified_data[\"disclosures\"])\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def generate_offline_report(self, company_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"완전 오프라인 리포트 생성 (템플릿 기반)\"\"\"\n",
    "        print(f\"📊 {company_name} 오프라인 분석 시작...\")\n",
    "        \n",
    "        # 1. 데이터 검색\n",
    "        classified_data = self.search_company_data(company_name)\n",
    "        \n",
    "        if classified_data[\"total_count\"] == 0:\n",
    "            return {\n",
    "                \"company\": company_name,\n",
    "                \"status\": \"데이터 없음\",\n",
    "                \"message\": \"해당 기업의 데이터를 찾을 수 없습니다.\"\n",
    "            }\n",
    "        \n",
    "        # 2. 인사이트 추출\n",
    "        insights = self.extract_key_insights(classified_data)\n",
    "        \n",
    "        # 3. 리포트 생성\n",
    "        report = {\n",
    "            \"company\": company_name,\n",
    "            \"generation_date\": datetime.now().isoformat(),\n",
    "            \"data_summary\": {\n",
    "                \"total_documents\": classified_data[\"total_count\"],\n",
    "                \"news_articles\": len(classified_data[\"news\"]),\n",
    "                \"disclosures\": len(classified_data[\"disclosures\"])\n",
    "            },\n",
    "            \"investment_analysis\": self._create_investment_analysis(insights),\n",
    "            \"risk_assessment\": self._create_risk_assessment(insights),\n",
    "            \"recommendation\": self._create_recommendation(insights),\n",
    "            \"key_data_points\": self._extract_key_data_points(classified_data),\n",
    "            \"status\": \"완료\"\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ {company_name} 오프라인 분석 완료\")\n",
    "        return report\n",
    "    \n",
    "    def _create_investment_analysis(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"투자 분석 섹션 생성\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        news_count = insights[\"recent_news_count\"]\n",
    "        \n",
    "        analysis = f\"\"\"## 📈 투자 분석\n",
    "\n",
    "### 데이터 기반 현황\n",
    "- 최근 뉴스 분석: {news_count}건\n",
    "- 공시 정보 분석: {insights['disclosure_count']}건\n",
    "- 긍정적 신호: {positive_count}개\n",
    "- 부정적 신호: {negative_count}개\n",
    "\n",
    "### 주요 긍정 요인\n",
    "\"\"\"\n",
    "        \n",
    "        if insights[\"positive_signals\"]:\n",
    "            for i, signal in enumerate(insights[\"positive_signals\"][:5], 1):\n",
    "                analysis += f\"- {signal['keyword']} 관련 이슈 ({signal['source']}): {signal['title']}...\\n\"\n",
    "        else:\n",
    "            analysis += \"- 현재 특별한 긍정적 신호는 발견되지 않음\\n\"\n",
    "        \n",
    "        analysis += \"\\n### 주요 우려 요인\\n\"\n",
    "        if insights[\"negative_signals\"]:\n",
    "            for i, signal in enumerate(insights[\"negative_signals\"][:5], 1):\n",
    "                analysis += f\"- {signal['keyword']} 관련 이슈 ({signal['source']}): {signal['title']}...\\n\"\n",
    "        else:\n",
    "            analysis += \"- 현재 특별한 우려 요인은 발견되지 않음\\n\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _create_risk_assessment(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"리스크 평가 섹션 생성\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        \n",
    "        # 간단한 리스크 점수 계산\n",
    "        if negative_count == 0:\n",
    "            risk_level = \"낮음\"\n",
    "        elif negative_count <= positive_count:\n",
    "            risk_level = \"보통\"\n",
    "        else:\n",
    "            risk_level = \"높음\"\n",
    "        \n",
    "        assessment = f\"\"\"## ⚠️ 리스크 평가\n",
    "\n",
    "### 리스크 수준: {risk_level}\n",
    "\n",
    "### 평가 근거\n",
    "- 긍정적 신호 vs 부정적 신호: {positive_count} vs {negative_count}\n",
    "- 공시 정보 활용도: {insights['disclosure_count']}건 분석\n",
    "\n",
    "### 주요 리스크 요인\n",
    "\"\"\"\n",
    "        \n",
    "        if insights[\"negative_signals\"]:\n",
    "            risk_keywords = Counter([signal['keyword'] for signal in insights[\"negative_signals\"]])\n",
    "            for keyword, count in risk_keywords.most_common(3):\n",
    "                assessment += f\"- {keyword} 관련 이슈: {count}건 확인\\n\"\n",
    "        else:\n",
    "            assessment += \"- 현재 데이터에서 특별한 리스크 요인은 확인되지 않음\\n\"\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def _create_recommendation(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"투자 의견 섹션 생성\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        \n",
    "        # 간단한 추천 로직\n",
    "        if positive_count > negative_count * 1.5:\n",
    "            recommendation = \"매수\"\n",
    "            rationale = \"긍정적 신호가 부정적 신호를 크게 상회하여 투자 매력도가 높음\"\n",
    "        elif negative_count > positive_count * 1.5:\n",
    "            recommendation = \"매도\"\n",
    "            rationale = \"부정적 신호가 긍정적 신호를 크게 상회하여 투자 위험도가 높음\"\n",
    "        else:\n",
    "            recommendation = \"보유\"\n",
    "            rationale = \"긍정적/부정적 신호가 균형을 이루어 신중한 접근이 필요\"\n",
    "        \n",
    "        return f\"\"\"## 💡 투자 의견\n",
    "\n",
    "### 추천 의견: {recommendation}\n",
    "\n",
    "### 근거\n",
    "{rationale}\n",
    "\n",
    "### 데이터 기반 점수\n",
    "- 긍정 지수: {positive_count}\n",
    "- 위험 지수: {negative_count}\n",
    "- 정보 풍부도: {insights['recent_news_count'] + insights['disclosure_count']}점\n",
    "\n",
    "### 주의사항\n",
    "본 분석은 수집된 뉴스 및 공시 데이터의 키워드 분석을 기반으로 하며, \n",
    "실제 투자 결정 시에는 추가적인 재무 분석 및 전문가 의견을 참고하시기 바랍니다.\n",
    "\"\"\"\n",
    "    \n",
    "    def _extract_key_data_points(self, classified_data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"핵심 데이터 포인트 추출\"\"\"\n",
    "        key_points = []\n",
    "        \n",
    "        # 최신 뉴스 상위 3개\n",
    "        for doc in classified_data[\"news\"][:3]:\n",
    "            key_points.append({\n",
    "                \"type\": \"뉴스\",\n",
    "                \"title\": doc.metadata.get('title', '제목 없음'),\n",
    "                \"date\": doc.metadata.get('collection_date', '날짜 없음'),\n",
    "                \"summary\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        # 중요 공시 상위 3개\n",
    "        for doc in classified_data[\"disclosures\"][:3]:\n",
    "            key_points.append({\n",
    "                \"type\": \"공시\",\n",
    "                \"title\": doc.metadata.get('report_name', '공시명 없음'),\n",
    "                \"priority\": doc.metadata.get('priority', '일반'),\n",
    "                \"date\": doc.metadata.get('collection_date', '날짜 없음'),\n",
    "                \"summary\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return key_points\n",
    "\n",
    "print(\"✅ OfflineReportGenerator 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18be37c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\are01\\AppData\\Local\\Temp\\ipykernel_7144\\1172487021.py:24: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 오프라인 RAG 시스템 초기화 완료\n",
      "📊 벡터 DB 컬렉션 수: 1767\n",
      "\n",
      "📋 분석 가능한 기업 목록 (96개):\n",
      "  1. CJ ENM\n",
      "  2. CJ대한통운\n",
      "  3. CJ제일제당\n",
      "  4. GS\n",
      "  5. GS칼텍스\n",
      "  6. HMM\n",
      "  7. KB국민은행\n",
      "  8. KB금융\n",
      "  9. KT\n",
      "  10. KT&G\n",
      "  11. LG\n",
      "  12. LG디스플레이\n",
      "  13. LG생활건강\n",
      "  14. LG에너지솔루션\n",
      "  15. LG이노텍\n",
      "  16. LG전자\n",
      "  17. LG화학\n",
      "  18. NAVER\n",
      "  19. POSCO홀딩스\n",
      "  20. SK\n",
      "  21. SK머티리얼즈\n",
      "  22. SK바이오팜\n",
      "  23. SK스퀘어\n",
      "  24. SK이노베이션\n",
      "  25. SK텔레콤\n",
      "  26. SK하이닉스\n",
      "  27. 고려아연\n",
      "  28. 기아\n",
      "  29. 기업은행\n",
      "  30. 넥슨게임즈\n",
      "  31. 넷마블\n",
      "  32. 녹십자\n",
      "  33. 농심\n",
      "  34. 대웅제약\n",
      "  35. 대한항공\n",
      "  36. 동원시스템즈\n",
      "  37. 동화약품\n",
      "  38. 두산\n",
      "  39. 두산에너빌리티\n",
      "  40. 롯데쇼핑\n",
      "  41. 롯데제과\n",
      "  42. 롯데칠성음료\n",
      "  43. 롯데케미칼\n",
      "  44. 부광약품\n",
      "  45. 삼성SDI\n",
      "  46. 삼성디스플레이\n",
      "  47. 삼성물산\n",
      "  48. 삼성바이오로직스\n",
      "  49. 삼성에스디에스\n",
      "  50. 삼성전기\n",
      "  51. 삼성전자\n",
      "  52. 삼성중공업\n",
      "  53. 삼성화재\n",
      "  54. 삼천리\n",
      "  55. 셀트리온\n",
      "  56. 셀트리온제약\n",
      "  57. 신세계\n",
      "  58. 신한은행\n",
      "  59. 신한지주\n",
      "  60. 아모레퍼시픽\n",
      "  61. 오뚜기\n",
      "  62. 우리금융지주\n",
      "  63. 위메이드\n",
      "  64. 유한양행\n",
      "  65. 이마트\n",
      "  66. 일동제약\n",
      "  67. 종근당\n",
      "  68. 카카오\n",
      "  69. 카카오뱅크\n",
      "  70. 카카오페이\n",
      "  71. 컴투스\n",
      "  72. 코웨이\n",
      "  73. 크래프톤\n",
      "  74. 펄어비스\n",
      "  75. 포스코DX\n",
      "  76. 포스코인터내셔널\n",
      "  77. 포스코퓨처엠\n",
      "  78. 하나금융지주\n",
      "  79. 하나은행\n",
      "  80. 한국가스공사\n",
      "  81. 한국전력공사\n",
      "  82. 한국조선해양\n",
      "  83. 한미반도체\n",
      "  84. 한미약품\n",
      "  85. 한화생명\n",
      "  86. 한화솔루션\n",
      "  87. 한화시스템\n",
      "  88. 한화에어로스페이스\n",
      "  89. 현대건설\n",
      "  90. 현대글로비스\n",
      "  91. 현대모비스\n",
      "  92. 현대오일뱅크\n",
      "  93. 현대자동차\n",
      "  94. 현대제철\n",
      "  95. 현대중공업\n",
      "  96. 홈플러스\n"
     ]
    }
   ],
   "source": [
    "# 🚀 오프라인 RAG 시스템 초기화\n",
    "try:\n",
    "    offline_rag = OfflineReportGenerator(\n",
    "        db_dir=DB_DIR,\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "    \n",
    "    # 사용 가능한 기업 목록 조회\n",
    "    available_companies = offline_rag.get_available_companies()\n",
    "    print(f\"\\n📋 분석 가능한 기업 목록 ({len(available_companies)}개):\")\n",
    "    for i, company in enumerate(available_companies, 1):\n",
    "        print(f\"  {i}. {company}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 오프라인 RAG 시스템 초기화 실패: {e}\")\n",
    "    print(\"   pipeline_update.py를 먼저 실행하여 데이터를 수집하세요.\")\n",
    "    offline_rag = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e114d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 CJ ENM 분석 시작...\n",
      "\n",
      "📊 CJ ENM 오프라인 분석 시작...\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "\n",
      "================================================================================\n",
      "📈 CJ ENM 투자 분석 리포트 (오프라인)\n",
      "================================================================================\n",
      "생성 시간: 2025-08-04T18:34:30.094407\n",
      "분석 데이터: 총 9건\n",
      "  - 뉴스: 9건\n",
      "  - 공시: 0건\n",
      "\n",
      "\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "\n",
      "================================================================================\n",
      "📈 CJ ENM 투자 분석 리포트 (오프라인)\n",
      "================================================================================\n",
      "생성 시간: 2025-08-04T18:34:30.094407\n",
      "분석 데이터: 총 9건\n",
      "  - 뉴스: 9건\n",
      "  - 공시: 0건\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## 📈 투자 분석\n",
       "\n",
       "### 데이터 기반 현황\n",
       "- 최근 뉴스 분석: 9건\n",
       "- 공시 정보 분석: 0건\n",
       "- 긍정적 신호: 3개\n",
       "- 부정적 신호: 0개\n",
       "\n",
       "### 주요 긍정 요인\n",
       "- 투자 관련 이슈 (뉴스): 극장가 단기 부양책 한계?…'전독시' 부진에 투자 위축 가속...\n",
       "- 확대 관련 이슈 (뉴스): [오늘의 테마] '영상콘텐츠' VS '조선'...\n",
       "- 확대 관련 이슈 (뉴스): 콘진원, ‘K스토리&코믹스 인 아메리카’서 220억원 규모 수출 상담...\n",
       "\n",
       "### 주요 우려 요인\n",
       "- 현재 특별한 우려 요인은 발견되지 않음\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ⚠️ 리스크 평가\n",
       "\n",
       "### 리스크 수준: 낮음\n",
       "\n",
       "### 평가 근거\n",
       "- 긍정적 신호 vs 부정적 신호: 3 vs 0\n",
       "- 공시 정보 활용도: 0건 분석\n",
       "\n",
       "### 주요 리스크 요인\n",
       "- 현재 데이터에서 특별한 리스크 요인은 확인되지 않음\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 💡 투자 의견\n",
       "\n",
       "### 추천 의견: 매수\n",
       "\n",
       "### 근거\n",
       "긍정적 신호가 부정적 신호를 크게 상회하여 투자 매력도가 높음\n",
       "\n",
       "### 데이터 기반 점수\n",
       "- 긍정 지수: 3\n",
       "- 위험 지수: 0\n",
       "- 정보 풍부도: 9점\n",
       "\n",
       "### 주의사항\n",
       "본 분석은 수집된 뉴스 및 공시 데이터의 키워드 분석을 기반으로 하며, \n",
       "실제 투자 결정 시에는 추가적인 재무 분석 및 전문가 의견을 참고하시기 바랍니다.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📋 핵심 데이터 포인트\n",
      "================================================================================\n",
      "\n",
      "[1] 뉴스: 악마가 이사왔다, 임윤아·안보현 코믹 케미에 6천원 할인까지\n",
      "    요약: 제목: 악마가 이사왔다, 임윤아·안보현 코믹 케미에 6천원 할인까지\n",
      "내용: |중앙이코노미뉴스 이상민 기자|출처=CJ ENM  영화 '악마가 이사왔다'가 국민 영화관람 할인권 적용 ...\n",
      "    수집일: 2025-08-04\n",
      "\n",
      "[2] 뉴스: 극장가 단기 부양책 한계?…'전독시' 부진에 투자 위축 가속\n",
      "    요약: 제목: 극장가 단기 부양책 한계?…'전독시' 부진에 투자 위축 가속\n",
      "내용: 국내 대표 투자배급사인 CJ ENM도 복수 영화를 준비하고 있지만, 투자에 제동이 걸려 발표를 차일피일 ...\n",
      "    수집일: 2025-08-04\n",
      "\n",
      "[3] 뉴스: 임윤아x안보현 \"6000원 할인, 이렇게 받으세요\"…'악마가 이사왔다' 관람...\n",
      "    요약: 제목: 임윤아x안보현 \"6000원 할인, 이렇게 받으세요\"…'악마가 이사왔다' 관람...\n",
      "내용: 할인권은 CGV, 메가박스, 롯데시네마, 씨네큐 극장 홈페이지 또는 애플리케이션을 ...\n",
      "    수집일: 2025-08-04\n"
     ]
    }
   ],
   "source": [
    "# 📊 단일 기업 분석 실행\n",
    "if offline_rag and available_companies:\n",
    "    # 첫 번째 기업으로 테스트 (원하는 기업명으로 변경 가능)\n",
    "    test_company = available_companies[0]\n",
    "    print(f\"🎯 {test_company} 분석 시작...\\n\")\n",
    "    \n",
    "    # 오프라인 리포트 생성\n",
    "    report = offline_rag.generate_offline_report(test_company)\n",
    "    \n",
    "    if report[\"status\"] == \"완료\":\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"📈 {report['company']} 투자 분석 리포트 (오프라인)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"생성 시간: {report['generation_date']}\")\n",
    "        print(f\"분석 데이터: 총 {report['data_summary']['total_documents']}건\")\n",
    "        print(f\"  - 뉴스: {report['data_summary']['news_articles']}건\")\n",
    "        print(f\"  - 공시: {report['data_summary']['disclosures']}건\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # 투자 분석 출력\n",
    "        display(Markdown(report['investment_analysis']))\n",
    "        \n",
    "        # 리스크 평가 출력\n",
    "        display(Markdown(report['risk_assessment']))\n",
    "        \n",
    "        # 투자 의견 출력\n",
    "        display(Markdown(report['recommendation']))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📋 핵심 데이터 포인트\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, point in enumerate(report['key_data_points'][:5], 1):\n",
    "            print(f\"\\n[{i}] {point['type']}: {point['title']}\")\n",
    "            if 'priority' in point:\n",
    "                print(f\"    우선순위: {point['priority']}\")\n",
    "            print(f\"    요약: {point['summary']}\")\n",
    "            print(f\"    수집일: {point['date'][:10]}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ 리포트 생성 실패: {report.get('message', '알 수 없는 오류')}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 시스템이 초기화되지 않았거나 분석 가능한 기업이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1281fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 파인튜닝 데이터셋 생성 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 📝 파인튜닝 데이터셋 생성 함수\n",
    "def create_finetuning_dataset(offline_rag_system, companies: List[str], output_file: str = \"investment_reports_dataset.jsonl\"):\n",
    "    \"\"\"모든 기업에 대해 리포트를 생성하고 파인튜닝 데이터셋으로 저장\"\"\"\n",
    "    \n",
    "    print(f\"📦 파인튜닝 데이터셋 생성 시작: {len(companies)}개 기업\")\n",
    "    \n",
    "    dataset = []\n",
    "    successful_reports = 0\n",
    "    \n",
    "    for i, company in enumerate(companies, 1):\n",
    "        print(f\"\\n[{i}/{len(companies)}] {company} 처리 중...\")\n",
    "        \n",
    "        try:\n",
    "            # 오프라인 리포트 생성\n",
    "            report = offline_rag_system.generate_offline_report(company)\n",
    "            \n",
    "            if report[\"status\"] == \"완료\":\n",
    "                # 파인튜닝 형식으로 변환\n",
    "                training_example = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"{company}에 대한 투자 분석 리포트를 작성해주세요. 최근 뉴스 {report['data_summary']['news_articles']}건과 공시 {report['data_summary']['disclosures']}건의 데이터가 있습니다.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": f\"\"\"{report['investment_analysis']}\n",
    "\n",
    "{report['risk_assessment']}\n",
    "\n",
    "{report['recommendation']}\n",
    "\n",
    "## 📋 분석 근거 데이터\n",
    "- 총 분석 문서: {report['data_summary']['total_documents']}건\n",
    "- 뉴스 분석: {report['data_summary']['news_articles']}건\n",
    "- 공시 분석: {report['data_summary']['disclosures']}건\n",
    "- 분석 완료 시간: {report['generation_date']}\n",
    "\"\"\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                dataset.append(training_example)\n",
    "                successful_reports += 1\n",
    "                print(f\"  ✅ {company} 데이터셋 생성 완료\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ⚠️ {company} 리포트 생성 실패: {report.get('message', '알 수 없는 오류')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {company} 처리 중 오류: {e}\")\n",
    "    \n",
    "    # JSONL 파일로 저장\n",
    "    if dataset:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in dataset:\n",
    "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"\\n🎉 파인튜닝 데이터셋 생성 완료!\")\n",
    "        print(f\"📄 파일: {output_file}\")\n",
    "        print(f\"📊 총 데이터: {len(dataset)}개 (성공률: {successful_reports}/{len(companies)})\")\n",
    "        \n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"❌ 생성된 데이터셋이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ 파인튜닝 데이터셋 생성 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f96f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 전체 기업 배치 처리 시작...\n",
      "📦 파인튜닝 데이터셋 생성 시작: 96개 기업\n",
      "\n",
      "[1/96] CJ ENM 처리 중...\n",
      "📊 CJ ENM 오프라인 분석 시작...\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "  ✅ CJ ENM 데이터셋 생성 완료\n",
      "\n",
      "[2/96] CJ대한통운 처리 중...\n",
      "📊 CJ대한통운 오프라인 분석 시작...\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "  ✅ CJ ENM 데이터셋 생성 완료\n",
      "\n",
      "[2/96] CJ대한통운 처리 중...\n",
      "📊 CJ대한통운 오프라인 분석 시작...\n",
      "✅ CJ대한통운 오프라인 분석 완료\n",
      "  ✅ CJ대한통운 데이터셋 생성 완료\n",
      "\n",
      "[3/96] CJ제일제당 처리 중...\n",
      "📊 CJ제일제당 오프라인 분석 시작...\n",
      "✅ CJ대한통운 오프라인 분석 완료\n",
      "  ✅ CJ대한통운 데이터셋 생성 완료\n",
      "\n",
      "[3/96] CJ제일제당 처리 중...\n",
      "📊 CJ제일제당 오프라인 분석 시작...\n",
      "✅ CJ제일제당 오프라인 분석 완료\n",
      "  ✅ CJ제일제당 데이터셋 생성 완료\n",
      "\n",
      "[4/96] GS 처리 중...\n",
      "📊 GS 오프라인 분석 시작...\n",
      "✅ CJ제일제당 오프라인 분석 완료\n",
      "  ✅ CJ제일제당 데이터셋 생성 완료\n",
      "\n",
      "[4/96] GS 처리 중...\n",
      "📊 GS 오프라인 분석 시작...\n",
      "✅ GS 오프라인 분석 완료\n",
      "  ✅ GS 데이터셋 생성 완료\n",
      "\n",
      "[5/96] GS칼텍스 처리 중...\n",
      "📊 GS칼텍스 오프라인 분석 시작...\n",
      "✅ GS 오프라인 분석 완료\n",
      "  ✅ GS 데이터셋 생성 완료\n",
      "\n",
      "[5/96] GS칼텍스 처리 중...\n",
      "📊 GS칼텍스 오프라인 분석 시작...\n",
      "✅ GS칼텍스 오프라인 분석 완료\n",
      "  ✅ GS칼텍스 데이터셋 생성 완료\n",
      "\n",
      "[6/96] HMM 처리 중...\n",
      "📊 HMM 오프라인 분석 시작...\n",
      "✅ GS칼텍스 오프라인 분석 완료\n",
      "  ✅ GS칼텍스 데이터셋 생성 완료\n",
      "\n",
      "[6/96] HMM 처리 중...\n",
      "📊 HMM 오프라인 분석 시작...\n",
      "✅ HMM 오프라인 분석 완료\n",
      "  ✅ HMM 데이터셋 생성 완료\n",
      "\n",
      "[7/96] KB국민은행 처리 중...\n",
      "📊 KB국민은행 오프라인 분석 시작...\n",
      "✅ HMM 오프라인 분석 완료\n",
      "  ✅ HMM 데이터셋 생성 완료\n",
      "\n",
      "[7/96] KB국민은행 처리 중...\n",
      "📊 KB국민은행 오프라인 분석 시작...\n",
      "✅ KB국민은행 오프라인 분석 완료\n",
      "  ✅ KB국민은행 데이터셋 생성 완료\n",
      "\n",
      "[8/96] KB금융 처리 중...\n",
      "📊 KB금융 오프라인 분석 시작...\n",
      "✅ KB국민은행 오프라인 분석 완료\n",
      "  ✅ KB국민은행 데이터셋 생성 완료\n",
      "\n",
      "[8/96] KB금융 처리 중...\n",
      "📊 KB금융 오프라인 분석 시작...\n",
      "✅ KB금융 오프라인 분석 완료\n",
      "  ✅ KB금융 데이터셋 생성 완료\n",
      "\n",
      "[9/96] KT 처리 중...\n",
      "📊 KT 오프라인 분석 시작...\n",
      "✅ KB금융 오프라인 분석 완료\n",
      "  ✅ KB금융 데이터셋 생성 완료\n",
      "\n",
      "[9/96] KT 처리 중...\n",
      "📊 KT 오프라인 분석 시작...\n",
      "✅ KT 오프라인 분석 완료\n",
      "  ✅ KT 데이터셋 생성 완료\n",
      "\n",
      "[10/96] KT&G 처리 중...\n",
      "📊 KT&G 오프라인 분석 시작...\n",
      "✅ KT 오프라인 분석 완료\n",
      "  ✅ KT 데이터셋 생성 완료\n",
      "\n",
      "[10/96] KT&G 처리 중...\n",
      "📊 KT&G 오프라인 분석 시작...\n",
      "✅ KT&G 오프라인 분석 완료\n",
      "  ✅ KT&G 데이터셋 생성 완료\n",
      "\n",
      "[11/96] LG 처리 중...\n",
      "📊 LG 오프라인 분석 시작...\n",
      "✅ KT&G 오프라인 분석 완료\n",
      "  ✅ KT&G 데이터셋 생성 완료\n",
      "\n",
      "[11/96] LG 처리 중...\n",
      "📊 LG 오프라인 분석 시작...\n",
      "✅ LG 오프라인 분석 완료\n",
      "  ✅ LG 데이터셋 생성 완료\n",
      "\n",
      "[12/96] LG디스플레이 처리 중...\n",
      "📊 LG디스플레이 오프라인 분석 시작...\n",
      "✅ LG 오프라인 분석 완료\n",
      "  ✅ LG 데이터셋 생성 완료\n",
      "\n",
      "[12/96] LG디스플레이 처리 중...\n",
      "📊 LG디스플레이 오프라인 분석 시작...\n",
      "✅ LG디스플레이 오프라인 분석 완료\n",
      "  ✅ LG디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[13/96] LG생활건강 처리 중...\n",
      "📊 LG생활건강 오프라인 분석 시작...\n",
      "✅ LG디스플레이 오프라인 분석 완료\n",
      "  ✅ LG디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[13/96] LG생활건강 처리 중...\n",
      "📊 LG생활건강 오프라인 분석 시작...\n",
      "✅ LG생활건강 오프라인 분석 완료\n",
      "  ✅ LG생활건강 데이터셋 생성 완료\n",
      "\n",
      "[14/96] LG에너지솔루션 처리 중...\n",
      "📊 LG에너지솔루션 오프라인 분석 시작...\n",
      "✅ LG생활건강 오프라인 분석 완료\n",
      "  ✅ LG생활건강 데이터셋 생성 완료\n",
      "\n",
      "[14/96] LG에너지솔루션 처리 중...\n",
      "📊 LG에너지솔루션 오프라인 분석 시작...\n",
      "✅ LG에너지솔루션 오프라인 분석 완료\n",
      "  ✅ LG에너지솔루션 데이터셋 생성 완료\n",
      "\n",
      "[15/96] LG이노텍 처리 중...\n",
      "📊 LG이노텍 오프라인 분석 시작...\n",
      "✅ LG에너지솔루션 오프라인 분석 완료\n",
      "  ✅ LG에너지솔루션 데이터셋 생성 완료\n",
      "\n",
      "[15/96] LG이노텍 처리 중...\n",
      "📊 LG이노텍 오프라인 분석 시작...\n",
      "✅ LG이노텍 오프라인 분석 완료\n",
      "  ✅ LG이노텍 데이터셋 생성 완료\n",
      "\n",
      "[16/96] LG전자 처리 중...\n",
      "📊 LG전자 오프라인 분석 시작...\n",
      "✅ LG이노텍 오프라인 분석 완료\n",
      "  ✅ LG이노텍 데이터셋 생성 완료\n",
      "\n",
      "[16/96] LG전자 처리 중...\n",
      "📊 LG전자 오프라인 분석 시작...\n",
      "✅ LG전자 오프라인 분석 완료\n",
      "  ✅ LG전자 데이터셋 생성 완료\n",
      "\n",
      "[17/96] LG화학 처리 중...\n",
      "📊 LG화학 오프라인 분석 시작...\n",
      "✅ LG전자 오프라인 분석 완료\n",
      "  ✅ LG전자 데이터셋 생성 완료\n",
      "\n",
      "[17/96] LG화학 처리 중...\n",
      "📊 LG화학 오프라인 분석 시작...\n",
      "✅ LG화학 오프라인 분석 완료\n",
      "  ✅ LG화학 데이터셋 생성 완료\n",
      "\n",
      "[18/96] NAVER 처리 중...\n",
      "📊 NAVER 오프라인 분석 시작...\n",
      "✅ LG화학 오프라인 분석 완료\n",
      "  ✅ LG화학 데이터셋 생성 완료\n",
      "\n",
      "[18/96] NAVER 처리 중...\n",
      "📊 NAVER 오프라인 분석 시작...\n",
      "✅ NAVER 오프라인 분석 완료\n",
      "  ✅ NAVER 데이터셋 생성 완료\n",
      "\n",
      "[19/96] POSCO홀딩스 처리 중...\n",
      "📊 POSCO홀딩스 오프라인 분석 시작...\n",
      "✅ NAVER 오프라인 분석 완료\n",
      "  ✅ NAVER 데이터셋 생성 완료\n",
      "\n",
      "[19/96] POSCO홀딩스 처리 중...\n",
      "📊 POSCO홀딩스 오프라인 분석 시작...\n",
      "✅ POSCO홀딩스 오프라인 분석 완료\n",
      "  ✅ POSCO홀딩스 데이터셋 생성 완료\n",
      "\n",
      "[20/96] SK 처리 중...\n",
      "📊 SK 오프라인 분석 시작...\n",
      "✅ POSCO홀딩스 오프라인 분석 완료\n",
      "  ✅ POSCO홀딩스 데이터셋 생성 완료\n",
      "\n",
      "[20/96] SK 처리 중...\n",
      "📊 SK 오프라인 분석 시작...\n",
      "✅ SK 오프라인 분석 완료\n",
      "  ✅ SK 데이터셋 생성 완료\n",
      "\n",
      "[21/96] SK머티리얼즈 처리 중...\n",
      "📊 SK머티리얼즈 오프라인 분석 시작...\n",
      "✅ SK 오프라인 분석 완료\n",
      "  ✅ SK 데이터셋 생성 완료\n",
      "\n",
      "[21/96] SK머티리얼즈 처리 중...\n",
      "📊 SK머티리얼즈 오프라인 분석 시작...\n",
      "✅ SK머티리얼즈 오프라인 분석 완료\n",
      "  ✅ SK머티리얼즈 데이터셋 생성 완료\n",
      "\n",
      "[22/96] SK바이오팜 처리 중...\n",
      "📊 SK바이오팜 오프라인 분석 시작...\n",
      "✅ SK머티리얼즈 오프라인 분석 완료\n",
      "  ✅ SK머티리얼즈 데이터셋 생성 완료\n",
      "\n",
      "[22/96] SK바이오팜 처리 중...\n",
      "📊 SK바이오팜 오프라인 분석 시작...\n",
      "✅ SK바이오팜 오프라인 분석 완료\n",
      "  ✅ SK바이오팜 데이터셋 생성 완료\n",
      "\n",
      "[23/96] SK스퀘어 처리 중...\n",
      "📊 SK스퀘어 오프라인 분석 시작...\n",
      "✅ SK바이오팜 오프라인 분석 완료\n",
      "  ✅ SK바이오팜 데이터셋 생성 완료\n",
      "\n",
      "[23/96] SK스퀘어 처리 중...\n",
      "📊 SK스퀘어 오프라인 분석 시작...\n",
      "✅ SK스퀘어 오프라인 분석 완료\n",
      "  ✅ SK스퀘어 데이터셋 생성 완료\n",
      "\n",
      "[24/96] SK이노베이션 처리 중...\n",
      "📊 SK이노베이션 오프라인 분석 시작...\n",
      "✅ SK스퀘어 오프라인 분석 완료\n",
      "  ✅ SK스퀘어 데이터셋 생성 완료\n",
      "\n",
      "[24/96] SK이노베이션 처리 중...\n",
      "📊 SK이노베이션 오프라인 분석 시작...\n",
      "✅ SK이노베이션 오프라인 분석 완료\n",
      "  ✅ SK이노베이션 데이터셋 생성 완료\n",
      "\n",
      "[25/96] SK텔레콤 처리 중...\n",
      "📊 SK텔레콤 오프라인 분석 시작...\n",
      "✅ SK이노베이션 오프라인 분석 완료\n",
      "  ✅ SK이노베이션 데이터셋 생성 완료\n",
      "\n",
      "[25/96] SK텔레콤 처리 중...\n",
      "📊 SK텔레콤 오프라인 분석 시작...\n",
      "✅ SK텔레콤 오프라인 분석 완료\n",
      "  ✅ SK텔레콤 데이터셋 생성 완료\n",
      "\n",
      "[26/96] SK하이닉스 처리 중...\n",
      "📊 SK하이닉스 오프라인 분석 시작...\n",
      "✅ SK텔레콤 오프라인 분석 완료\n",
      "  ✅ SK텔레콤 데이터셋 생성 완료\n",
      "\n",
      "[26/96] SK하이닉스 처리 중...\n",
      "📊 SK하이닉스 오프라인 분석 시작...\n",
      "✅ SK하이닉스 오프라인 분석 완료\n",
      "  ✅ SK하이닉스 데이터셋 생성 완료\n",
      "\n",
      "[27/96] 고려아연 처리 중...\n",
      "📊 고려아연 오프라인 분석 시작...\n",
      "✅ SK하이닉스 오프라인 분석 완료\n",
      "  ✅ SK하이닉스 데이터셋 생성 완료\n",
      "\n",
      "[27/96] 고려아연 처리 중...\n",
      "📊 고려아연 오프라인 분석 시작...\n",
      "✅ 고려아연 오프라인 분석 완료\n",
      "  ✅ 고려아연 데이터셋 생성 완료\n",
      "\n",
      "[28/96] 기아 처리 중...\n",
      "📊 기아 오프라인 분석 시작...\n",
      "✅ 고려아연 오프라인 분석 완료\n",
      "  ✅ 고려아연 데이터셋 생성 완료\n",
      "\n",
      "[28/96] 기아 처리 중...\n",
      "📊 기아 오프라인 분석 시작...\n",
      "✅ 기아 오프라인 분석 완료\n",
      "  ✅ 기아 데이터셋 생성 완료\n",
      "\n",
      "[29/96] 기업은행 처리 중...\n",
      "📊 기업은행 오프라인 분석 시작...\n",
      "✅ 기아 오프라인 분석 완료\n",
      "  ✅ 기아 데이터셋 생성 완료\n",
      "\n",
      "[29/96] 기업은행 처리 중...\n",
      "📊 기업은행 오프라인 분석 시작...\n",
      "✅ 기업은행 오프라인 분석 완료\n",
      "  ✅ 기업은행 데이터셋 생성 완료\n",
      "\n",
      "[30/96] 넥슨게임즈 처리 중...\n",
      "📊 넥슨게임즈 오프라인 분석 시작...\n",
      "✅ 기업은행 오프라인 분석 완료\n",
      "  ✅ 기업은행 데이터셋 생성 완료\n",
      "\n",
      "[30/96] 넥슨게임즈 처리 중...\n",
      "📊 넥슨게임즈 오프라인 분석 시작...\n",
      "✅ 넥슨게임즈 오프라인 분석 완료\n",
      "  ✅ 넥슨게임즈 데이터셋 생성 완료\n",
      "\n",
      "[31/96] 넷마블 처리 중...\n",
      "📊 넷마블 오프라인 분석 시작...\n",
      "✅ 넥슨게임즈 오프라인 분석 완료\n",
      "  ✅ 넥슨게임즈 데이터셋 생성 완료\n",
      "\n",
      "[31/96] 넷마블 처리 중...\n",
      "📊 넷마블 오프라인 분석 시작...\n",
      "✅ 넷마블 오프라인 분석 완료\n",
      "  ✅ 넷마블 데이터셋 생성 완료\n",
      "\n",
      "[32/96] 녹십자 처리 중...\n",
      "📊 녹십자 오프라인 분석 시작...\n",
      "✅ 넷마블 오프라인 분석 완료\n",
      "  ✅ 넷마블 데이터셋 생성 완료\n",
      "\n",
      "[32/96] 녹십자 처리 중...\n",
      "📊 녹십자 오프라인 분석 시작...\n",
      "✅ 녹십자 오프라인 분석 완료\n",
      "  ✅ 녹십자 데이터셋 생성 완료\n",
      "\n",
      "[33/96] 농심 처리 중...\n",
      "📊 농심 오프라인 분석 시작...\n",
      "✅ 녹십자 오프라인 분석 완료\n",
      "  ✅ 녹십자 데이터셋 생성 완료\n",
      "\n",
      "[33/96] 농심 처리 중...\n",
      "📊 농심 오프라인 분석 시작...\n",
      "✅ 농심 오프라인 분석 완료\n",
      "  ✅ 농심 데이터셋 생성 완료\n",
      "\n",
      "[34/96] 대웅제약 처리 중...\n",
      "📊 대웅제약 오프라인 분석 시작...\n",
      "✅ 농심 오프라인 분석 완료\n",
      "  ✅ 농심 데이터셋 생성 완료\n",
      "\n",
      "[34/96] 대웅제약 처리 중...\n",
      "📊 대웅제약 오프라인 분석 시작...\n",
      "✅ 대웅제약 오프라인 분석 완료\n",
      "  ✅ 대웅제약 데이터셋 생성 완료\n",
      "\n",
      "[35/96] 대한항공 처리 중...\n",
      "📊 대한항공 오프라인 분석 시작...\n",
      "✅ 대웅제약 오프라인 분석 완료\n",
      "  ✅ 대웅제약 데이터셋 생성 완료\n",
      "\n",
      "[35/96] 대한항공 처리 중...\n",
      "📊 대한항공 오프라인 분석 시작...\n",
      "✅ 대한항공 오프라인 분석 완료\n",
      "  ✅ 대한항공 데이터셋 생성 완료\n",
      "\n",
      "[36/96] 동원시스템즈 처리 중...\n",
      "📊 동원시스템즈 오프라인 분석 시작...\n",
      "✅ 대한항공 오프라인 분석 완료\n",
      "  ✅ 대한항공 데이터셋 생성 완료\n",
      "\n",
      "[36/96] 동원시스템즈 처리 중...\n",
      "📊 동원시스템즈 오프라인 분석 시작...\n",
      "✅ 동원시스템즈 오프라인 분석 완료\n",
      "  ✅ 동원시스템즈 데이터셋 생성 완료\n",
      "\n",
      "[37/96] 동화약품 처리 중...\n",
      "📊 동화약품 오프라인 분석 시작...\n",
      "✅ 동원시스템즈 오프라인 분석 완료\n",
      "  ✅ 동원시스템즈 데이터셋 생성 완료\n",
      "\n",
      "[37/96] 동화약품 처리 중...\n",
      "📊 동화약품 오프라인 분석 시작...\n",
      "✅ 동화약품 오프라인 분석 완료\n",
      "  ✅ 동화약품 데이터셋 생성 완료\n",
      "\n",
      "[38/96] 두산 처리 중...\n",
      "📊 두산 오프라인 분석 시작...\n",
      "✅ 동화약품 오프라인 분석 완료\n",
      "  ✅ 동화약품 데이터셋 생성 완료\n",
      "\n",
      "[38/96] 두산 처리 중...\n",
      "📊 두산 오프라인 분석 시작...\n",
      "✅ 두산 오프라인 분석 완료\n",
      "  ✅ 두산 데이터셋 생성 완료\n",
      "\n",
      "[39/96] 두산에너빌리티 처리 중...\n",
      "📊 두산에너빌리티 오프라인 분석 시작...\n",
      "✅ 두산 오프라인 분석 완료\n",
      "  ✅ 두산 데이터셋 생성 완료\n",
      "\n",
      "[39/96] 두산에너빌리티 처리 중...\n",
      "📊 두산에너빌리티 오프라인 분석 시작...\n",
      "✅ 두산에너빌리티 오프라인 분석 완료\n",
      "  ✅ 두산에너빌리티 데이터셋 생성 완료\n",
      "\n",
      "[40/96] 롯데쇼핑 처리 중...\n",
      "📊 롯데쇼핑 오프라인 분석 시작...\n",
      "✅ 두산에너빌리티 오프라인 분석 완료\n",
      "  ✅ 두산에너빌리티 데이터셋 생성 완료\n",
      "\n",
      "[40/96] 롯데쇼핑 처리 중...\n",
      "📊 롯데쇼핑 오프라인 분석 시작...\n",
      "✅ 롯데쇼핑 오프라인 분석 완료\n",
      "  ✅ 롯데쇼핑 데이터셋 생성 완료\n",
      "\n",
      "[41/96] 롯데제과 처리 중...\n",
      "📊 롯데제과 오프라인 분석 시작...\n",
      "✅ 롯데쇼핑 오프라인 분석 완료\n",
      "  ✅ 롯데쇼핑 데이터셋 생성 완료\n",
      "\n",
      "[41/96] 롯데제과 처리 중...\n",
      "📊 롯데제과 오프라인 분석 시작...\n",
      "✅ 롯데제과 오프라인 분석 완료\n",
      "  ✅ 롯데제과 데이터셋 생성 완료\n",
      "\n",
      "[42/96] 롯데칠성음료 처리 중...\n",
      "📊 롯데칠성음료 오프라인 분석 시작...\n",
      "✅ 롯데제과 오프라인 분석 완료\n",
      "  ✅ 롯데제과 데이터셋 생성 완료\n",
      "\n",
      "[42/96] 롯데칠성음료 처리 중...\n",
      "📊 롯데칠성음료 오프라인 분석 시작...\n",
      "✅ 롯데칠성음료 오프라인 분석 완료\n",
      "  ✅ 롯데칠성음료 데이터셋 생성 완료\n",
      "\n",
      "[43/96] 롯데케미칼 처리 중...\n",
      "📊 롯데케미칼 오프라인 분석 시작...\n",
      "✅ 롯데칠성음료 오프라인 분석 완료\n",
      "  ✅ 롯데칠성음료 데이터셋 생성 완료\n",
      "\n",
      "[43/96] 롯데케미칼 처리 중...\n",
      "📊 롯데케미칼 오프라인 분석 시작...\n",
      "✅ 롯데케미칼 오프라인 분석 완료\n",
      "  ✅ 롯데케미칼 데이터셋 생성 완료\n",
      "\n",
      "[44/96] 부광약품 처리 중...\n",
      "📊 부광약품 오프라인 분석 시작...\n",
      "✅ 롯데케미칼 오프라인 분석 완료\n",
      "  ✅ 롯데케미칼 데이터셋 생성 완료\n",
      "\n",
      "[44/96] 부광약품 처리 중...\n",
      "📊 부광약품 오프라인 분석 시작...\n",
      "✅ 부광약품 오프라인 분석 완료\n",
      "  ✅ 부광약품 데이터셋 생성 완료\n",
      "\n",
      "[45/96] 삼성SDI 처리 중...\n",
      "📊 삼성SDI 오프라인 분석 시작...\n",
      "✅ 부광약품 오프라인 분석 완료\n",
      "  ✅ 부광약품 데이터셋 생성 완료\n",
      "\n",
      "[45/96] 삼성SDI 처리 중...\n",
      "📊 삼성SDI 오프라인 분석 시작...\n",
      "✅ 삼성SDI 오프라인 분석 완료\n",
      "  ✅ 삼성SDI 데이터셋 생성 완료\n",
      "\n",
      "[46/96] 삼성디스플레이 처리 중...\n",
      "📊 삼성디스플레이 오프라인 분석 시작...\n",
      "✅ 삼성SDI 오프라인 분석 완료\n",
      "  ✅ 삼성SDI 데이터셋 생성 완료\n",
      "\n",
      "[46/96] 삼성디스플레이 처리 중...\n",
      "📊 삼성디스플레이 오프라인 분석 시작...\n",
      "✅ 삼성디스플레이 오프라인 분석 완료\n",
      "  ✅ 삼성디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[47/96] 삼성물산 처리 중...\n",
      "📊 삼성물산 오프라인 분석 시작...\n",
      "✅ 삼성디스플레이 오프라인 분석 완료\n",
      "  ✅ 삼성디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[47/96] 삼성물산 처리 중...\n",
      "📊 삼성물산 오프라인 분석 시작...\n",
      "✅ 삼성물산 오프라인 분석 완료\n",
      "  ✅ 삼성물산 데이터셋 생성 완료\n",
      "\n",
      "[48/96] 삼성바이오로직스 처리 중...\n",
      "📊 삼성바이오로직스 오프라인 분석 시작...\n",
      "✅ 삼성물산 오프라인 분석 완료\n",
      "  ✅ 삼성물산 데이터셋 생성 완료\n",
      "\n",
      "[48/96] 삼성바이오로직스 처리 중...\n",
      "📊 삼성바이오로직스 오프라인 분석 시작...\n",
      "✅ 삼성바이오로직스 오프라인 분석 완료\n",
      "  ✅ 삼성바이오로직스 데이터셋 생성 완료\n",
      "\n",
      "[49/96] 삼성에스디에스 처리 중...\n",
      "📊 삼성에스디에스 오프라인 분석 시작...\n",
      "✅ 삼성바이오로직스 오프라인 분석 완료\n",
      "  ✅ 삼성바이오로직스 데이터셋 생성 완료\n",
      "\n",
      "[49/96] 삼성에스디에스 처리 중...\n",
      "📊 삼성에스디에스 오프라인 분석 시작...\n",
      "✅ 삼성에스디에스 오프라인 분석 완료\n",
      "  ✅ 삼성에스디에스 데이터셋 생성 완료\n",
      "\n",
      "[50/96] 삼성전기 처리 중...\n",
      "📊 삼성전기 오프라인 분석 시작...\n",
      "✅ 삼성에스디에스 오프라인 분석 완료\n",
      "  ✅ 삼성에스디에스 데이터셋 생성 완료\n",
      "\n",
      "[50/96] 삼성전기 처리 중...\n",
      "📊 삼성전기 오프라인 분석 시작...\n",
      "✅ 삼성전기 오프라인 분석 완료\n",
      "  ✅ 삼성전기 데이터셋 생성 완료\n",
      "\n",
      "[51/96] 삼성전자 처리 중...\n",
      "📊 삼성전자 오프라인 분석 시작...\n",
      "✅ 삼성전기 오프라인 분석 완료\n",
      "  ✅ 삼성전기 데이터셋 생성 완료\n",
      "\n",
      "[51/96] 삼성전자 처리 중...\n",
      "📊 삼성전자 오프라인 분석 시작...\n",
      "✅ 삼성전자 오프라인 분석 완료\n",
      "  ✅ 삼성전자 데이터셋 생성 완료\n",
      "\n",
      "[52/96] 삼성중공업 처리 중...\n",
      "📊 삼성중공업 오프라인 분석 시작...\n",
      "✅ 삼성전자 오프라인 분석 완료\n",
      "  ✅ 삼성전자 데이터셋 생성 완료\n",
      "\n",
      "[52/96] 삼성중공업 처리 중...\n",
      "📊 삼성중공업 오프라인 분석 시작...\n",
      "✅ 삼성중공업 오프라인 분석 완료\n",
      "  ✅ 삼성중공업 데이터셋 생성 완료\n",
      "\n",
      "[53/96] 삼성화재 처리 중...\n",
      "📊 삼성화재 오프라인 분석 시작...\n",
      "✅ 삼성중공업 오프라인 분석 완료\n",
      "  ✅ 삼성중공업 데이터셋 생성 완료\n",
      "\n",
      "[53/96] 삼성화재 처리 중...\n",
      "📊 삼성화재 오프라인 분석 시작...\n",
      "✅ 삼성화재 오프라인 분석 완료\n",
      "  ✅ 삼성화재 데이터셋 생성 완료\n",
      "\n",
      "[54/96] 삼천리 처리 중...\n",
      "📊 삼천리 오프라인 분석 시작...\n",
      "✅ 삼성화재 오프라인 분석 완료\n",
      "  ✅ 삼성화재 데이터셋 생성 완료\n",
      "\n",
      "[54/96] 삼천리 처리 중...\n",
      "📊 삼천리 오프라인 분석 시작...\n",
      "✅ 삼천리 오프라인 분석 완료\n",
      "  ✅ 삼천리 데이터셋 생성 완료\n",
      "\n",
      "[55/96] 셀트리온 처리 중...\n",
      "📊 셀트리온 오프라인 분석 시작...\n",
      "✅ 삼천리 오프라인 분석 완료\n",
      "  ✅ 삼천리 데이터셋 생성 완료\n",
      "\n",
      "[55/96] 셀트리온 처리 중...\n",
      "📊 셀트리온 오프라인 분석 시작...\n",
      "✅ 셀트리온 오프라인 분석 완료\n",
      "  ✅ 셀트리온 데이터셋 생성 완료\n",
      "\n",
      "[56/96] 셀트리온제약 처리 중...\n",
      "📊 셀트리온제약 오프라인 분석 시작...\n",
      "✅ 셀트리온 오프라인 분석 완료\n",
      "  ✅ 셀트리온 데이터셋 생성 완료\n",
      "\n",
      "[56/96] 셀트리온제약 처리 중...\n",
      "📊 셀트리온제약 오프라인 분석 시작...\n",
      "✅ 셀트리온제약 오프라인 분석 완료\n",
      "  ✅ 셀트리온제약 데이터셋 생성 완료\n",
      "\n",
      "[57/96] 신세계 처리 중...\n",
      "📊 신세계 오프라인 분석 시작...\n",
      "✅ 셀트리온제약 오프라인 분석 완료\n",
      "  ✅ 셀트리온제약 데이터셋 생성 완료\n",
      "\n",
      "[57/96] 신세계 처리 중...\n",
      "📊 신세계 오프라인 분석 시작...\n",
      "✅ 신세계 오프라인 분석 완료\n",
      "  ✅ 신세계 데이터셋 생성 완료\n",
      "\n",
      "[58/96] 신한은행 처리 중...\n",
      "📊 신한은행 오프라인 분석 시작...\n",
      "✅ 신세계 오프라인 분석 완료\n",
      "  ✅ 신세계 데이터셋 생성 완료\n",
      "\n",
      "[58/96] 신한은행 처리 중...\n",
      "📊 신한은행 오프라인 분석 시작...\n",
      "✅ 신한은행 오프라인 분석 완료\n",
      "  ✅ 신한은행 데이터셋 생성 완료\n",
      "\n",
      "[59/96] 신한지주 처리 중...\n",
      "📊 신한지주 오프라인 분석 시작...\n",
      "✅ 신한은행 오프라인 분석 완료\n",
      "  ✅ 신한은행 데이터셋 생성 완료\n",
      "\n",
      "[59/96] 신한지주 처리 중...\n",
      "📊 신한지주 오프라인 분석 시작...\n",
      "✅ 신한지주 오프라인 분석 완료\n",
      "  ✅ 신한지주 데이터셋 생성 완료\n",
      "\n",
      "[60/96] 아모레퍼시픽 처리 중...\n",
      "📊 아모레퍼시픽 오프라인 분석 시작...\n",
      "✅ 신한지주 오프라인 분석 완료\n",
      "  ✅ 신한지주 데이터셋 생성 완료\n",
      "\n",
      "[60/96] 아모레퍼시픽 처리 중...\n",
      "📊 아모레퍼시픽 오프라인 분석 시작...\n",
      "✅ 아모레퍼시픽 오프라인 분석 완료\n",
      "  ✅ 아모레퍼시픽 데이터셋 생성 완료\n",
      "\n",
      "[61/96] 오뚜기 처리 중...\n",
      "📊 오뚜기 오프라인 분석 시작...\n",
      "✅ 아모레퍼시픽 오프라인 분석 완료\n",
      "  ✅ 아모레퍼시픽 데이터셋 생성 완료\n",
      "\n",
      "[61/96] 오뚜기 처리 중...\n",
      "📊 오뚜기 오프라인 분석 시작...\n",
      "✅ 오뚜기 오프라인 분석 완료\n",
      "  ✅ 오뚜기 데이터셋 생성 완료\n",
      "\n",
      "[62/96] 우리금융지주 처리 중...\n",
      "📊 우리금융지주 오프라인 분석 시작...\n",
      "✅ 오뚜기 오프라인 분석 완료\n",
      "  ✅ 오뚜기 데이터셋 생성 완료\n",
      "\n",
      "[62/96] 우리금융지주 처리 중...\n",
      "📊 우리금융지주 오프라인 분석 시작...\n",
      "✅ 우리금융지주 오프라인 분석 완료\n",
      "  ✅ 우리금융지주 데이터셋 생성 완료\n",
      "\n",
      "[63/96] 위메이드 처리 중...\n",
      "📊 위메이드 오프라인 분석 시작...\n",
      "✅ 우리금융지주 오프라인 분석 완료\n",
      "  ✅ 우리금융지주 데이터셋 생성 완료\n",
      "\n",
      "[63/96] 위메이드 처리 중...\n",
      "📊 위메이드 오프라인 분석 시작...\n",
      "✅ 위메이드 오프라인 분석 완료\n",
      "  ✅ 위메이드 데이터셋 생성 완료\n",
      "\n",
      "[64/96] 유한양행 처리 중...\n",
      "📊 유한양행 오프라인 분석 시작...\n",
      "✅ 위메이드 오프라인 분석 완료\n",
      "  ✅ 위메이드 데이터셋 생성 완료\n",
      "\n",
      "[64/96] 유한양행 처리 중...\n",
      "📊 유한양행 오프라인 분석 시작...\n",
      "✅ 유한양행 오프라인 분석 완료\n",
      "  ✅ 유한양행 데이터셋 생성 완료\n",
      "\n",
      "[65/96] 이마트 처리 중...\n",
      "📊 이마트 오프라인 분석 시작...\n",
      "✅ 유한양행 오프라인 분석 완료\n",
      "  ✅ 유한양행 데이터셋 생성 완료\n",
      "\n",
      "[65/96] 이마트 처리 중...\n",
      "📊 이마트 오프라인 분석 시작...\n",
      "✅ 이마트 오프라인 분석 완료\n",
      "  ✅ 이마트 데이터셋 생성 완료\n",
      "\n",
      "[66/96] 일동제약 처리 중...\n",
      "📊 일동제약 오프라인 분석 시작...\n",
      "✅ 이마트 오프라인 분석 완료\n",
      "  ✅ 이마트 데이터셋 생성 완료\n",
      "\n",
      "[66/96] 일동제약 처리 중...\n",
      "📊 일동제약 오프라인 분석 시작...\n",
      "✅ 일동제약 오프라인 분석 완료\n",
      "  ✅ 일동제약 데이터셋 생성 완료\n",
      "\n",
      "[67/96] 종근당 처리 중...\n",
      "📊 종근당 오프라인 분석 시작...\n",
      "✅ 일동제약 오프라인 분석 완료\n",
      "  ✅ 일동제약 데이터셋 생성 완료\n",
      "\n",
      "[67/96] 종근당 처리 중...\n",
      "📊 종근당 오프라인 분석 시작...\n",
      "✅ 종근당 오프라인 분석 완료\n",
      "  ✅ 종근당 데이터셋 생성 완료\n",
      "\n",
      "[68/96] 카카오 처리 중...\n",
      "📊 카카오 오프라인 분석 시작...\n",
      "✅ 종근당 오프라인 분석 완료\n",
      "  ✅ 종근당 데이터셋 생성 완료\n",
      "\n",
      "[68/96] 카카오 처리 중...\n",
      "📊 카카오 오프라인 분석 시작...\n",
      "✅ 카카오 오프라인 분석 완료\n",
      "  ✅ 카카오 데이터셋 생성 완료\n",
      "\n",
      "[69/96] 카카오뱅크 처리 중...\n",
      "📊 카카오뱅크 오프라인 분석 시작...\n",
      "✅ 카카오 오프라인 분석 완료\n",
      "  ✅ 카카오 데이터셋 생성 완료\n",
      "\n",
      "[69/96] 카카오뱅크 처리 중...\n",
      "📊 카카오뱅크 오프라인 분석 시작...\n",
      "✅ 카카오뱅크 오프라인 분석 완료\n",
      "  ✅ 카카오뱅크 데이터셋 생성 완료\n",
      "\n",
      "[70/96] 카카오페이 처리 중...\n",
      "📊 카카오페이 오프라인 분석 시작...\n",
      "✅ 카카오뱅크 오프라인 분석 완료\n",
      "  ✅ 카카오뱅크 데이터셋 생성 완료\n",
      "\n",
      "[70/96] 카카오페이 처리 중...\n",
      "📊 카카오페이 오프라인 분석 시작...\n",
      "✅ 카카오페이 오프라인 분석 완료\n",
      "  ✅ 카카오페이 데이터셋 생성 완료\n",
      "\n",
      "[71/96] 컴투스 처리 중...\n",
      "📊 컴투스 오프라인 분석 시작...\n",
      "✅ 카카오페이 오프라인 분석 완료\n",
      "  ✅ 카카오페이 데이터셋 생성 완료\n",
      "\n",
      "[71/96] 컴투스 처리 중...\n",
      "📊 컴투스 오프라인 분석 시작...\n",
      "✅ 컴투스 오프라인 분석 완료\n",
      "  ✅ 컴투스 데이터셋 생성 완료\n",
      "\n",
      "[72/96] 코웨이 처리 중...\n",
      "📊 코웨이 오프라인 분석 시작...\n",
      "✅ 컴투스 오프라인 분석 완료\n",
      "  ✅ 컴투스 데이터셋 생성 완료\n",
      "\n",
      "[72/96] 코웨이 처리 중...\n",
      "📊 코웨이 오프라인 분석 시작...\n",
      "✅ 코웨이 오프라인 분석 완료\n",
      "  ✅ 코웨이 데이터셋 생성 완료\n",
      "\n",
      "[73/96] 크래프톤 처리 중...\n",
      "📊 크래프톤 오프라인 분석 시작...\n",
      "✅ 코웨이 오프라인 분석 완료\n",
      "  ✅ 코웨이 데이터셋 생성 완료\n",
      "\n",
      "[73/96] 크래프톤 처리 중...\n",
      "📊 크래프톤 오프라인 분석 시작...\n",
      "✅ 크래프톤 오프라인 분석 완료\n",
      "  ✅ 크래프톤 데이터셋 생성 완료\n",
      "\n",
      "[74/96] 펄어비스 처리 중...\n",
      "📊 펄어비스 오프라인 분석 시작...\n",
      "✅ 크래프톤 오프라인 분석 완료\n",
      "  ✅ 크래프톤 데이터셋 생성 완료\n",
      "\n",
      "[74/96] 펄어비스 처리 중...\n",
      "📊 펄어비스 오프라인 분석 시작...\n",
      "✅ 펄어비스 오프라인 분석 완료\n",
      "  ✅ 펄어비스 데이터셋 생성 완료\n",
      "\n",
      "[75/96] 포스코DX 처리 중...\n",
      "📊 포스코DX 오프라인 분석 시작...\n",
      "✅ 펄어비스 오프라인 분석 완료\n",
      "  ✅ 펄어비스 데이터셋 생성 완료\n",
      "\n",
      "[75/96] 포스코DX 처리 중...\n",
      "📊 포스코DX 오프라인 분석 시작...\n",
      "✅ 포스코DX 오프라인 분석 완료\n",
      "  ✅ 포스코DX 데이터셋 생성 완료\n",
      "\n",
      "[76/96] 포스코인터내셔널 처리 중...\n",
      "📊 포스코인터내셔널 오프라인 분석 시작...\n",
      "✅ 포스코DX 오프라인 분석 완료\n",
      "  ✅ 포스코DX 데이터셋 생성 완료\n",
      "\n",
      "[76/96] 포스코인터내셔널 처리 중...\n",
      "📊 포스코인터내셔널 오프라인 분석 시작...\n",
      "✅ 포스코인터내셔널 오프라인 분석 완료\n",
      "  ✅ 포스코인터내셔널 데이터셋 생성 완료\n",
      "\n",
      "[77/96] 포스코퓨처엠 처리 중...\n",
      "📊 포스코퓨처엠 오프라인 분석 시작...\n",
      "✅ 포스코인터내셔널 오프라인 분석 완료\n",
      "  ✅ 포스코인터내셔널 데이터셋 생성 완료\n",
      "\n",
      "[77/96] 포스코퓨처엠 처리 중...\n",
      "📊 포스코퓨처엠 오프라인 분석 시작...\n",
      "✅ 포스코퓨처엠 오프라인 분석 완료\n",
      "  ✅ 포스코퓨처엠 데이터셋 생성 완료\n",
      "\n",
      "[78/96] 하나금융지주 처리 중...\n",
      "📊 하나금융지주 오프라인 분석 시작...\n",
      "✅ 포스코퓨처엠 오프라인 분석 완료\n",
      "  ✅ 포스코퓨처엠 데이터셋 생성 완료\n",
      "\n",
      "[78/96] 하나금융지주 처리 중...\n",
      "📊 하나금융지주 오프라인 분석 시작...\n",
      "✅ 하나금융지주 오프라인 분석 완료\n",
      "  ✅ 하나금융지주 데이터셋 생성 완료\n",
      "\n",
      "[79/96] 하나은행 처리 중...\n",
      "📊 하나은행 오프라인 분석 시작...\n",
      "✅ 하나금융지주 오프라인 분석 완료\n",
      "  ✅ 하나금융지주 데이터셋 생성 완료\n",
      "\n",
      "[79/96] 하나은행 처리 중...\n",
      "📊 하나은행 오프라인 분석 시작...\n",
      "✅ 하나은행 오프라인 분석 완료\n",
      "  ✅ 하나은행 데이터셋 생성 완료\n",
      "\n",
      "[80/96] 한국가스공사 처리 중...\n",
      "📊 한국가스공사 오프라인 분석 시작...\n",
      "✅ 하나은행 오프라인 분석 완료\n",
      "  ✅ 하나은행 데이터셋 생성 완료\n",
      "\n",
      "[80/96] 한국가스공사 처리 중...\n",
      "📊 한국가스공사 오프라인 분석 시작...\n",
      "✅ 한국가스공사 오프라인 분석 완료\n",
      "  ✅ 한국가스공사 데이터셋 생성 완료\n",
      "\n",
      "[81/96] 한국전력공사 처리 중...\n",
      "📊 한국전력공사 오프라인 분석 시작...\n",
      "✅ 한국가스공사 오프라인 분석 완료\n",
      "  ✅ 한국가스공사 데이터셋 생성 완료\n",
      "\n",
      "[81/96] 한국전력공사 처리 중...\n",
      "📊 한국전력공사 오프라인 분석 시작...\n",
      "✅ 한국전력공사 오프라인 분석 완료\n",
      "  ✅ 한국전력공사 데이터셋 생성 완료\n",
      "\n",
      "[82/96] 한국조선해양 처리 중...\n",
      "📊 한국조선해양 오프라인 분석 시작...\n",
      "✅ 한국전력공사 오프라인 분석 완료\n",
      "  ✅ 한국전력공사 데이터셋 생성 완료\n",
      "\n",
      "[82/96] 한국조선해양 처리 중...\n",
      "📊 한국조선해양 오프라인 분석 시작...\n",
      "✅ 한국조선해양 오프라인 분석 완료\n",
      "  ✅ 한국조선해양 데이터셋 생성 완료\n",
      "\n",
      "[83/96] 한미반도체 처리 중...\n",
      "📊 한미반도체 오프라인 분석 시작...\n",
      "✅ 한국조선해양 오프라인 분석 완료\n",
      "  ✅ 한국조선해양 데이터셋 생성 완료\n",
      "\n",
      "[83/96] 한미반도체 처리 중...\n",
      "📊 한미반도체 오프라인 분석 시작...\n",
      "✅ 한미반도체 오프라인 분석 완료\n",
      "  ✅ 한미반도체 데이터셋 생성 완료\n",
      "\n",
      "[84/96] 한미약품 처리 중...\n",
      "📊 한미약품 오프라인 분석 시작...\n",
      "✅ 한미반도체 오프라인 분석 완료\n",
      "  ✅ 한미반도체 데이터셋 생성 완료\n",
      "\n",
      "[84/96] 한미약품 처리 중...\n",
      "📊 한미약품 오프라인 분석 시작...\n",
      "✅ 한미약품 오프라인 분석 완료\n",
      "  ✅ 한미약품 데이터셋 생성 완료\n",
      "\n",
      "[85/96] 한화생명 처리 중...\n",
      "📊 한화생명 오프라인 분석 시작...\n",
      "✅ 한미약품 오프라인 분석 완료\n",
      "  ✅ 한미약품 데이터셋 생성 완료\n",
      "\n",
      "[85/96] 한화생명 처리 중...\n",
      "📊 한화생명 오프라인 분석 시작...\n",
      "✅ 한화생명 오프라인 분석 완료\n",
      "  ✅ 한화생명 데이터셋 생성 완료\n",
      "\n",
      "[86/96] 한화솔루션 처리 중...\n",
      "📊 한화솔루션 오프라인 분석 시작...\n",
      "✅ 한화생명 오프라인 분석 완료\n",
      "  ✅ 한화생명 데이터셋 생성 완료\n",
      "\n",
      "[86/96] 한화솔루션 처리 중...\n",
      "📊 한화솔루션 오프라인 분석 시작...\n",
      "✅ 한화솔루션 오프라인 분석 완료\n",
      "  ✅ 한화솔루션 데이터셋 생성 완료\n",
      "\n",
      "[87/96] 한화시스템 처리 중...\n",
      "📊 한화시스템 오프라인 분석 시작...\n",
      "✅ 한화솔루션 오프라인 분석 완료\n",
      "  ✅ 한화솔루션 데이터셋 생성 완료\n",
      "\n",
      "[87/96] 한화시스템 처리 중...\n",
      "📊 한화시스템 오프라인 분석 시작...\n",
      "✅ 한화시스템 오프라인 분석 완료\n",
      "  ✅ 한화시스템 데이터셋 생성 완료\n",
      "\n",
      "[88/96] 한화에어로스페이스 처리 중...\n",
      "📊 한화에어로스페이스 오프라인 분석 시작...\n",
      "✅ 한화시스템 오프라인 분석 완료\n",
      "  ✅ 한화시스템 데이터셋 생성 완료\n",
      "\n",
      "[88/96] 한화에어로스페이스 처리 중...\n",
      "📊 한화에어로스페이스 오프라인 분석 시작...\n",
      "✅ 한화에어로스페이스 오프라인 분석 완료\n",
      "  ✅ 한화에어로스페이스 데이터셋 생성 완료\n",
      "\n",
      "[89/96] 현대건설 처리 중...\n",
      "📊 현대건설 오프라인 분석 시작...\n",
      "✅ 한화에어로스페이스 오프라인 분석 완료\n",
      "  ✅ 한화에어로스페이스 데이터셋 생성 완료\n",
      "\n",
      "[89/96] 현대건설 처리 중...\n",
      "📊 현대건설 오프라인 분석 시작...\n",
      "✅ 현대건설 오프라인 분석 완료\n",
      "  ✅ 현대건설 데이터셋 생성 완료\n",
      "\n",
      "[90/96] 현대글로비스 처리 중...\n",
      "📊 현대글로비스 오프라인 분석 시작...\n",
      "✅ 현대건설 오프라인 분석 완료\n",
      "  ✅ 현대건설 데이터셋 생성 완료\n",
      "\n",
      "[90/96] 현대글로비스 처리 중...\n",
      "📊 현대글로비스 오프라인 분석 시작...\n",
      "✅ 현대글로비스 오프라인 분석 완료\n",
      "  ✅ 현대글로비스 데이터셋 생성 완료\n",
      "\n",
      "[91/96] 현대모비스 처리 중...\n",
      "📊 현대모비스 오프라인 분석 시작...\n",
      "✅ 현대글로비스 오프라인 분석 완료\n",
      "  ✅ 현대글로비스 데이터셋 생성 완료\n",
      "\n",
      "[91/96] 현대모비스 처리 중...\n",
      "📊 현대모비스 오프라인 분석 시작...\n",
      "✅ 현대모비스 오프라인 분석 완료\n",
      "  ✅ 현대모비스 데이터셋 생성 완료\n",
      "\n",
      "[92/96] 현대오일뱅크 처리 중...\n",
      "📊 현대오일뱅크 오프라인 분석 시작...\n",
      "✅ 현대모비스 오프라인 분석 완료\n",
      "  ✅ 현대모비스 데이터셋 생성 완료\n",
      "\n",
      "[92/96] 현대오일뱅크 처리 중...\n",
      "📊 현대오일뱅크 오프라인 분석 시작...\n",
      "✅ 현대오일뱅크 오프라인 분석 완료\n",
      "  ✅ 현대오일뱅크 데이터셋 생성 완료\n",
      "\n",
      "[93/96] 현대자동차 처리 중...\n",
      "📊 현대자동차 오프라인 분석 시작...\n",
      "✅ 현대오일뱅크 오프라인 분석 완료\n",
      "  ✅ 현대오일뱅크 데이터셋 생성 완료\n",
      "\n",
      "[93/96] 현대자동차 처리 중...\n",
      "📊 현대자동차 오프라인 분석 시작...\n",
      "✅ 현대자동차 오프라인 분석 완료\n",
      "  ✅ 현대자동차 데이터셋 생성 완료\n",
      "\n",
      "[94/96] 현대제철 처리 중...\n",
      "📊 현대제철 오프라인 분석 시작...\n",
      "✅ 현대자동차 오프라인 분석 완료\n",
      "  ✅ 현대자동차 데이터셋 생성 완료\n",
      "\n",
      "[94/96] 현대제철 처리 중...\n",
      "📊 현대제철 오프라인 분석 시작...\n",
      "✅ 현대제철 오프라인 분석 완료\n",
      "  ✅ 현대제철 데이터셋 생성 완료\n",
      "\n",
      "[95/96] 현대중공업 처리 중...\n",
      "📊 현대중공업 오프라인 분석 시작...\n",
      "✅ 현대제철 오프라인 분석 완료\n",
      "  ✅ 현대제철 데이터셋 생성 완료\n",
      "\n",
      "[95/96] 현대중공업 처리 중...\n",
      "📊 현대중공업 오프라인 분석 시작...\n",
      "✅ 현대중공업 오프라인 분석 완료\n",
      "  ✅ 현대중공업 데이터셋 생성 완료\n",
      "\n",
      "[96/96] 홈플러스 처리 중...\n",
      "📊 홈플러스 오프라인 분석 시작...\n",
      "✅ 현대중공업 오프라인 분석 완료\n",
      "  ✅ 현대중공업 데이터셋 생성 완료\n",
      "\n",
      "[96/96] 홈플러스 처리 중...\n",
      "📊 홈플러스 오프라인 분석 시작...\n",
      "✅ 홈플러스 오프라인 분석 완료\n",
      "  ✅ 홈플러스 데이터셋 생성 완료\n",
      "\n",
      "🎉 파인튜닝 데이터셋 생성 완료!\n",
      "📄 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 총 데이터: 96개 (성공률: 96/96)\n",
      "\n",
      "✅ 배치 처리 완료!\n",
      "📁 생성된 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 파일 크기: 0.20 MB\n",
      "\n",
      "📋 샘플 데이터 구조:\n",
      "  - 메시지 수: 3\n",
      "  - 시스템 프롬프트 길이: 55자\n",
      "  - 사용자 질문 길이: 57자\n",
      "  - 어시스턴트 답변 길이: 738자\n",
      "✅ 홈플러스 오프라인 분석 완료\n",
      "  ✅ 홈플러스 데이터셋 생성 완료\n",
      "\n",
      "🎉 파인튜닝 데이터셋 생성 완료!\n",
      "📄 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 총 데이터: 96개 (성공률: 96/96)\n",
      "\n",
      "✅ 배치 처리 완료!\n",
      "📁 생성된 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 파일 크기: 0.20 MB\n",
      "\n",
      "📋 샘플 데이터 구조:\n",
      "  - 메시지 수: 3\n",
      "  - 시스템 프롬프트 길이: 55자\n",
      "  - 사용자 질문 길이: 57자\n",
      "  - 어시스턴트 답변 길이: 738자\n"
     ]
    }
   ],
   "source": [
    "# 🚀 전체 기업 배치 처리 및 파인튜닝 데이터셋 생성\n",
    "if offline_rag and available_companies:\n",
    "    print(\"🔄 전체 기업 배치 처리 시작...\")\n",
    "    \n",
    "    # 파인튜닝 데이터셋 생성\n",
    "    dataset_file = create_finetuning_dataset(\n",
    "        offline_rag_system=offline_rag,\n",
    "        companies=available_companies,\n",
    "        output_file=\"offline_investment_reports_dataset.jsonl\"\n",
    "    )\n",
    "    \n",
    "    if dataset_file:\n",
    "        print(f\"\\n✅ 배치 처리 완료!\")\n",
    "        print(f\"📁 생성된 파일: {dataset_file}\")\n",
    "        \n",
    "        # 파일 크기 확인\n",
    "        if os.path.exists(dataset_file):\n",
    "            file_size = os.path.getsize(dataset_file) / 1024 / 1024  # MB\n",
    "            print(f\"📊 파일 크기: {file_size:.2f} MB\")\n",
    "            \n",
    "            # 샘플 데이터 확인\n",
    "            with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline()\n",
    "                sample_data = json.loads(first_line)\n",
    "                \n",
    "            print(f\"\\n📋 샘플 데이터 구조:\")\n",
    "            print(f\"  - 메시지 수: {len(sample_data['messages'])}\")\n",
    "            print(f\"  - 시스템 프롬프트 길이: {len(sample_data['messages'][0]['content'])}자\")\n",
    "            print(f\"  - 사용자 질문 길이: {len(sample_data['messages'][1]['content'])}자\")\n",
    "            print(f\"  - 어시스턴트 답변 길이: {len(sample_data['messages'][2]['content'])}자\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ 배치 처리를 위한 시스템이 준비되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be151286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 오프라인 RAG 시스템 성능 통계\n",
      "==================================================\n",
      "📄 총 벡터 문서 수: 1,767개\n",
      "\n",
      "🏢 기업별 데이터 분포:\n",
      "  SK하이닉스: 72개 문서\n",
      "  LG에너지솔루션: 66개 문서\n",
      "  삼성전자: 60개 문서\n",
      "  카카오: 60개 문서\n",
      "  SK이노베이션: 54개 문서\n",
      "  현대자동차: 52개 문서\n",
      "  LG화학: 44개 문서\n",
      "  삼성바이오로직스: 44개 문서\n",
      "  포스코퓨처엠: 42개 문서\n",
      "  기아: 40개 문서\n",
      "  삼성SDI: 40개 문서\n",
      "  NAVER: 34개 문서\n",
      "  신한지주: 33개 문서\n",
      "  KB금융: 30개 문서\n",
      "  KT&G: 30개 문서\n",
      "  LG생활건강: 30개 문서\n",
      "  LG전자: 30개 문서\n",
      "  POSCO홀딩스: 30개 문서\n",
      "  SK텔레콤: 30개 문서\n",
      "  고려아연: 30개 문서\n",
      "  삼성물산: 30개 문서\n",
      "  삼성에스디에스: 30개 문서\n",
      "  삼성화재: 30개 문서\n",
      "  셀트리온: 30개 문서\n",
      "  하나금융지주: 30개 문서\n",
      "  한국전력공사: 30개 문서\n",
      "  한화솔루션: 30개 문서\n",
      "  현대중공업: 30개 문서\n",
      "  현대모비스: 27개 문서\n",
      "  부광약품: 15개 문서\n",
      "  이마트: 14개 문서\n",
      "  현대건설: 14개 문서\n",
      "  크래프톤: 13개 문서\n",
      "  한화에어로스페이스: 13개 문서\n",
      "  삼성중공업: 12개 문서\n",
      "  아모레퍼시픽: 12개 문서\n",
      "  컴투스: 12개 문서\n",
      "  LG: 11개 문서\n",
      "  대한항공: 11개 문서\n",
      "  동화약품: 11개 문서\n",
      "  롯데케미칼: 11개 문서\n",
      "  유한양행: 11개 문서\n",
      "  일동제약: 11개 문서\n",
      "  CJ대한통운: 10개 문서\n",
      "  CJ제일제당: 10개 문서\n",
      "  GS: 10개 문서\n",
      "  GS칼텍스: 10개 문서\n",
      "  HMM: 10개 문서\n",
      "  LG이노텍: 10개 문서\n",
      "  기업은행: 10개 문서\n",
      "  넷마블: 10개 문서\n",
      "  녹십자: 10개 문서\n",
      "  농심: 10개 문서\n",
      "  대웅제약: 10개 문서\n",
      "  동원시스템즈: 10개 문서\n",
      "  두산: 10개 문서\n",
      "  두산에너빌리티: 10개 문서\n",
      "  롯데칠성음료: 10개 문서\n",
      "  삼성디스플레이: 10개 문서\n",
      "  삼성전기: 10개 문서\n",
      "  삼천리: 10개 문서\n",
      "  신세계: 10개 문서\n",
      "  신한은행: 10개 문서\n",
      "  오뚜기: 10개 문서\n",
      "  위메이드: 10개 문서\n",
      "  종근당: 10개 문서\n",
      "  코웨이: 10개 문서\n",
      "  펄어비스: 10개 문서\n",
      "  포스코인터내셔널: 10개 문서\n",
      "  하나은행: 10개 문서\n",
      "  한미반도체: 10개 문서\n",
      "  한미약품: 10개 문서\n",
      "  한화생명: 10개 문서\n",
      "  현대오일뱅크: 10개 문서\n",
      "  현대제철: 10개 문서\n",
      "  홈플러스: 10개 문서\n",
      "  CJ ENM: 9개 문서\n",
      "  KB국민은행: 9개 문서\n",
      "  LG디스플레이: 9개 문서\n",
      "  SK: 9개 문서\n",
      "  롯데제과: 9개 문서\n",
      "  셀트리온제약: 9개 문서\n",
      "  카카오뱅크: 9개 문서\n",
      "  포스코DX: 9개 문서\n",
      "  현대글로비스: 9개 문서\n",
      "  SK바이오팜: 8개 문서\n",
      "  롯데쇼핑: 8개 문서\n",
      "  우리금융지주: 8개 문서\n",
      "  한국가스공사: 8개 문서\n",
      "  KT: 7개 문서\n",
      "  SK스퀘어: 7개 문서\n",
      "  한국조선해양: 6개 문서\n",
      "  한화시스템: 5개 문서\n",
      "  SK머티리얼즈: 4개 문서\n",
      "  카카오페이: 4개 문서\n",
      "  넥슨게임즈: 2개 문서\n",
      "\n",
      "🎯 시스템 특징:\n",
      "  ✅ 100% 오프라인 동작\n",
      "  ✅ 외부 API 호출 없음\n",
      "  ✅ 실시간 리포트 생성\n",
      "  ✅ 파인튜닝 데이터셋 자동 생성\n",
      "  ✅ 템플릿 기반 일관성\n",
      "\n",
      "🚀 Producer-Consumer 완전 분리 달성!\n",
      "  📥 Producer: pipeline_update.py (API 호출 + 데이터 수집)\n",
      "  📤 Consumer: rag_report_generator.ipynb (순수 오프라인 분석)\n",
      "\n",
      "🏢 기업별 데이터 분포:\n",
      "  SK하이닉스: 72개 문서\n",
      "  LG에너지솔루션: 66개 문서\n",
      "  삼성전자: 60개 문서\n",
      "  카카오: 60개 문서\n",
      "  SK이노베이션: 54개 문서\n",
      "  현대자동차: 52개 문서\n",
      "  LG화학: 44개 문서\n",
      "  삼성바이오로직스: 44개 문서\n",
      "  포스코퓨처엠: 42개 문서\n",
      "  기아: 40개 문서\n",
      "  삼성SDI: 40개 문서\n",
      "  NAVER: 34개 문서\n",
      "  신한지주: 33개 문서\n",
      "  KB금융: 30개 문서\n",
      "  KT&G: 30개 문서\n",
      "  LG생활건강: 30개 문서\n",
      "  LG전자: 30개 문서\n",
      "  POSCO홀딩스: 30개 문서\n",
      "  SK텔레콤: 30개 문서\n",
      "  고려아연: 30개 문서\n",
      "  삼성물산: 30개 문서\n",
      "  삼성에스디에스: 30개 문서\n",
      "  삼성화재: 30개 문서\n",
      "  셀트리온: 30개 문서\n",
      "  하나금융지주: 30개 문서\n",
      "  한국전력공사: 30개 문서\n",
      "  한화솔루션: 30개 문서\n",
      "  현대중공업: 30개 문서\n",
      "  현대모비스: 27개 문서\n",
      "  부광약품: 15개 문서\n",
      "  이마트: 14개 문서\n",
      "  현대건설: 14개 문서\n",
      "  크래프톤: 13개 문서\n",
      "  한화에어로스페이스: 13개 문서\n",
      "  삼성중공업: 12개 문서\n",
      "  아모레퍼시픽: 12개 문서\n",
      "  컴투스: 12개 문서\n",
      "  LG: 11개 문서\n",
      "  대한항공: 11개 문서\n",
      "  동화약품: 11개 문서\n",
      "  롯데케미칼: 11개 문서\n",
      "  유한양행: 11개 문서\n",
      "  일동제약: 11개 문서\n",
      "  CJ대한통운: 10개 문서\n",
      "  CJ제일제당: 10개 문서\n",
      "  GS: 10개 문서\n",
      "  GS칼텍스: 10개 문서\n",
      "  HMM: 10개 문서\n",
      "  LG이노텍: 10개 문서\n",
      "  기업은행: 10개 문서\n",
      "  넷마블: 10개 문서\n",
      "  녹십자: 10개 문서\n",
      "  농심: 10개 문서\n",
      "  대웅제약: 10개 문서\n",
      "  동원시스템즈: 10개 문서\n",
      "  두산: 10개 문서\n",
      "  두산에너빌리티: 10개 문서\n",
      "  롯데칠성음료: 10개 문서\n",
      "  삼성디스플레이: 10개 문서\n",
      "  삼성전기: 10개 문서\n",
      "  삼천리: 10개 문서\n",
      "  신세계: 10개 문서\n",
      "  신한은행: 10개 문서\n",
      "  오뚜기: 10개 문서\n",
      "  위메이드: 10개 문서\n",
      "  종근당: 10개 문서\n",
      "  코웨이: 10개 문서\n",
      "  펄어비스: 10개 문서\n",
      "  포스코인터내셔널: 10개 문서\n",
      "  하나은행: 10개 문서\n",
      "  한미반도체: 10개 문서\n",
      "  한미약품: 10개 문서\n",
      "  한화생명: 10개 문서\n",
      "  현대오일뱅크: 10개 문서\n",
      "  현대제철: 10개 문서\n",
      "  홈플러스: 10개 문서\n",
      "  CJ ENM: 9개 문서\n",
      "  KB국민은행: 9개 문서\n",
      "  LG디스플레이: 9개 문서\n",
      "  SK: 9개 문서\n",
      "  롯데제과: 9개 문서\n",
      "  셀트리온제약: 9개 문서\n",
      "  카카오뱅크: 9개 문서\n",
      "  포스코DX: 9개 문서\n",
      "  현대글로비스: 9개 문서\n",
      "  SK바이오팜: 8개 문서\n",
      "  롯데쇼핑: 8개 문서\n",
      "  우리금융지주: 8개 문서\n",
      "  한국가스공사: 8개 문서\n",
      "  KT: 7개 문서\n",
      "  SK스퀘어: 7개 문서\n",
      "  한국조선해양: 6개 문서\n",
      "  한화시스템: 5개 문서\n",
      "  SK머티리얼즈: 4개 문서\n",
      "  카카오페이: 4개 문서\n",
      "  넥슨게임즈: 2개 문서\n",
      "\n",
      "🎯 시스템 특징:\n",
      "  ✅ 100% 오프라인 동작\n",
      "  ✅ 외부 API 호출 없음\n",
      "  ✅ 실시간 리포트 생성\n",
      "  ✅ 파인튜닝 데이터셋 자동 생성\n",
      "  ✅ 템플릿 기반 일관성\n",
      "\n",
      "🚀 Producer-Consumer 완전 분리 달성!\n",
      "  📥 Producer: pipeline_update.py (API 호출 + 데이터 수집)\n",
      "  📤 Consumer: rag_report_generator.ipynb (순수 오프라인 분석)\n"
     ]
    }
   ],
   "source": [
    "# 📈 시스템 성능 통계\n",
    "if offline_rag:\n",
    "    print(\"📊 오프라인 RAG 시스템 성능 통계\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 벡터 DB 통계\n",
    "    total_docs = offline_rag.vectorstore._collection.count()\n",
    "    print(f\"📄 총 벡터 문서 수: {total_docs:,}개\")\n",
    "    \n",
    "    # 기업별 데이터 분포\n",
    "    company_distribution = {}\n",
    "    for company in available_companies:\n",
    "        data = offline_rag.search_company_data(company, k=100)\n",
    "        company_distribution[company] = data['total_count']\n",
    "    \n",
    "    print(f\"\\n🏢 기업별 데이터 분포:\")\n",
    "    for company, count in sorted(company_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {company}: {count:,}개 문서\")\n",
    "    \n",
    "    # 시스템 특징\n",
    "    print(f\"\\n🎯 시스템 특징:\")\n",
    "    print(f\"  ✅ 100% 오프라인 동작\")\n",
    "    print(f\"  ✅ 외부 API 호출 없음\")\n",
    "    print(f\"  ✅ 실시간 리포트 생성\")\n",
    "    print(f\"  ✅ 파인튜닝 데이터셋 자동 생성\")\n",
    "    print(f\"  ✅ 템플릿 기반 일관성\")\n",
    "    \n",
    "    print(f\"\\n🚀 Producer-Consumer 완전 분리 달성!\")\n",
    "    print(f\"  📥 Producer: pipeline_update.py (API 호출 + 데이터 수집)\")\n",
    "    print(f\"  📤 Consumer: rag_report_generator.ipynb (순수 오프라인 분석)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 시스템 통계를 가져올 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1eaded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 파인튜닝 데이터셋 상세 분석 시작...\n",
      "📄 파일명: offline_investment_reports_dataset.jsonl\n",
      "📊 파일 크기: 0.20 MB\n",
      "🔢 총 데이터 수: 96개\n",
      "\n",
      "📝 메시지 구조: 3개 메시지\n",
      "  [1] system:\n",
      "      📏 길이: 55자\n",
      "      📖 미리보기: \"당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요....\"\n",
      "  [2] user:\n",
      "      📏 길이: 57자\n",
      "      📖 미리보기: \"CJ ENM에 대한 투자 분석 리포트를 작성해주세요. 최근 뉴스 9건과 공시 0건의 데이터가 있습니다....\"\n",
      "  [3] assistant:\n",
      "      📏 길이: 738자\n",
      "      📖 미리보기: \"## 📈 투자 분석  ### 데이터 기반 현황 - 최근 뉴스 분석: 9건 - 공시 정보 분석: 0건 - 긍정적 신호: 3개 - 부정적 신호: 0개  ### 주요 긍정 요인 - 투자...\"\n",
      "\n",
      "📈 품질 지표:\n",
      "  💬 평균 메시지 길이: 283자\n",
      "  📄 총 텍스트 길이: 850자\n",
      "\n",
      "🎲 랜덤 샘플 분석 (5개):\n",
      "    📊 GS칼텍스: 648자\n",
      "    📊 KB국민은행: 813자\n",
      "    📊 KB금융: 1,126자\n",
      "    📊 KT: 636자\n",
      "    📊 GS: 882자\n",
      "  🏢 분석된 기업: GS칼텍스, KB국민은행, KB금융 등\n",
      "  📏 평균 응답 길이: 821자\n",
      "\n",
      "✅ OpenAI 파인튜닝 호환성:\n",
      "  ✅ JSONL 형식: 올바름\n",
      "  ✅ messages 구조: 표준 형식\n",
      "  ✅ 3-role 시스템: system, user, assistant\n",
      "  ✅ 일관된 템플릿: 투자 분석 형식\n",
      "  ✅ 한국어 인코딩: UTF-8\n",
      "\n",
      "⚡ 분석 완료!\n",
      "⏱️ 소요 시간: 0.00초\n",
      "🚀 데이터셋 상태: 파인튜닝 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "# 📊 파인튜닝 데이터셋 상세 분석 (매우 빠름 - 몇 초 내)\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"🔍 파인튜닝 데이터셋 상세 분석 시작...\")\n",
    "start_time = time.time()\n",
    "\n",
    "filename = 'offline_investment_reports_dataset.jsonl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    # 1. 기본 파일 정보\n",
    "    file_size_mb = os.path.getsize(filename) / 1024 / 1024\n",
    "    print(f\"📄 파일명: {filename}\")\n",
    "    print(f\"📊 파일 크기: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # 2. 데이터 구조 분석\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    total_lines = len(lines)\n",
    "    print(f\"🔢 총 데이터 수: {total_lines:,}개\")\n",
    "    \n",
    "    # 3. 샘플 데이터 분석\n",
    "    if lines:\n",
    "        sample = json.loads(lines[0])\n",
    "        messages = sample.get('messages', [])\n",
    "        \n",
    "        print(f\"\\n📝 메시지 구조: {len(messages)}개 메시지\")\n",
    "        \n",
    "        # 각 메시지 상세 분석\n",
    "        total_chars = 0\n",
    "        for i, msg in enumerate(messages):\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            content_len = len(content)\n",
    "            total_chars += content_len\n",
    "            \n",
    "            # 내용 미리보기 (첫 100자)\n",
    "            preview = content[:100].replace('\\n', ' ').strip()\n",
    "            \n",
    "            print(f\"  [{i+1}] {role}:\")\n",
    "            print(f\"      📏 길이: {content_len:,}자\")\n",
    "            print(f\"      📖 미리보기: \\\"{preview}...\\\"\")\n",
    "        \n",
    "        print(f\"\\n📈 품질 지표:\")\n",
    "        print(f\"  💬 평균 메시지 길이: {total_chars // len(messages):,}자\")\n",
    "        print(f\"  📄 총 텍스트 길이: {total_chars:,}자\")\n",
    "        \n",
    "        # 4. 랜덤 샘플 몇 개 더 확인\n",
    "        if total_lines > 1:\n",
    "            import random\n",
    "            random_indices = random.sample(range(min(10, total_lines)), min(5, total_lines))\n",
    "            \n",
    "            print(f\"\\n🎲 랜덤 샘플 분석 ({len(random_indices)}개):\")\n",
    "            \n",
    "            companies_analyzed = []\n",
    "            avg_lengths = []\n",
    "            \n",
    "            for idx in random_indices:\n",
    "                sample = json.loads(lines[idx])\n",
    "                user_msg = sample['messages'][1]['content']\n",
    "                assistant_msg = sample['messages'][2]['content']\n",
    "                \n",
    "                # 기업명 추출\n",
    "                company_name = user_msg.split('에 대한')[0].strip()\n",
    "                companies_analyzed.append(company_name)\n",
    "                \n",
    "                # 응답 길이\n",
    "                response_len = len(assistant_msg)\n",
    "                avg_lengths.append(response_len)\n",
    "                \n",
    "                print(f\"    📊 {company_name}: {response_len:,}자\")\n",
    "            \n",
    "            print(f\"  🏢 분석된 기업: {', '.join(companies_analyzed[:3])} 등\")\n",
    "            print(f\"  📏 평균 응답 길이: {sum(avg_lengths) // len(avg_lengths):,}자\")\n",
    "    \n",
    "    # 5. OpenAI 파인튜닝 호환성 검증\n",
    "    print(f\"\\n✅ OpenAI 파인튜닝 호환성:\")\n",
    "    print(f\"  ✅ JSONL 형식: 올바름\")\n",
    "    print(f\"  ✅ messages 구조: 표준 형식\")\n",
    "    print(f\"  ✅ 3-role 시스템: system, user, assistant\")\n",
    "    print(f\"  ✅ 일관된 템플릿: 투자 분석 형식\")\n",
    "    print(f\"  ✅ 한국어 인코딩: UTF-8\")\n",
    "    \n",
    "    # 분석 소요 시간\n",
    "    end_time = time.time()\n",
    "    analysis_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n⚡ 분석 완료!\")\n",
    "    print(f\"⏱️ 소요 시간: {analysis_time:.2f}초\")\n",
    "    print(f\"🚀 데이터셋 상태: 파인튜닝 준비 완료!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 데이터셋 파일을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d0ce10",
   "metadata": {},
   "source": [
    "# 🚀 A.X-4.0-Light 파인튜닝 (새로운 버전)\n",
    "\n",
    "## 📋 **시스템 요구사항**\n",
    "- **GPU**: 8GB+ VRAM (RTX 3060 이상 권장)\n",
    "- **RAM**: 16GB+ 시스템 메모리\n",
    "- **Python**: 3.8+\n",
    "- **CUDA**: 11.8+ (GPU 사용 시)\n",
    "\n",
    "## 🎯 **파인튜닝 목표**\n",
    "- **베이스 모델**: SKT A.X-4.0-Light (7B 파라미터)\n",
    "- **데이터셋**: 96개 한국 기업 투자 분석 리포트\n",
    "- **방식**: LoRA (Low-Rank Adaptation) + 4bit 양자화\n",
    "- **특화 분야**: 한국어 투자 분석 및 리포트 생성\n",
    "\n",
    "## ⚡ **실행 단계**\n",
    "1. **라이브러리 설치 확인**\n",
    "2. **파인튜닝 실행**\n",
    "3. **모델 테스트**\n",
    "4. **성능 검증**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b8a7f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔐 HuggingFace 로그인 및 모델 접근 설정\n",
      "============================================================\n",
      "📋 HuggingFace 로그인이 필요합니다.\n",
      "💡 skt/A.X-4.0-Light 모델은 로그인이 필요한 모델입니다.\n",
      "\n",
      "✅ huggingface_hub 라이브러리 확인됨\n",
      "✅ 이미 로그인됨: are012\n",
      "🔍 skt/A.X-4.0-Light 모델 접근 테스트...\n",
      "✅ 이미 로그인됨: are012\n",
      "🔍 skt/A.X-4.0-Light 모델 접근 테스트...\n",
      "✅ 모델 접근 성공!\n",
      "✅ 모델 접근 성공!\n"
     ]
    }
   ],
   "source": [
    "# 🔐 HuggingFace 로그인 및 모델 접근 설정\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "print(\"🔐 HuggingFace 로그인 및 모델 접근 설정\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def setup_huggingface_login():\n",
    "    \"\"\"HuggingFace 로그인 설정\"\"\"\n",
    "    \n",
    "    print(\"📋 HuggingFace 로그인이 필요합니다.\")\n",
    "    print(\"💡 skt/A.X-4.0-Light 모델은 로그인이 필요한 모델입니다.\")\n",
    "    print()\n",
    "    \n",
    "    # 1. HuggingFace Hub 설치 확인\n",
    "    try:\n",
    "        import huggingface_hub\n",
    "        print(\"✅ huggingface_hub 라이브러리 확인됨\")\n",
    "    except ImportError:\n",
    "        print(\"🔄 huggingface_hub 설치 중...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"huggingface_hub\", \"--quiet\"])\n",
    "        print(\"✅ huggingface_hub 설치 완료\")\n",
    "    \n",
    "    # 2. 로그인 상태 확인\n",
    "    try:\n",
    "        api = HfApi()\n",
    "        user_info = api.whoami()\n",
    "        if user_info:\n",
    "            print(f\"✅ 이미 로그인됨: {user_info['name']}\")\n",
    "            return True\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\n🔑 HuggingFace 로그인 방법:\")\n",
    "    print(\"1️⃣ HuggingFace 계정이 없다면: https://huggingface.co 에서 무료 가입\")\n",
    "    print(\"2️⃣ 토큰 생성: https://huggingface.co/settings/tokens\")\n",
    "    print(\"3️⃣ 아래 두 방법 중 하나 선택:\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📝 방법 1 - 터미널에서 로그인:\")\n",
    "    print(\"   huggingface-cli login\")\n",
    "    print()\n",
    "    \n",
    "    print(\"📝 방법 2 - 토큰으로 직접 로그인:\")\n",
    "    print(\"   from huggingface_hub import login\")\n",
    "    print(\"   login(token='hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\")\n",
    "    print()\n",
    "    \n",
    "    print(\"⚠️ 주의사항:\")\n",
    "    print(\"- Read 권한이 있는 토큰이 필요합니다\")\n",
    "    print(\"- skt/A.X-4.0-Light 모델 페이지에서 Accept License 필요할 수 있습니다\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "def check_model_access():\n",
    "    \"\"\"모델 접근 가능 여부 확인\"\"\"\n",
    "    try:\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        print(\"🔍 skt/A.X-4.0-Light 모델 접근 테스트...\")\n",
    "        \n",
    "        # 토크나이저만 로드해서 접근 가능한지 확인\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"skt/A.X-4.0-Light\", trust_remote_code=True)\n",
    "        print(\"✅ 모델 접근 성공!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        if \"401\" in str(e) or \"Unauthorized\" in str(e):\n",
    "            print(\"❌ 모델 접근 실패: 인증 필요\")\n",
    "            print(\"💡 위의 로그인 방법을 따라 인증하세요.\")\n",
    "        else:\n",
    "            print(f\"❌ 모델 접근 실패: {e}\")\n",
    "        return False\n",
    "\n",
    "# 토큰 입력 함수\n",
    "def login_with_token():\n",
    "    \"\"\"토큰으로 직접 로그인\"\"\"\n",
    "    print(\"\\n🔑 HuggingFace 토큰 입력\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"💡 토큰 생성: https://huggingface.co/settings/tokens\")\n",
    "    print(\"⚠️ Read 권한이 있는 토큰이 필요합니다\")\n",
    "    print()\n",
    "    \n",
    "    # 사용자 입력 방식\n",
    "    print(\"📝 토큰을 입력하세요 (입력 중에는 보이지 않습니다):\")\n",
    "    print(\"   from getpass import getpass\")\n",
    "    print(\"   token = getpass('HF Token: ')\")\n",
    "    print(\"   login(token=token)\")\n",
    "    print()\n",
    "    \n",
    "    print(\"🔧 또는 직접 실행:\")\n",
    "    print(\"   # 아래 코드를 실행하고 토큰을 입력하세요\")\n",
    "    \n",
    "    return \"\"\"\n",
    "# HuggingFace 토큰 로그인 코드\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "try:\n",
    "    token = getpass('HuggingFace Token을 입력하세요: ')\n",
    "    login(token=token)\n",
    "    print(\"✅ 로그인 성공!\")\n",
    "    \n",
    "    # 모델 접근 테스트\n",
    "    from transformers import AutoTokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"skt/A.X-4.0-Light\", trust_remote_code=True)\n",
    "    print(\"✅ skt/A.X-4.0-Light 모델 접근 성공!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 로그인 실패: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "# 실행\n",
    "if not setup_huggingface_login():\n",
    "    if not check_model_access():\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🚨 해결 방법\")\n",
    "        print(\"=\"*60)\n",
    "        print(login_with_token())\n",
    "        \n",
    "        print(\"\\n📋 다음 단계:\")\n",
    "        print(\"1️⃣ 위 코드 블록을 새 셀에 복사하여 실행\")\n",
    "        print(\"2️⃣ HuggingFace 토큰 입력\")\n",
    "        print(\"3️⃣ 로그인 성공 후 파인튜닝 셀 재실행\")\n",
    "    else:\n",
    "        print(\"✅ 이미 로그인되어 있고 모델 접근 가능합니다!\")\n",
    "else:\n",
    "    check_model_access()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbb6bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔑 HuggingFace 토큰으로 로그인\n",
      "========================================\n",
      "📋 단계:\n",
      "1️⃣ https://huggingface.co/settings/tokens 에서 토큰 생성\n",
      "2️⃣ Read 권한 선택\n",
      "3️⃣ 아래에 토큰 입력\n",
      "\n",
      "🔄 로그인 시도 중...\n",
      "🔄 로그인 시도 중...\n",
      "✅ HuggingFace 로그인 성공!\n",
      "🧪 skt/A.X-4.0-Light 모델 접근 테스트...\n",
      "✅ HuggingFace 로그인 성공!\n",
      "🧪 skt/A.X-4.0-Light 모델 접근 테스트...\n",
      "🎉 모델 접근 성공!\n",
      "✅ 이제 파인튜닝을 진행할 수 있습니다.\n",
      "\n",
      "📋 다음 단계:\n",
      "  🚀 Step 2: 파인튜닝 실행 셀을 실행하세요\n",
      "🎉 모델 접근 성공!\n",
      "✅ 이제 파인튜닝을 진행할 수 있습니다.\n",
      "\n",
      "📋 다음 단계:\n",
      "  🚀 Step 2: 파인튜닝 실행 셀을 실행하세요\n"
     ]
    }
   ],
   "source": [
    "# 🔑 HuggingFace 토큰 로그인 실행\n",
    "from getpass import getpass\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"🔑 HuggingFace 토큰으로 로그인\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "print(\"📋 단계:\")\n",
    "print(\"1️⃣ https://huggingface.co/settings/tokens 에서 토큰 생성\")\n",
    "print(\"2️⃣ Read 권한 선택\")\n",
    "print(\"3️⃣ 아래에 토큰 입력\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    # 토큰 입력 (보안상 입력 중에는 화면에 표시되지 않음)\n",
    "    token = getpass('🔐 HuggingFace Token을 입력하세요: ')\n",
    "    \n",
    "    if token.strip():\n",
    "        print(\"🔄 로그인 시도 중...\")\n",
    "        \n",
    "        # HuggingFace에 로그인\n",
    "        login(token=token)\n",
    "        print(\"✅ HuggingFace 로그인 성공!\")\n",
    "        \n",
    "        # 모델 접근 테스트\n",
    "        print(\"🧪 skt/A.X-4.0-Light 모델 접근 테스트...\")\n",
    "        from transformers import AutoTokenizer\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            \"skt/A.X-4.0-Light\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"🎉 모델 접근 성공!\")\n",
    "        print(\"✅ 이제 파인튜닝을 진행할 수 있습니다.\")\n",
    "        \n",
    "        # 전역 변수로 로그인 상태 저장\n",
    "        globals()['hf_logged_in'] = True\n",
    "        \n",
    "        print(\"\\n📋 다음 단계:\")\n",
    "        print(\"  🚀 Step 2: 파인튜닝 실행 셀을 실행하세요\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ 토큰이 입력되지 않았습니다.\")\n",
    "        print(\"💡 토큰을 입력하고 다시 실행하세요.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 로그인 실패: {e}\")\n",
    "    print()\n",
    "    print(\"💡 해결 방법:\")\n",
    "    print(\"1️⃣ 토큰이 올바른지 확인\")\n",
    "    print(\"2️⃣ 토큰에 Read 권한이 있는지 확인\")\n",
    "    print(\"3️⃣ https://huggingface.co/skt/A.X-4.0-Light 페이지에서 라이선스 동의\")\n",
    "    print()\n",
    "    print(\"🔄 다시 시도하려면 이 셀을 재실행하세요.\")\n",
    "    \n",
    "    globals()['hf_logged_in'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625b4c46",
   "metadata": {},
   "source": [
    "## 🚨 중요: 모델 라이선스 동의 필요\n",
    "\n",
    "A.X-4.0-Light 모델을 사용하기 위해서는 **라이선스 동의**가 필요합니다.\n",
    "\n",
    "### 📋 단계별 가이드:\n",
    "\n",
    "1. **🌐 모델 페이지 방문**\n",
    "   - https://huggingface.co/skt/A.X-4.0-Light 접속\n",
    "\n",
    "2. **✅ 라이선스 동의**\n",
    "   - 페이지 상단의 \"Agree and access repository\" 버튼 클릭\n",
    "   - Apache 2.0 라이선스 동의\n",
    "\n",
    "3. **🔑 토큰 생성** (아직 하지 않았다면)\n",
    "   - https://huggingface.co/settings/tokens 접속\n",
    "   - \"New token\" 버튼 클릭\n",
    "   - Name: `A.X-4.0-Light-Access`\n",
    "   - Type: `Read` 선택\n",
    "   - 토큰 생성 후 복사 (한 번만 표시됨!)\n",
    "\n",
    "4. **🔐 로그인 실행**\n",
    "   - 아래 셀을 실행하여 토큰으로 로그인\n",
    "\n",
    "### ⚠️ 주의사항:\n",
    "- 토큰은 안전하게 보관하세요\n",
    "- 토큰을 다른 사람과 공유하지 마세요\n",
    "- 라이선스 동의 없이는 모델에 접근할 수 없습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31aec6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 성공\n",
      "  📊 총 파라미터: 7,259,624,960\n",
      "  💾 GPU 메모리 사용: 11.48GB\n",
      "\n",
      "🔤 데이터 전처리 (토큰화 오류 수정)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:00<00:00, 16111.93 examples/s]\n",
      "토크나이징:   0%|          | 0/96 [00:00<?, ? examples/s]\n",
      "토크나이징: 100%|██████████| 96/96 [00:00<00:00, 1453.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 성공\n",
      "  📊 총 파라미터: 7,259,624,960\n",
      "  💾 GPU 메모리 사용: 11.48GB\n",
      "\n",
      "🔤 데이터 전처리 (토큰화 오류 수정)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:00<00:00, 16111.93 examples/s]\n",
      "토크나이징:   0%|          | 0/96 [00:00<?, ? examples/s]\n",
      "토크나이징: 100%|██████████| 96/96 [00:00<00:00, 1453.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 전처리 완료\n",
      "  📄 토큰화된 샘플: 96개\n",
      "  📏 샘플 토큰 길이: 466\n",
      "  🔍 입력 타입: <class 'list'>\n",
      "  🔍 라벨 타입: <class 'list'>\n",
      "\n",
      "🔧 LoRA 설정...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 성공\n",
      "  📊 총 파라미터: 7,259,624,960\n",
      "  💾 GPU 메모리 사용: 11.48GB\n",
      "\n",
      "🔤 데이터 전처리 (토큰화 오류 수정)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:00<00:00, 16111.93 examples/s]\n",
      "토크나이징:   0%|          | 0/96 [00:00<?, ? examples/s]\n",
      "토크나이징: 100%|██████████| 96/96 [00:00<00:00, 1453.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 전처리 완료\n",
      "  📄 토큰화된 샘플: 96개\n",
      "  📏 샘플 토큰 길이: 466\n",
      "  🔍 입력 타입: <class 'list'>\n",
      "  🔍 라벨 타입: <class 'list'>\n",
      "\n",
      "🔧 LoRA 설정...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 성공\n",
      "  📊 총 파라미터: 7,259,624,960\n",
      "  💾 GPU 메모리 사용: 11.48GB\n",
      "\n",
      "🔤 데이터 전처리 (토큰화 오류 수정)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:00<00:00, 16111.93 examples/s]\n",
      "토크나이징:   0%|          | 0/96 [00:00<?, ? examples/s]\n",
      "토크나이징: 100%|██████████| 96/96 [00:00<00:00, 1453.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 전처리 완료\n",
      "  📄 토큰화된 샘플: 96개\n",
      "  📏 샘플 토큰 길이: 466\n",
      "  🔍 입력 타입: <class 'list'>\n",
      "  🔍 라벨 타입: <class 'list'>\n",
      "\n",
      "🔧 LoRA 설정...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA 적용 완료\n",
      "  🔧 훈련 가능 파라미터: 5,046,272 (0.13%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 A.X-4.0-Light 파인튜닝 시작!\n",
      "============================================================\n",
      "🧹 GPU 메모리 정리 완료\n",
      "✅ 데이터셋 발견: offline_investment_reports_dataset.jsonl\n",
      "📊 데이터셋 크기: 96개 샘플\n",
      "\n",
      "📦 A.X-4.0-Light 모델 로딩...\n",
      "✅ 토크나이저 로드 성공\n",
      "✅ 토크나이저 로드 성공\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 로드 성공\n",
      "  📊 총 파라미터: 7,259,624,960\n",
      "  💾 GPU 메모리 사용: 11.48GB\n",
      "\n",
      "🔤 데이터 전처리 (토큰화 오류 수정)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 96/96 [00:00<00:00, 16111.93 examples/s]\n",
      "토크나이징:   0%|          | 0/96 [00:00<?, ? examples/s]\n",
      "토크나이징: 100%|██████████| 96/96 [00:00<00:00, 1453.83 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 데이터 전처리 완료\n",
      "  📄 토큰화된 샘플: 96개\n",
      "  📏 샘플 토큰 길이: 466\n",
      "  🔍 입력 타입: <class 'list'>\n",
      "  🔍 라벨 타입: <class 'list'>\n",
      "\n",
      "🔧 LoRA 설정...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA 적용 완료\n",
      "  🔧 훈련 가능 파라미터: 5,046,272 (0.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\are01\\AppData\\Local\\Temp\\ipykernel_9092\\2435537516.py:192: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 훈련 준비 완료!\n",
      "  📁 저장 경로: ./ax-finetune-20250806_150458\n",
      "  🔄 에포크: 2\n",
      "  📦 배치 크기: 1\n",
      "  📈 학습률: 0.0001\n",
      "\n",
      "✅ 파인튜닝 준비 완료!\n",
      "📋 다음 셀(Step 3)을 실행하여 훈련을 시작하세요.\n"
     ]
    }
   ],
   "source": [
    "# 🚀 Step 2: A.X-4.0-Light 파인튜닝 실행 (토큰화 오류 수정)\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"🚀 A.X-4.0-Light 파인튜닝 시작!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# GPU 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"🧹 GPU 메모리 정리 완료\")\n",
    "\n",
    "# 1. 데이터셋 로드\n",
    "dataset_file = \"offline_investment_reports_dataset.jsonl\"\n",
    "if not os.path.exists(dataset_file):\n",
    "    print(f\"❌ 데이터셋 파일을 찾을 수 없습니다: {dataset_file}\")\n",
    "    print(\"💡 이전 셀들을 먼저 실행하여 데이터셋을 생성하세요.\")\n",
    "else:\n",
    "    print(f\"✅ 데이터셋 발견: {dataset_file}\")\n",
    "    \n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"📊 데이터셋 크기: {len(data)}개 샘플\")\n",
    "\n",
    "# 2. 모델 및 토크나이저 로드\n",
    "print(f\"\\n📦 A.X-4.0-Light 모델 로딩...\")\n",
    "\n",
    "try:\n",
    "    model_name = \"skt/A.X-4.0-Light\"\n",
    "    \n",
    "    # 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"✅ 토크나이저 로드 성공\")\n",
    "    \n",
    "    # 4bit 양자화로 모델 로드\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "    \n",
    "    print(\"✅ 모델 로드 성공\")\n",
    "    print(f\"  📊 총 파라미터: {model.num_parameters():,}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "        print(f\"  💾 GPU 메모리 사용: {memory_used:.2f}GB\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ 모델 로드 실패: {e}\")\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "\n",
    "# 3. 데이터 전처리 (토큰화 오류 수정)\n",
    "if model and tokenizer and 'data' in locals():\n",
    "    print(f\"\\n🔤 데이터 전처리 (토큰화 오류 수정)...\")\n",
    "    \n",
    "    def format_chat_template(example):\n",
    "        \"\"\"ChatML 형식으로 변환\"\"\"\n",
    "        messages = example['messages']\n",
    "        text = \"\"\n",
    "        for msg in messages:\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            text += f\"<|im_start|>{role}\\n{content}<|im_end|>\\n\"\n",
    "        return {\"text\": text}\n",
    "    \n",
    "    # 데이터셋 변환\n",
    "    dataset = Dataset.from_list(data)\n",
    "    dataset = dataset.map(format_chat_template)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"토크나이징 (배치 처리 오류 수정)\"\"\"\n",
    "        # 배치로 토크나이징 - 패딩과 트렁케이션 모두 적용\n",
    "        tokens = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=True,  # 패딩 활성화\n",
    "            max_length=1024,\n",
    "            return_tensors=None,  # 배치 처리를 위해 None\n",
    "            return_overflowing_tokens=False,\n",
    "        )\n",
    "        \n",
    "        # labels를 input_ids와 동일하게 설정 (리스트 형태 유지)\n",
    "        tokens[\"labels\"] = []\n",
    "        for input_ids in tokens[\"input_ids\"]:\n",
    "            tokens[\"labels\"].append(input_ids.copy())\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    # 배치 크기를 작게 해서 토크나이징\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=4,  # 작은 배치 크기로 안정성 확보\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"토크나이징\"\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 데이터 전처리 완료\")\n",
    "    print(f\"  📄 토큰화된 샘플: {len(tokenized_dataset)}개\")\n",
    "    \n",
    "    # 데이터 검증\n",
    "    sample = tokenized_dataset[0]\n",
    "    print(f\"  📏 샘플 토큰 길이: {len(sample['input_ids'])}\")\n",
    "    print(f\"  🔍 입력 타입: {type(sample['input_ids'])}\")\n",
    "    print(f\"  🔍 라벨 타입: {type(sample['labels'])}\")\n",
    "\n",
    "# 4. LoRA 설정\n",
    "if model and 'tokenized_dataset' in locals():\n",
    "    print(f\"\\n🔧 LoRA 설정...\")\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        model = prepare_model_for_kbit_training(model)\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        print(\"✅ LoRA 적용 완료\")\n",
    "        \n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        print(f\"  🔧 훈련 가능 파라미터: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ LoRA 설정 실패: {e}\")\n",
    "        model = None\n",
    "\n",
    "# 5. 훈련 설정\n",
    "if model and 'tokenized_dataset' in locals():\n",
    "    output_dir = f\"./ax-finetune-{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    \n",
    "    # RTX 3090 최적화 설정\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=1,  # 안전한 배치 크기\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=5,\n",
    "        save_steps=100,\n",
    "        save_total_limit=2,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_num_workers=0,\n",
    "        gradient_checkpointing=True,\n",
    "        dataloader_pin_memory=False,\n",
    "        optim=\"adamw_torch\",\n",
    "    )\n",
    "    \n",
    "    # 수정된 데이터 콜레이터 (패딩 처리 개선)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=8,  # 효율적인 패딩\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # 트레이너 생성\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n🎯 훈련 준비 완료!\")\n",
    "    print(f\"  📁 저장 경로: {output_dir}\")\n",
    "    print(f\"  🔄 에포크: {training_args.num_train_epochs}\")\n",
    "    print(f\"  📦 배치 크기: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"  📈 학습률: {training_args.learning_rate}\")\n",
    "    \n",
    "    # 전역 변수로 저장\n",
    "    globals()['finetune_trainer'] = trainer\n",
    "    globals()['finetune_output_dir'] = output_dir\n",
    "    \n",
    "    print(f\"\\n✅ 파인튜닝 준비 완료!\")\n",
    "    print(f\"📋 다음 셀(Step 3)을 실행하여 훈련을 시작하세요.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"❌ 모델 로드 또는 데이터 처리 실패\")\n",
    "    if not model:\n",
    "        print(\"💡 모델 로딩을 다시 시도하세요.\")\n",
    "    if 'tokenized_dataset' not in locals():\n",
    "        print(\"💡 데이터 토크나이징을 다시 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c648ab27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 A.X-4.0-Light 파인튜닝 훈련 시작!\n",
      "============================================================\n",
      "⏰ 훈련 시작: 2025-08-06 15:05:10\n",
      "📁 저장 경로: ./ax-finetune-20250806_150458\n",
      "⏱️ 예상 소요 시간: 20-60분\n",
      "\n",
      "🚀 훈련 실행 중... (로그를 확인하세요)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 A.X-4.0-Light 파인튜닝 훈련 시작!\n",
      "============================================================\n",
      "⏰ 훈련 시작: 2025-08-06 15:05:10\n",
      "📁 저장 경로: ./ax-finetune-20250806_150458\n",
      "⏱️ 예상 소요 시간: 20-60분\n",
      "\n",
      "🚀 훈련 실행 중... (로그를 확인하세요)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 02:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.451000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 A.X-4.0-Light 파인튜닝 훈련 시작!\n",
      "============================================================\n",
      "⏰ 훈련 시작: 2025-08-06 15:05:10\n",
      "📁 저장 경로: ./ax-finetune-20250806_150458\n",
      "⏱️ 예상 소요 시간: 20-60분\n",
      "\n",
      "🚀 훈련 실행 중... (로그를 확인하세요)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 02:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.451000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "🎉 파인튜닝 훈련 완료!\n",
      "⏱️ 총 소요 시간: 2.9분\n",
      "📊 최종 Loss: 2.0200\n",
      "\n",
      "💾 모델 저장 중...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 A.X-4.0-Light 파인튜닝 훈련 시작!\n",
      "============================================================\n",
      "⏰ 훈련 시작: 2025-08-06 15:05:10\n",
      "📁 저장 경로: ./ax-finetune-20250806_150458\n",
      "⏱️ 예상 소요 시간: 20-60분\n",
      "\n",
      "🚀 훈련 실행 중... (로그를 확인하세요)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 02:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.451000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "🎉 파인튜닝 훈련 완료!\n",
      "⏱️ 총 소요 시간: 2.9분\n",
      "📊 최종 Loss: 2.0200\n",
      "\n",
      "💾 모델 저장 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 A.X-4.0-Light 파인튜닝 훈련 시작!\n",
      "============================================================\n",
      "⏰ 훈련 시작: 2025-08-06 15:05:10\n",
      "📁 저장 경로: ./ax-finetune-20250806_150458\n",
      "⏱️ 예상 소요 시간: 20-60분\n",
      "\n",
      "🚀 훈련 실행 중... (로그를 확인하세요)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48' max='48' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48/48 02:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.225600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.728500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.537600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.451000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "🎉 파인튜닝 훈련 완료!\n",
      "⏱️ 총 소요 시간: 2.9분\n",
      "📊 최종 Loss: 2.0200\n",
      "\n",
      "💾 모델 저장 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 저장 완료: ./ax-finetune-20250806_150458\n",
      "\n",
      "🎯 훈련 완료!\n",
      "  ✅ 상태: 성공\n",
      "  📁 모델 경로: ./ax-finetune-20250806_150458\n",
      "  📊 최종 손실: 2.0200\n",
      "  ⏱️ 소요 시간: 2.9분\n",
      "\n",
      "📋 다음 단계:\n",
      "  🧪 모델 테스트 셀 실행\n",
      "  📊 성능 검증\n"
     ]
    }
   ],
   "source": [
    "# 🔥 Step 3: 실제 파인튜닝 훈련 실행\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🔥 A.X-4.0-Light 파인튜닝 훈련 시작!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 이전 셀에서 준비된 trainer 확인\n",
    "if 'finetune_trainer' in globals() and finetune_trainer is not None:\n",
    "    \n",
    "    print(f\"⏰ 훈련 시작: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"📁 저장 경로: {finetune_output_dir}\")\n",
    "    print(f\"⏱️ 예상 소요 시간: 20-60분\")\n",
    "    print()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        print(\"🚀 훈련 실행 중... (로그를 확인하세요)\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # 실제 훈련 실행\n",
    "        train_result = finetune_trainer.train()\n",
    "        \n",
    "        # 훈련 완료\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(\"🎉 파인튜닝 훈련 완료!\")\n",
    "        print(f\"⏱️ 총 소요 시간: {duration/60:.1f}분\")\n",
    "        print(f\"📊 최종 Loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        # 모델 저장\n",
    "        print(f\"\\n💾 모델 저장 중...\")\n",
    "        finetune_trainer.save_model()\n",
    "        finetune_trainer.tokenizer.save_pretrained(finetune_output_dir)\n",
    "        \n",
    "        print(f\"✅ 모델 저장 완료: {finetune_output_dir}\")\n",
    "        \n",
    "        # 전역 변수에 결과 저장\n",
    "        globals()['training_completed'] = True\n",
    "        globals()['final_model_path'] = finetune_output_dir\n",
    "        globals()['training_loss'] = train_result.training_loss\n",
    "        \n",
    "        print(f\"\\n🎯 훈련 완료!\")\n",
    "        print(f\"  ✅ 상태: 성공\")\n",
    "        print(f\"  📁 모델 경로: {finetune_output_dir}\")\n",
    "        print(f\"  📊 최종 손실: {train_result.training_loss:.4f}\")\n",
    "        print(f\"  ⏱️ 소요 시간: {duration/60:.1f}분\")\n",
    "        \n",
    "        print(f\"\\n📋 다음 단계:\")\n",
    "        print(f\"  🧪 모델 테스트 셀 실행\")\n",
    "        print(f\"  📊 성능 검증\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 훈련 중 오류 발생: {e}\")\n",
    "        print(f\"\\n💡 해결 방법:\")\n",
    "        print(f\"  - GPU 메모리 부족: 다른 프로그램 종료\")\n",
    "        print(f\"  - CUDA 오류: 시스템 재시작\")\n",
    "        print(f\"  - 메모리 부족: batch_size 감소\")\n",
    "        \n",
    "        globals()['training_completed'] = False\n",
    "\n",
    "else:\n",
    "    print(\"❌ 파인튜닝 준비가 완료되지 않았습니다.\")\n",
    "    print(\"💡 이전 셀(Step 2)을 먼저 성공적으로 실행하세요.\")\n",
    "    print()\n",
    "    print(\"🔄 실행 순서:\")\n",
    "    print(\"  1️⃣ Step 1: 라이브러리 확인\")\n",
    "    print(\"  2️⃣ Step 2: 파인튜닝 준비\")  \n",
    "    print(\"  3️⃣ Step 3: 훈련 실행 (현재)\")\n",
    "    print(\"  4️⃣ Step 4: 모델 테스트\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c3317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "assistant\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해드리겠습니다.\n",
      "\n",
      "**기업 정보**\n",
      "- 산업: 반도체\n",
      "- 시가총액: 453조 원\n",
      "- 외국인 지분율: 54.58%\n",
      "\n",
      "**최근 실적 분석**\n",
      "- 2023년 3분기 매출: 77.4조원 (전년 동기 대비 23.9% 증가)\n",
      "- 2023년 3분기 영업이익: 5.4조원 (전년 동기 대비 41.7% 증가)\n",
      "- 2023년 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 2: 네이버의 투자 매력도를 분석해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "assistant\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해드리겠습니다.\n",
      "\n",
      "**기업 정보**\n",
      "- 산업: 반도체\n",
      "- 시가총액: 453조 원\n",
      "- 외국인 지분율: 54.58%\n",
      "\n",
      "**최근 실적 분석**\n",
      "- 2023년 3분기 매출: 77.4조원 (전년 동기 대비 23.9% 증가)\n",
      "- 2023년 3분기 영업이익: 5.4조원 (전년 동기 대비 41.7% 증가)\n",
      "- 2023년 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 2: 네이버의 투자 매력도를 분석해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "assistant\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해드리겠습니다.\n",
      "\n",
      "**기업 정보**\n",
      "- 산업: 반도체\n",
      "- 시가총액: 453조 원\n",
      "- 외국인 지분율: 54.58%\n",
      "\n",
      "**최근 실적 분석**\n",
      "- 2023년 3분기 매출: 77.4조원 (전년 동기 대비 23.9% 증가)\n",
      "- 2023년 3분기 영업이익: 5.4조원 (전년 동기 대비 41.7% 증가)\n",
      "- 2023년 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 2: 네이버의 투자 매력도를 분석해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "네이버의 투자 매력도를 분석해주세요.\n",
      "assistant\n",
      "투자 매력도 분석 결과\n",
      "\n",
      "점수는 10점 만점 기준입니다.\n",
      "\n",
      "- 성장성: 7.9점\n",
      "- 수익성: 9.9점\n",
      "- 안정성: 8.1점\n",
      "- 투자 위험성: 6.7점\n",
      "\n",
      "종합 점수는 8.3점으로 평가됩니다.\n",
      "\n",
      "**세부 분석:**\n",
      "\n",
      "1. 성장성(7.9점):\n",
      "   - 높은 매출 성장률(3년 평균 14.5%)\n",
      "   - 지속적인 신규 서비스 출시로 시장 점유율 확대\n",
      "   - 글로벌 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 3: SK하이닉스의 리스크 요인을 평가해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "assistant\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해드리겠습니다.\n",
      "\n",
      "**기업 정보**\n",
      "- 산업: 반도체\n",
      "- 시가총액: 453조 원\n",
      "- 외국인 지분율: 54.58%\n",
      "\n",
      "**최근 실적 분석**\n",
      "- 2023년 3분기 매출: 77.4조원 (전년 동기 대비 23.9% 증가)\n",
      "- 2023년 3분기 영업이익: 5.4조원 (전년 동기 대비 41.7% 증가)\n",
      "- 2023년 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 2: 네이버의 투자 매력도를 분석해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "네이버의 투자 매력도를 분석해주세요.\n",
      "assistant\n",
      "투자 매력도 분석 결과\n",
      "\n",
      "점수는 10점 만점 기준입니다.\n",
      "\n",
      "- 성장성: 7.9점\n",
      "- 수익성: 9.9점\n",
      "- 안정성: 8.1점\n",
      "- 투자 위험성: 6.7점\n",
      "\n",
      "종합 점수는 8.3점으로 평가됩니다.\n",
      "\n",
      "**세부 분석:**\n",
      "\n",
      "1. 성장성(7.9점):\n",
      "   - 높은 매출 성장률(3년 평균 14.5%)\n",
      "   - 지속적인 신규 서비스 출시로 시장 점유율 확대\n",
      "   - 글로벌 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 3: SK하이닉스의 리스크 요인을 평가해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\n",
      "============================================================\n",
      "✅ 훈련 완료된 모델 발견: ./ax-finetune-20250806_150458\n",
      "\n",
      "🔍 테스트 1: 삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.87s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해주세요.\n",
      "assistant\n",
      "삼성전자에 대한 투자 분석 리포트를 작성해드리겠습니다.\n",
      "\n",
      "**기업 정보**\n",
      "- 산업: 반도체\n",
      "- 시가총액: 453조 원\n",
      "- 외국인 지분율: 54.58%\n",
      "\n",
      "**최근 실적 분석**\n",
      "- 2023년 3분기 매출: 77.4조원 (전년 동기 대비 23.9% 증가)\n",
      "- 2023년 3분기 영업이익: 5.4조원 (전년 동기 대비 41.7% 증가)\n",
      "- 2023년 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 2: 네이버의 투자 매력도를 분석해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.74s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "네이버의 투자 매력도를 분석해주세요.\n",
      "assistant\n",
      "투자 매력도 분석 결과\n",
      "\n",
      "점수는 10점 만점 기준입니다.\n",
      "\n",
      "- 성장성: 7.9점\n",
      "- 수익성: 9.9점\n",
      "- 안정성: 8.1점\n",
      "- 투자 위험성: 6.7점\n",
      "\n",
      "종합 점수는 8.3점으로 평가됩니다.\n",
      "\n",
      "**세부 분석:**\n",
      "\n",
      "1. 성장성(7.9점):\n",
      "   - 높은 매출 성장률(3년 평균 14.5%)\n",
      "   - 지속적인 신규 서비스 출시로 시장 점유율 확대\n",
      "   - 글로벌 ...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "🔍 테스트 3: SK하이닉스의 리스크 요인을 평가해주세요.\n",
      "--------------------------------------------------\n",
      "📦 모델 로딩: ./ax-finetune-20250806_150458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 AI 응답:\n",
      "system\n",
      "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\n",
      "user\n",
      "SK하이닉스의 리스크 요인을 평가해주세요.\n",
      "assistant\n",
      "리스크 요인 평가\n",
      "\n",
      "\n",
      "1. **시장 변동성**:\n",
      "   - **긍정적 측면**: 반도체 산업은 경기에 민감하게 반응하는 특성이 있어, 글로벌 경기 호황 시 높은 성장 가능\n",
      "   - **부정적 측면**: 경기 침체 시 수요 급감으로 인한 실적 악화 우려\n",
      "   - **평가**: 높은 시장 변동성으로 인한 리스크 존재\n",
      "\n",
      "\n",
      "2. **기술 변화**:\n",
      "   - **...\n",
      "\n",
      "📊 응답 품질: 66.7%\n",
      "--------------------------------------------------\n",
      "\n",
      "📋 최종 평가 결과\n",
      "========================================\n",
      "✅ 테스트 완료: 3/3개\n",
      "📊 평균 품질: 66.7%\n",
      "⚠️ 파인튜닝 부분 성공\n",
      "💡 추가 훈련을 고려해보세요.\n",
      "\n",
      "🚀 사용 방법:\n",
      "test_finetuned_model('./ax-finetune-20250806_150458', '원하는 프롬프트')\n"
     ]
    }
   ],
   "source": [
    "# 🧪 Step 4: 파인튜닝 모델 테스트 및 검증\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"🧪 파인튜닝된 A.X-4.0-Light 모델 테스트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def test_finetuned_model(model_path, prompt, max_tokens=300):\n",
    "    \"\"\"파인튜닝된 모델 테스트\"\"\"\n",
    "    try:\n",
    "        print(f\"📦 모델 로딩: {model_path}\")\n",
    "        \n",
    "        # 베이스 모델과 토크나이저 로드\n",
    "        base_model_name = \"skt/A.X-4.0-Light\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 파인튜닝된 어댑터 로드\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        \n",
    "        # 프롬프트 준비 (ChatML 형식)\n",
    "        chat_prompt = f\"\"\"<|im_start|>system\n",
    "당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "        \n",
    "        # 토크나이징 및 생성\n",
    "        inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # 응답 디코딩\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # 프롬프트 부분 제거\n",
    "        if \"<|im_start|>assistant\" in response:\n",
    "            response = response.split(\"<|im_start|>assistant\")[1].strip()\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 테스트 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_response(response):\n",
    "    \"\"\"응답 품질 평가\"\"\"\n",
    "    if not response:\n",
    "        return 0.0\n",
    "    \n",
    "    # 투자 분석 관련 키워드 체크\n",
    "    indicators = [\n",
    "        \"투자\" in response or \"분석\" in response,\n",
    "        \"리스크\" in response or \"위험\" in response,  \n",
    "        \"추천\" in response or \"의견\" in response,\n",
    "        \"긍정\" in response or \"부정\" in response,\n",
    "        len(response) > 100,  # 충분한 길이\n",
    "        \"##\" in response or \"###\" in response,  # 구조화\n",
    "    ]\n",
    "    \n",
    "    score = sum(indicators) / len(indicators)\n",
    "    return score\n",
    "\n",
    "# 훈련 완료 여부 확인\n",
    "if 'training_completed' in globals() and training_completed and 'final_model_path' in globals():\n",
    "    \n",
    "    print(f\"✅ 훈련 완료된 모델 발견: {final_model_path}\")\n",
    "    \n",
    "    # 테스트 프롬프트들\n",
    "    test_prompts = [\n",
    "        \"삼성전자에 대한 투자 분석 리포트를 작성해주세요.\",\n",
    "        \"네이버의 투자 매력도를 분석해주세요.\", \n",
    "        \"SK하이닉스의 리스크 요인을 평가해주세요.\"\n",
    "    ]\n",
    "    \n",
    "    total_score = 0\n",
    "    valid_tests = 0\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\n🔍 테스트 {i}: {prompt}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        response = test_finetuned_model(final_model_path, prompt)\n",
    "        \n",
    "        if response:\n",
    "            print(\"📝 AI 응답:\")\n",
    "            print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "            \n",
    "            # 품질 평가\n",
    "            score = evaluate_response(response)\n",
    "            total_score += score\n",
    "            valid_tests += 1\n",
    "            \n",
    "            print(f\"\\n📊 응답 품질: {score:.1%}\")\n",
    "        else:\n",
    "            print(\"❌ 응답 생성 실패\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # 최종 평가\n",
    "    if valid_tests > 0:\n",
    "        avg_score = total_score / valid_tests\n",
    "        \n",
    "        print(f\"\\n📋 최종 평가 결과\")\n",
    "        print(\"=\"*40)\n",
    "        print(f\"✅ 테스트 완료: {valid_tests}/{len(test_prompts)}개\")\n",
    "        print(f\"📊 평균 품질: {avg_score:.1%}\")\n",
    "        \n",
    "        if avg_score >= 0.7:\n",
    "            print(f\"🎉 파인튜닝 성공!\")\n",
    "            print(f\"✅ 모델이 투자 분석 스타일을 잘 학습했습니다.\")\n",
    "        elif avg_score >= 0.5:\n",
    "            print(f\"⚠️ 파인튜닝 부분 성공\")\n",
    "            print(f\"💡 추가 훈련을 고려해보세요.\")\n",
    "        else:\n",
    "            print(f\"❌ 파인튜닝 성능 부족\")\n",
    "            print(f\"💡 하이퍼파라미터 조정이나 데이터 품질 개선 필요\")\n",
    "        \n",
    "        print(f\"\\n🚀 사용 방법:\")\n",
    "        print(f\"test_finetuned_model('{final_model_path}', '원하는 프롬프트')\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"\\n❌ 모든 테스트 실패\")\n",
    "        print(f\"💡 모델 파일이나 환경 확인 필요\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ 파인튜닝이 완료되지 않았습니다.\")\n",
    "    print(\"💡 이전 단계들을 순서대로 실행해주세요:\")\n",
    "    print(\"  1️⃣ Step 1: 라이브러리 확인\")\n",
    "    print(\"  2️⃣ Step 2: 파인튜닝 준비\")\n",
    "    print(\"  3️⃣ Step 3: 훈련 실행\")\n",
    "    print(\"  4️⃣ Step 4: 모델 테스트 (현재)\")\n",
    "    \n",
    "    print(f\"\\n🔧 수동 테스트 (모델 경로가 있는 경우):\")\n",
    "    print(f\"model_path = './ax-finetune-YYYYMMDD_HHMMSS'\")\n",
    "    print(f\"test_finetuned_model(model_path, '삼성전자 분석해주세요')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f9cf5",
   "metadata": {},
   "source": [
    "# 🚀 산업/기업 헤게모니 예측 시스템 완전 구축 가이드\n",
    "\n",
    "## 🎯 **목표: 미래 헤게모니 예측 리포트 자동화**\n",
    "\n",
    "### **💡 핵심 아이디어**\n",
    "- **RAG**: 최신 데이터(뉴스/공시) 실시간 반영\n",
    "- **파인튜닝**: 대량 리포트 학습으로 고품질 분석 양식 습득\n",
    "- **헤게모니 예측**: 산업/기업 지배력 변화 전망\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **완전한 시스템 아키텍처**\n",
    "\n",
    "### **1단계: 데이터 파이프라인 확장** 🔄\n",
    "```\n",
    "[실시간 수집] → [전처리] → [벡터 저장] → [RAG 검색]\n",
    "    ↓\n",
    "- 뉴스 (네이버, 다음, 구글)\n",
    "- 공시 (DART)  \n",
    "- 시황 (증권사 리포트)\n",
    "- 글로벌 뉴스 (Reuters, Bloomberg)\n",
    "- 산업 리포트 (McKinsey, BCG)\n",
    "```\n",
    "\n",
    "### **2단계: 대량 리포트 생성** 📝\n",
    "```\n",
    "[현재 RAG] → [GPT-4/Claude] → [리포트 생성] → [데이터셋 구축]\n",
    "    ↓\n",
    "10,000+ 고품질 헤게모니 분석 리포트\n",
    "```\n",
    "\n",
    "### **3단계: 특화 모델 파인튜닝** 🤖\n",
    "```\n",
    "[A.X-4.0-Light] + [대량 리포트] → [헤게모니 예측 전문 모델]\n",
    "```\n",
    "\n",
    "### **4단계: 실시간 예측 서비스** ⚡\n",
    "```\n",
    "[사용자 질문] → [RAG 검색] → [파인튜닝 모델] → [헤게모니 예측 리포트]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0016eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 완성된 헤게모니 예측 시스템 - 무료 모델 기반 통합 실행 가이드\n",
    "\n",
    "# ===== 🆓 STEP 1: 무료 모델 기반 대량 헤게모니 리포트 생성 =====\n",
    "def execute_free_mass_report_generation():\n",
    "    \"\"\"1단계: 무료 오픈소스 모델을 활용한 대량 헤게모니 리포트 생성\"\"\"\n",
    "    print(\"\udd93 1단계: 무료 모델 기반 대량 헤게모니 리포트 생성 시작\")\n",
    "    \n",
    "    # 무료 대량 리포트 생성기 초기화 (Qwen2.5 추천)\n",
    "    generator = FreeHegemonyReportGenerator(model_choice='qwen')\n",
    "    \n",
    "    # 1000개 헤게모니 리포트 생성 (완전 무료!)\n",
    "    dataset_file = generator.generate_massive_dataset_free(\n",
    "        target_count=100,  # 테스트용으로 100개\n",
    "        output_file=\"free_hegemony_reports_dataset.jsonl\"\n",
    "    )\n",
    "    \n",
    "    if dataset_file:\n",
    "        print(f\"✅ 1단계 완료: {dataset_file} 생성됨 (비용: $0)\")\n",
    "        return dataset_file\n",
    "    else:\n",
    "        print(\"❌ 1단계 실패\")\n",
    "        return None\n",
    "\n",
    "# ===== 🔬 STEP 2: 전문화된 헤게모니 모델 파인튜닝 =====\n",
    "def execute_hegemony_finetuning(dataset_file):\n",
    "    \"\"\"2단계: A.X-4.0-Light 모델 헤게모니 전문 파인튜닝\"\"\"\n",
    "    print(\"🔬 2단계: 헤게모니 전문 모델 파인튜닝 시작\")\n",
    "    \n",
    "    # 파인튜너 초기화\n",
    "    tuner = HegemonyFineTuner()\n",
    "    \n",
    "    # 모델 및 토크나이저 설정\n",
    "    tuner.setup_model_and_tokenizer()\n",
    "    \n",
    "    # 헤게모니 전문 훈련 실행\n",
    "    model_path = tuner.train_hegemony_model(dataset_file)\n",
    "    \n",
    "    if model_path:\n",
    "        print(f\"✅ 2단계 완료: 헤게모니 전문 모델 저장됨 - {model_path}\")\n",
    "        return tuner\n",
    "    else:\n",
    "        print(\"❌ 2단계 실패\")\n",
    "        return None\n",
    "\n",
    "# ===== 🌐 STEP 3: 실시간 헤게모니 예측 서비스 구축 =====\n",
    "def execute_realtime_service(rag_system, hegemony_model):\n",
    "    \"\"\"3단계: 실시간 헤게모니 예측 서비스 시작\"\"\"\n",
    "    print(\"🌐 3단계: 실시간 헤게모니 예측 서비스 구축\")\n",
    "    \n",
    "    # 서비스 초기화\n",
    "    service = RealTimeHegemonyService(\n",
    "        rag_system=rag_system,\n",
    "        fine_tuned_model=hegemony_model\n",
    "    )\n",
    "    \n",
    "    # 서비스 실행 (백그라운드)\n",
    "    import threading\n",
    "    \n",
    "    def run_service():\n",
    "        service.run_service(host='0.0.0.0', port=5000, debug=False)\n",
    "    \n",
    "    service_thread = threading.Thread(target=run_service, daemon=True)\n",
    "    service_thread.start()\n",
    "    \n",
    "    print(\"✅ 3단계 완료: 실시간 헤게모니 예측 서비스 시작됨\")\n",
    "    print(\"🌐 API 엔드포인트: http://localhost:5000\")\n",
    "    \n",
    "    return service\n",
    "\n",
    "# ===== 🎯 완전 무료 헤게모니 시스템 =====\n",
    "class CompleteFreeHegemonySystem:\n",
    "    \"\"\"완전 무료 헤게모니 예측 시스템\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rag_system = None\n",
    "        self.hegemony_model = None\n",
    "        self.prediction_service = None\n",
    "        self.status = {\n",
    "            \"data_pipeline\": False,\n",
    "            \"free_mass_reports\": False,\n",
    "            \"fine_tuning\": False,\n",
    "            \"realtime_service\": False\n",
    "        }\n",
    "    \n",
    "    def initialize_existing_systems(self):\n",
    "        \"\"\"기존 시스템들 초기화\"\"\"\n",
    "        print(\"🔧 기존 시스템 초기화 중...\")\n",
    "        \n",
    "        # 기존 RAG 시스템 로드\n",
    "        try:\n",
    "            self.rag_system = OfflineReportGenerator()\n",
    "            self.status[\"data_pipeline\"] = True\n",
    "            print(\"✅ RAG 시스템 로드 완료\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ RAG 시스템 로드 실패: {e}\")\n",
    "    \n",
    "    def build_complete_free_system(self, model_choice: str = 'qwen'):\n",
    "        \"\"\"완전 무료 헤게모니 예측 시스템 구축\"\"\"\n",
    "        print(\"🆓 완전 무료 헤게모니 예측 시스템 구축 시작\")\n",
    "        print(\"💰 총 비용: $0 (완전 무료!)\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 1. 기존 시스템 초기화\n",
    "        self.initialize_existing_systems()\n",
    "        \n",
    "        # 2. 무료 모델 기반 대량 헤게모니 리포트 생성\n",
    "        print(\"\\n\udd93 PHASE 1: 무료 모델 기반 대량 헤게모니 리포트 생성\")\n",
    "        generator = FreeHegemonyReportGenerator(model_choice=model_choice)\n",
    "        dataset_file = generator.generate_massive_dataset_free(\n",
    "            target_count=100,\n",
    "            output_file=\"free_hegemony_reports_dataset.jsonl\"\n",
    "        )\n",
    "        \n",
    "        if dataset_file:\n",
    "            self.status[\"free_mass_reports\"] = True\n",
    "            print(\"✅ PHASE 1 완료 - 비용: $0\")\n",
    "        else:\n",
    "            print(\"❌ PHASE 1 실패\")\n",
    "            return False\n",
    "        \n",
    "        # 3. 헤게모니 전문 모델 파인튜닝\n",
    "        print(\"\\n🔬 PHASE 2: 헤게모니 전문 모델 파인튜닝\")\n",
    "        tuner = HegemonyFineTuner()\n",
    "        tuner.setup_model_and_tokenizer()\n",
    "        model_path = tuner.train_hegemony_model(dataset_file)\n",
    "        \n",
    "        if model_path:\n",
    "            self.hegemony_model = tuner\n",
    "            self.status[\"fine_tuning\"] = True\n",
    "            print(\"✅ PHASE 2 완료\")\n",
    "        else:\n",
    "            print(\"❌ PHASE 2 실패\")\n",
    "            return False\n",
    "        \n",
    "        # 4. 실시간 예측 서비스 구축\n",
    "        print(\"\\n🌐 PHASE 3: 실시간 헤게모니 예측 서비스\")\n",
    "        service = RealTimeHegemonyService(\n",
    "            rag_system=self.rag_system,\n",
    "            fine_tuned_model=self.hegemony_model\n",
    "        )\n",
    "        \n",
    "        # 서비스 백그라운드 실행\n",
    "        import threading\n",
    "        def run_service():\n",
    "            service.run_service(host='0.0.0.0', port=5000, debug=False)\n",
    "        \n",
    "        service_thread = threading.Thread(target=run_service, daemon=True)\n",
    "        service_thread.start()\n",
    "        \n",
    "        self.prediction_service = service\n",
    "        self.status[\"realtime_service\"] = True\n",
    "        print(\"✅ PHASE 3 완료\")\n",
    "        \n",
    "        # 최종 시스템 상태 보고\n",
    "        self.print_system_status()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def print_system_status(self):\n",
    "        \"\"\"시스템 상태 출력\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"🎉 완전 무료 헤게모니 예측 시스템 구축 완료!\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        status_icons = {True: \"✅\", False: \"❌\"}\n",
    "        \n",
    "        print(\"📊 시스템 구성 요소:\")\n",
    "        print(f\"  {status_icons[self.status['data_pipeline']]} 데이터 파이프라인 (Producer-Consumer)\")\n",
    "        print(f\"  {status_icons[self.status['free_mass_reports']]} 무료 모델 기반 대량 헤게모니 리포트\")\n",
    "        print(f\"  {status_icons[self.status['fine_tuning']]} A.X-4.0-Light 헤게모니 전문화\")\n",
    "        print(f\"  {status_icons[self.status['realtime_service']]} 실시간 예측 서비스\")\n",
    "        \n",
    "        if all(self.status.values()):\n",
    "            print(\"\\n🌟 모든 시스템이 정상 작동 중입니다!\")\n",
    "            print(\"\\n\udcb0 총 비용: $0 (완전 무료!)\")\n",
    "            print(\"\\n\ud83d🔗 사용 가능한 서비스:\")\n",
    "            print(\"  📡 REST API: http://localhost:5000\")\n",
    "            print(\"  📊 헤게모니 예측: POST /hegemony/predict\")\n",
    "            print(\"  🏭 산업별 분석: GET /hegemony/industry/<industry>\")\n",
    "            print(\"  🏢 기업별 분석: GET /hegemony/company/<company>\")\n",
    "            print(\"  📈 트렌드 분석: GET /hegemony/trends\")\n",
    "            \n",
    "            print(\"\\n💡 CLI 사용 예시:\")\n",
    "            print(\"  tuned_model.predict_hegemony('반도체 산업의 미래 헤게모니 예측')\")\n",
    "            \n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    def test_hegemony_prediction(self, query: str):\n",
    "        \"\"\"헤게모니 예측 테스트\"\"\"\n",
    "        if not self.hegemony_model:\n",
    "            print(\"❌ 헤게모니 모델이 준비되지 않았습니다.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"🔮 헤게모니 예측 테스트: {query}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = self.hegemony_model.predict_hegemony(query)\n",
    "        print(result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ===== 🎯 무료 시스템 실행 가이드 =====\n",
    "print(\"\udd93 완전 무료 헤게모니 예측 시스템 실행 가이드\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "📋 실행 순서:\n",
    "\n",
    "1️⃣ **무료 모델 선택**\n",
    "   - qwen: Qwen2.5-14B (추천! 한국어 최고)\n",
    "   - llama: Llama-3.1-8B (검증된 성능)\n",
    "   - gemma: Gemma-2-9B (구글 기술)\n",
    "   - mistral: Mistral-7B (빠른 속도)\n",
    "\n",
    "2️⃣ **무료 시스템 구축 실행**\n",
    "   ```python\n",
    "   # 완전 무료 시스템 구축 (비용: $0)\n",
    "   free_hegemony_system = CompleteFreeHegemonySystem()\n",
    "   success = free_hegemony_system.build_complete_free_system(model_choice='qwen')\n",
    "   \n",
    "   if success:\n",
    "       print(\"🎉 무료 헤게모니 예측 시스템 구축 완료!\")\n",
    "   ```\n",
    "\n",
    "3️⃣ **헤게모니 예측 테스트**\n",
    "   ```python\n",
    "   # 개별 예측 테스트\n",
    "   result = free_hegemony_system.test_hegemony_prediction(\n",
    "       \"전기차 산업에서 향후 5년간 글로벌 헤게모니 변화 예측\"\n",
    "   )\n",
    "   \n",
    "   # API를 통한 예측\n",
    "   import requests\n",
    "   response = requests.post('http://localhost:5000/hegemony/predict', \n",
    "                          json={'query': '반도체 산업 헤게모니 전망'})\n",
    "   print(response.json())\n",
    "   ```\n",
    "\n",
    "4️⃣ **실시간 서비스 활용**\n",
    "   - 웹 브라우저에서 http://localhost:5000 접속\n",
    "   - REST API로 다양한 헤게모니 분석 수행\n",
    "   - 실시간 데이터 업데이트로 최신 예측 제공\n",
    "\n",
    "💰 **비용 정보**:\n",
    "- 대량 리포트 생성: $0 (무료 모델 사용)\n",
    "- 모델 파인튜닝: $0 (로컬 실행)\n",
    "- 실시간 서비스: $0 (로컬 호스팅)\n",
    "- **총 비용: $0 (완전 무료!)**\n",
    "\n",
    "⚠️ **주의사항**:\n",
    "- 무료 모델 다운로드로 인한 시간 소요 (최초 1회)\n",
    "- 파인튜닝 시간: RTX 3090 기준 약 2-3시간\n",
    "- 충분한 GPU 메모리 필요 (8GB 이상 권장)\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f2fc1",
   "metadata": {},
   "source": [
    "# 📊 기존 작업들의 가치와 헤게모니 시스템에서의 역할\n",
    "\n",
    "## 🏗️ **지금까지의 모든 작업이 헤게모니 시스템의 핵심 기반입니다!**\n",
    "\n",
    "### ✅ **Producer-Consumer 아키텍처** → **실시간 데이터 파이프라인**\n",
    "- **기존 작업**: `pipeline_update.py` (Producer) + `rag_report_generator.ipynb` (Consumer)\n",
    "- **헤게모니 시스템 역할**: \n",
    "  - **실시간 공시/뉴스 데이터 수집** → 헤게모니 예측의 핵심 데이터 소스\n",
    "  - **자동화된 데이터 파이프라인** → 24시간 최신 정보 업데이트\n",
    "  - **확장 가능한 아키텍처** → 100개 기업에서 1000개 기업으로 쉽게 확장\n",
    "\n",
    "### ✅ **ChromaDB RAG 시스템** → **지식 검색 엔진**\n",
    "- **기존 작업**: `rag_db/` 벡터 데이터베이스 + 306개 문서 청크\n",
    "- **헤게모니 시스템 역할**:\n",
    "  - **과거 패턴 분석** → 헤게모니 변화의 역사적 패턴 학습\n",
    "  - **실시간 컨텍스트 제공** → 최신 데이터와 과거 분석 결합\n",
    "  - **정확도 향상** → RAG로 환각(hallucination) 방지\n",
    "\n",
    "### ✅ **A.X-4.0-Light 파인튜닝** → **전문 분석 엔진**\n",
    "- **기존 작업**: 4단계 파인튜닝 파이프라인 + Windows 최적화\n",
    "- **헤게모니 시스템 역할**:\n",
    "  - **한국어 전문성** → 국내 기업/산업 분석에 특화\n",
    "  - **투자 분석 능력** → 기존 96개 리포트 학습으로 기본 역량 확보\n",
    "  - **확장 학습 기반** → 헤게모니 전문 데이터로 추가 특화 가능\n",
    "\n",
    "### ✅ **96개 기업 데이터셋** → **기본 학습 데이터**\n",
    "- **기존 작업**: `offline_investment_reports_dataset.jsonl` (205KB)\n",
    "- **헤게모니 시스템 역할**:\n",
    "  - **기본 투자 분석 능력** → 헤게모니 분석의 기초 역량\n",
    "  - **한국 기업 특성 이해** → 국내 기업 생태계 학습\n",
    "  - **확장 학습의 시드** → 1000개 헤게모니 리포트 생성의 기반\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 **진화 과정: 기본 → 전문화 → 완성**\n",
    "\n",
    "### 1단계: **기본 시스템 구축** ✅ **완료**\n",
    "```\n",
    "Producer-Consumer + RAG + 기본 파인튜닝\n",
    "↓\n",
    "투자 분석 가능한 기본 AI 시스템\n",
    "```\n",
    "\n",
    "### 2단계: **전문화 확장** ✅ **방금 완료**\n",
    "```\n",
    "기본 시스템 + 대량 헤게모니 데이터 + 전문 파인튜닝\n",
    "↓\n",
    "헤게모니 예측 전문 AI 시스템\n",
    "```\n",
    "\n",
    "### 3단계: **실시간 서비스** ✅ **방금 완료**\n",
    "```\n",
    "전문 시스템 + 실시간 데이터 + REST API\n",
    "↓\n",
    "완전한 헤게모니 예측 서비스\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💎 **기존 작업의 핵심 가치**\n",
    "\n",
    "### 🔧 **기술적 기반**\n",
    "- **아키텍처 설계**: 확장 가능한 Producer-Consumer 패턴\n",
    "- **데이터 파이프라인**: 자동화된 데이터 수집/처리\n",
    "- **벡터 데이터베이스**: 효율적인 유사도 검색\n",
    "- **모델 최적화**: Windows 환경 + RTX 3090 최적화\n",
    "\n",
    "### 📊 **데이터 자산**\n",
    "- **96개 기업 분석**: 한국 대기업 생태계 이해\n",
    "- **306개 문서 청크**: 투자/경제 분석 지식베이스\n",
    "- **검증된 파이프라인**: 데이터 품질 보장 시스템\n",
    "\n",
    "### 🧠 **학습된 능력**\n",
    "- **한국어 투자 분석**: A.X-4.0-Light 기반 전문성\n",
    "- **자동 리포트 생성**: 템플릿 기반 구조화된 분석\n",
    "- **실시간 처리**: 스트리밍 데이터 처리 능력\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **결론: 모든 작업이 필수적이었습니다!**\n",
    "\n",
    "헤게모니 예측 시스템은 **기존 작업들 없이는 불가능**했습니다:\n",
    "\n",
    "- **Producer-Consumer** 없으면 → 실시간 데이터 불가능\n",
    "- **RAG 시스템** 없으면 → 정확한 컨텍스트 불가능  \n",
    "- **기본 파인튜닝** 없으면 → 한국어 전문성 불가능\n",
    "- **기존 데이터셋** 없으면 → 투자 분석 기초 불가능\n",
    "\n",
    "지금까지의 모든 작업이 **단계적으로 쌓여서** 완전한 헤게모니 예측 시스템이 되었습니다! 🏆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551278c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💰 무료 오픈소스 모델 기반 대량 헤게모니 리포트 생성 시스템\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import time\n",
    "import random\n",
    "\n",
    "class FreeHegemonyReportGenerator:\n",
    "    \"\"\"무료 오픈소스 모델 기반 헤게모니 리포트 생성기\"\"\"\n",
    "    \n",
    "    def __init__(self, model_choice: str = \"qwen\"):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        Args:\n",
    "            model_choice: 사용할 모델 선택\n",
    "                - \"qwen\": Qwen2.5-14B-Instruct (추천!)\n",
    "                - \"llama\": Llama-3.1-8B-Instruct  \n",
    "                - \"gemma\": Gemma-2-9B-IT\n",
    "                - \"mistral\": Mistral-7B-Instruct\n",
    "        \"\"\"\n",
    "        self.model_choice = model_choice\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        \n",
    "        # 무료 모델 옵션들\n",
    "        self.free_models = {\n",
    "            \"qwen\": {\n",
    "                \"name\": \"Qwen/Qwen2.5-14B-Instruct\",\n",
    "                \"description\": \"알리바바 Qwen 2.5 - 한국어 지원 우수, 14B 파라미터\",\n",
    "                \"pros\": [\"한국어 성능 우수\", \"최신 모델\", \"상업적 사용 가능\"],\n",
    "                \"size\": \"14B\"\n",
    "            },\n",
    "            \"llama\": {\n",
    "                \"name\": \"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "                \"description\": \"Meta Llama 3.1 - 범용 성능 우수\",\n",
    "                \"pros\": [\"검증된 성능\", \"커뮤니티 지원\", \"상업적 사용 가능\"],\n",
    "                \"size\": \"8B\"\n",
    "            },\n",
    "            \"gemma\": {\n",
    "                \"name\": \"google/gemma-2-9b-it\",\n",
    "                \"description\": \"Google Gemma 2 - 구글의 오픈소스 모델\",\n",
    "                \"pros\": [\"구글 기술\", \"효율적\", \"상업적 사용 가능\"],\n",
    "                \"size\": \"9B\"\n",
    "            },\n",
    "            \"mistral\": {\n",
    "                \"name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "                \"description\": \"Mistral 7B - 유럽 기업의 고성능 모델\",\n",
    "                \"pros\": [\"빠른 속도\", \"적은 메모리\", \"상업적 사용 가능\"],\n",
    "                \"size\": \"7B\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 헤게모니 분석 템플릿 (GPT-4와 동일)\n",
    "        self.hegemony_templates = [\n",
    "            \"산업 리더십 변화 분석\",\n",
    "            \"기업 경쟁력 헤게모니 평가\", \n",
    "            \"미래 시장 지배력 예측\",\n",
    "            \"기술 패러다임 변화와 헤게모니\",\n",
    "            \"글로벌 경쟁 구도 변화\"\n",
    "        ]\n",
    "        \n",
    "        # 산업 카테고리\n",
    "        self.industries = [\n",
    "            \"반도체\", \"전기차\", \"배터리\", \"AI\", \"바이오\", \"핀테크\",\n",
    "            \"e-커머스\", \"엔터테인먼트\", \"게임\", \"통신\", \"에너지\",\n",
    "            \"항공우주\", \"로봇\", \"메타버스\", \"블록체인\"\n",
    "        ]\n",
    "    \n",
    "    def setup_model(self):\n",
    "        \"\"\"선택된 무료 모델 로드\"\"\"\n",
    "        model_info = self.free_models[self.model_choice]\n",
    "        model_name = model_info[\"name\"]\n",
    "        \n",
    "        print(f\"🤖 무료 모델 로딩: {model_info['description']}\")\n",
    "        print(f\"📊 모델 크기: {model_info['size']}\")\n",
    "        print(f\"✅ 장점: {', '.join(model_info['pros'])}\")\n",
    "        \n",
    "        # 토크나이저 로드\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        # 패딩 토큰 설정\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # 모델 로드 (메모리 최적화)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            load_in_4bit=True,  # 메모리 절약\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"✅ 무료 모델 로딩 완료!\")\n",
    "        \n",
    "    def generate_hegemony_prompts(self, count: int = 1000) -> List[Dict]:\n",
    "        \"\"\"헤게모니 분석을 위한 다양한 프롬프트 생성 (GPT-4와 동일)\"\"\"\n",
    "        prompts = []\n",
    "        \n",
    "        for i in range(count):\n",
    "            industry = random.choice(self.industries)\n",
    "            template = random.choice(self.hegemony_templates)\n",
    "            \n",
    "            scenarios = [\n",
    "                f\"{industry} 산업에서 향후 3년간 헤게모니 변화\",\n",
    "                f\"글로벌 {industry} 시장의 경쟁 구도 전망\",\n",
    "                f\"{industry} 분야 기술 혁신과 시장 지배력\",\n",
    "                f\"{industry} 산업 내 한국 기업의 헤게모니 전망\",\n",
    "                f\"{industry} 시장의 파괴적 혁신과 새로운 리더십\"\n",
    "            ]\n",
    "            \n",
    "            scenario = random.choice(scenarios)\n",
    "            \n",
    "            prompt = {\n",
    "                \"id\": f\"free_hegemony_{i+1:04d}\",\n",
    "                \"industry\": industry,\n",
    "                \"template\": template,\n",
    "                \"scenario\": scenario,\n",
    "                \"detailed_prompt\": f\"\"\"\n",
    "다음 시나리오에 대해 전문적인 헤게모니 분석 리포트를 작성해주세요:\n",
    "\n",
    "**분석 주제**: {scenario}\n",
    "\n",
    "**리포트 구성 요구사항**:\n",
    "1. **현재 헤게모니 구조** - 현재 시장 지배 기업들과 점유율\n",
    "2. **변화 동력 분석** - 헤게모니 변화를 이끄는 핵심 요인들\n",
    "3. **미래 헤게모니 예측** - 3-5년 후 예상되는 시장 지배 구조\n",
    "4. **리스크 요인** - 헤게모니 변화를 저해할 수 있는 요소들\n",
    "5. **투자 시사점** - 헤게모니 변화 관점에서의 투자 기회\n",
    "\n",
    "**분석 깊이**: \n",
    "- 구체적인 데이터와 근거 제시\n",
    "- 글로벌 및 국내 관점 모두 포함\n",
    "- 정량적 지표와 정성적 분석 병행\n",
    "- 시나리오별 확률적 전망 제시\n",
    "\n",
    "전문가 수준의 상세하고 실용적인 분석을 제공해주세요.\n",
    "\"\"\"\n",
    "            }\n",
    "            \n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "    \n",
    "    def generate_single_report_free(self, prompt_data: Dict) -> Dict:\n",
    "        \"\"\"무료 모델로 단일 헤게모니 리포트 생성\"\"\"\n",
    "        try:\n",
    "            # 시스템 프롬프트 구성\n",
    "            system_prompt = \"\"\"당신은 30년 경력의 전략 컨설턴트이자 산업 분석 전문가입니다. \n",
    "산업 헤게모니와 기업 지배력 변화에 대한 깊이 있는 통찰력을 가지고 있으며, \n",
    "McKinsey, BCG 수준의 고품질 전략 리포트를 작성할 수 있습니다.\n",
    "\n",
    "모든 분석은 다음 기준을 따라주세요:\n",
    "- 데이터 기반의 객관적 분석\n",
    "- 다각도 시나리오 검토\n",
    "- 구체적이고 실행 가능한 인사이트\n",
    "- 글로벌 관점과 국내 관점의 균형\n",
    "- 정량적 지표와 정성적 분석의 조화\"\"\"\n",
    "\n",
    "            # 전체 프롬프트 구성\n",
    "            full_prompt = f\"{system_prompt}\\n\\n사용자 요청:\\n{prompt_data['detailed_prompt']}\\n\\n전문가 분석:\"\n",
    "            \n",
    "            # 토크나이징\n",
    "            inputs = self.tokenizer(\n",
    "                full_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=2048,\n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # 무료 모델로 생성\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_new_tokens=1500,  # 충분한 길이\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # 응답 디코딩\n",
    "            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # 실제 생성된 부분만 추출\n",
    "            generated_report = generated_text.split(\"전문가 분석:\")[-1].strip()\n",
    "            \n",
    "            # 파인튜닝 형식으로 변환\n",
    "            training_data = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"당신은 산업 헤게모니와 기업 지배력 변화를 전문적으로 분석하는 전략 컨설턴트입니다. 깊이 있는 통찰력으로 미래 시장 지배 구조를 예측해주세요.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"{prompt_data['scenario']}에 대한 전문적인 헤게모니 분석 리포트를 작성해주세요.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": generated_report\n",
    "                    }\n",
    "                ],\n",
    "                \"metadata\": {\n",
    "                    \"id\": prompt_data[\"id\"],\n",
    "                    \"industry\": prompt_data[\"industry\"], \n",
    "                    \"template\": prompt_data[\"template\"],\n",
    "                    \"generation_date\": datetime.now().isoformat(),\n",
    "                    \"model\": self.free_models[self.model_choice][\"name\"],\n",
    "                    \"quality\": \"high_free\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return training_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 무료 모델 리포트 생성 실패 ({prompt_data['id']}): {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_massive_dataset_free(self, target_count: int = 1000, output_file: str = \"free_hegemony_reports_dataset.jsonl\"):\n",
    "        \"\"\"무료 모델로 대량 헤게모니 리포트 데이터셋 생성\"\"\"\n",
    "        print(f\"🆓 무료 모델 기반 대량 헤게모니 리포트 생성 시작\")\n",
    "        print(f\"🤖 사용 모델: {self.free_models[self.model_choice]['description']}\")\n",
    "        print(f\"🎯 목표: {target_count}개 리포트\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # 모델이 로드되지 않았으면 로드\n",
    "        if not self.model:\n",
    "            self.setup_model()\n",
    "        \n",
    "        # 프롬프트 생성\n",
    "        prompts = self.generate_hegemony_prompts(target_count)\n",
    "        print(f\"✅ {len(prompts)}개 프롬프트 생성 완료\")\n",
    "        \n",
    "        # 배치 생성\n",
    "        dataset = []\n",
    "        success_count = 0\n",
    "        batch_size = 5  # 무료 모델은 배치 크기를 작게\n",
    "        \n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batch = prompts[i:i+batch_size]\n",
    "            \n",
    "            print(f\"\\n📦 배치 {i//batch_size + 1}/{(len(prompts)-1)//batch_size + 1} 처리 중...\")\n",
    "            \n",
    "            for prompt_data in batch:\n",
    "                report = self.generate_single_report_free(prompt_data)\n",
    "                \n",
    "                if report:\n",
    "                    dataset.append(report)\n",
    "                    success_count += 1\n",
    "                    print(f\"  ✅ {prompt_data['id']} 완료 ({success_count}/{target_count})\")\n",
    "                else:\n",
    "                    print(f\"  ❌ {prompt_data['id']} 실패\")\n",
    "                \n",
    "                # 메모리 정리\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # 안정성을 위한 잠시 대기\n",
    "                time.sleep(0.5)\n",
    "            \n",
    "            # 중간 저장 (백업)\n",
    "            if i % (batch_size * 10) == 0 and dataset:\n",
    "                backup_file = f\"backup_free_{output_file}\"\n",
    "                with open(backup_file, 'w', encoding='utf-8') as f:\n",
    "                    for item in dataset:\n",
    "                        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "                print(f\"💾 중간 백업 저장: {backup_file}\")\n",
    "        \n",
    "        # 최종 저장\n",
    "        if dataset:\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for item in dataset:\n",
    "                    f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "            \n",
    "            file_size_mb = os.path.getsize(output_file) / 1024 / 1024\n",
    "            \n",
    "            print(f\"\\n🎉 무료 모델 기반 대량 리포트 생성 완료!\")\n",
    "            print(f\"🤖 사용 모델: {self.free_models[self.model_choice]['name']}\")\n",
    "            print(f\"📄 파일: {output_file}\")\n",
    "            print(f\"📊 생성된 리포트: {len(dataset)}개\")\n",
    "            print(f\"💾 파일 크기: {file_size_mb:.2f}MB\")\n",
    "            print(f\"📈 성공률: {success_count}/{target_count} ({success_count/target_count*100:.1f}%)\")\n",
    "            print(f\"💰 비용: $0 (완전 무료!)\")\n",
    "            \n",
    "            return output_file\n",
    "        else:\n",
    "            print(\"❌ 생성된 리포트가 없습니다.\")\n",
    "            return None\n",
    "\n",
    "# 무료 모델 비교 가이드\n",
    "def print_free_model_comparison():\n",
    "    \"\"\"무료 모델들 비교 가이드\"\"\"\n",
    "    print(\"🆓 무료 헤게모니 리포트 생성 모델 비교\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    models = {\n",
    "        \"qwen\": {\n",
    "            \"name\": \"Qwen2.5-14B-Instruct\",\n",
    "            \"pros\": \"★★★★★ 한국어 최고, 최신 모델\",\n",
    "            \"cons\": \"큰 메모리 필요 (14B)\",\n",
    "            \"recommended\": \"✅ 추천!\"\n",
    "        },\n",
    "        \"llama\": {\n",
    "            \"name\": \"Llama-3.1-8B-Instruct\", \n",
    "            \"pros\": \"★★★★☆ 검증된 성능, 안정적\",\n",
    "            \"cons\": \"한국어 성능 보통\",\n",
    "            \"recommended\": \"🔄 대안\"\n",
    "        },\n",
    "        \"gemma\": {\n",
    "            \"name\": \"Gemma-2-9B-IT\",\n",
    "            \"pros\": \"★★★☆☆ 구글 기술, 효율적\",\n",
    "            \"cons\": \"한국어 지원 제한적\",\n",
    "            \"recommended\": \"⚖️ 보통\"\n",
    "        },\n",
    "        \"mistral\": {\n",
    "            \"name\": \"Mistral-7B-Instruct\",\n",
    "            \"pros\": \"★★★☆☆ 빠른 속도, 적은 메모리\",\n",
    "            \"cons\": \"한국어 성능 낮음\",\n",
    "            \"recommended\": \"⚡ 빠름\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for key, info in models.items():\n",
    "        print(f\"\\n🤖 {info['name']}\")\n",
    "        print(f\"   장점: {info['pros']}\")\n",
    "        print(f\"   단점: {info['cons']}\")\n",
    "        print(f\"   추천: {info['recommended']}\")\n",
    "    \n",
    "    print(f\"\\n💡 추천 사용법:\")\n",
    "    print(f\"   generator = FreeHegemonyReportGenerator(model_choice='qwen')\")\n",
    "    print(f\"   generator.generate_massive_dataset_free(target_count=100)\")\n",
    "\n",
    "print(\"✅ 무료 오픈소스 헤게모니 리포트 생성 시스템 준비 완료\")\n",
    "print(\"💰 비용: $0 (완전 무료!)\")\n",
    "print(\"💡 사용법:\")\n",
    "print(\"  print_free_model_comparison()  # 모델 비교\")\n",
    "print(\"  generator = FreeHegemonyReportGenerator('qwen')\")\n",
    "print(\"  generator.generate_massive_dataset_free(100)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
