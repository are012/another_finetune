{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5097bc8",
   "metadata": {},
   "source": [
    "# 📊 완전 오프라인 RAG 리포트 생성기 (Pure Consumer)\n",
    "\n",
    "## 🎯 시스템 개요 \n",
    "이 노트북은 **100% 오프라인**으로 작동하는 Consumer입니다.\n",
    "외부 API 호출 없이 로컬 벡터 DB에서만 정보를 추출하여 리포트를 생성합니다.\n",
    "\n",
    "### 📋 핵심 특징\n",
    "- **완전 오프라인**: 외부 API 호출 전혀 없음\n",
    "- **순수 RAG**: 로컬 벡터 DB에서만 정보 검색\n",
    "- **템플릿 기반**: 규칙 기반 리포트 생성\n",
    "- **파인튜닝 친화**: 일관된 구조의 고품질 데이터셋 생성\n",
    "- **빠른 처리**: API 대기시간 없음\n",
    "\n",
    "### 🔄 Producer-Consumer 분리\n",
    "- **Producer** (`pipeline_update.py`): 데이터 수집 + API 호출 + 벡터 DB 저장\n",
    "- **Consumer** (이 노트북): 벡터 DB 검색 + 템플릿 기반 리포트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a143aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 라이브러리 임포트 완료 (순수 오프라인 모드)\n",
      "🚫 외부 API 의존성 없음\n"
     ]
    }
   ],
   "source": [
    "# 📦 필수 라이브러리 임포트 (API 관련 제외)\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# 벡터 DB 관련 (임베딩은 저장된 것만 사용)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# 노트북 출력용\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 완료 (순수 오프라인 모드)\")\n",
    "print(\"🚫 외부 API 의존성 없음\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f733d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 오프라인 설정 완료\n",
      "📁 벡터 DB 디렉토리: rag_db\n",
      "✅ 벡터 DB 발견\n"
     ]
    }
   ],
   "source": [
    "# 🔧 오프라인 설정\n",
    "DB_DIR = \"rag_db\"  # 벡터 DB 디렉토리\n",
    "GOOGLE_API_KEY = \"\"  # 임베딩 로드용만\n",
    "\n",
    "print(\"✅ 오프라인 설정 완료\")\n",
    "print(f\"📁 벡터 DB 디렉토리: {DB_DIR}\")\n",
    "\n",
    "# DB 존재 여부 확인\n",
    "if os.path.exists(DB_DIR):\n",
    "    print(\"✅ 벡터 DB 발견\")\n",
    "else:\n",
    "    print(\"❌ 벡터 DB를 찾을 수 없습니다. pipeline_update.py를 먼저 실행하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ba8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OfflineReportGenerator 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "class OfflineReportGenerator:\n",
    "    \"\"\"완전 오프라인 리포트 생성기 - 외부 API 호출 없음\"\"\"\n",
    "    \n",
    "    def __init__(self, db_dir: str = \"rag_db\", google_api_key: str = None):\n",
    "        \"\"\"\n",
    "        초기화 - 저장된 벡터 DB만 로드\n",
    "        \n",
    "        Args:\n",
    "            db_dir (str): 벡터 DB 디렉토리\n",
    "            google_api_key (str): 임베딩 로드용 (새로운 임베딩 생성 안함)\n",
    "        \"\"\"\n",
    "        self.db_dir = db_dir\n",
    "        \n",
    "        if not os.path.exists(db_dir):\n",
    "            raise FileNotFoundError(f\"벡터 DB 디렉토리를 찾을 수 없습니다: {db_dir}\")\n",
    "        \n",
    "        # 임베딩 모델 초기화 (기존 임베딩 로드용만)\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=google_api_key\n",
    "        )\n",
    "        \n",
    "        # 벡터 DB 로드 (저장된 데이터만 사용)\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 오프라인 RAG 시스템 초기화 완료\")\n",
    "        print(f\"📊 벡터 DB 컬렉션 수: {self.vectorstore._collection.count()}\")\n",
    "    \n",
    "    def get_available_companies(self) -> List[str]:\n",
    "        \"\"\"DB에서 사용 가능한 기업 목록 조회\"\"\"\n",
    "        try:\n",
    "            results = self.vectorstore.get()\n",
    "            companies = set()\n",
    "            \n",
    "            for metadata in results['metadatas']:\n",
    "                if 'company' in metadata:\n",
    "                    companies.add(metadata['company'])\n",
    "            \n",
    "            return sorted(list(companies))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ 기업 목록 조회 실패: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_company_data(self, company_name: str, k: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"특정 기업의 모든 관련 데이터 검색 및 분류\"\"\"\n",
    "        try:\n",
    "            # 기업명으로 검색\n",
    "            results = self.vectorstore.similarity_search(\n",
    "                query=f\"{company_name} 뉴스 공시 분석\",\n",
    "                k=k,\n",
    "                filter={\"company\": company_name}\n",
    "            )\n",
    "            \n",
    "            # 데이터 타입별로 분류\n",
    "            classified_data = {\n",
    "                \"news\": [],\n",
    "                \"disclosures\": [],\n",
    "                \"other\": [],\n",
    "                \"total_count\": len(results)\n",
    "            }\n",
    "            \n",
    "            for doc in results:\n",
    "                source = doc.metadata.get('source', 'other')\n",
    "                if source == 'news':\n",
    "                    classified_data[\"news\"].append(doc)\n",
    "                elif source == 'disclosure':\n",
    "                    classified_data[\"disclosures\"].append(doc)\n",
    "                else:\n",
    "                    classified_data[\"other\"].append(doc)\n",
    "            \n",
    "            return classified_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ {company_name} 데이터 검색 실패: {e}\")\n",
    "            return {\"news\": [], \"disclosures\": [], \"other\": [], \"total_count\": 0}\n",
    "    \n",
    "    def extract_key_insights(self, classified_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"검색된 데이터에서 핵심 인사이트 추출 (규칙 기반)\"\"\"\n",
    "        insights = {\n",
    "            \"positive_signals\": [],\n",
    "            \"negative_signals\": [],\n",
    "            \"key_events\": [],\n",
    "            \"disclosure_priorities\": defaultdict(int),\n",
    "            \"recent_news_count\": 0,\n",
    "            \"disclosure_count\": 0\n",
    "        }\n",
    "        \n",
    "        # 긍정/부정 키워드 정의\n",
    "        positive_keywords = ['증가', '상승', '성장', '확대', '투자', '개발', '협력', '계약', '수주', '매출']\n",
    "        negative_keywords = ['감소', '하락', '축소', '손실', '리스크', '우려', '취소', '지연', '문제']\n",
    "        \n",
    "        # 뉴스 분석\n",
    "        for news_doc in classified_data[\"news\"]:\n",
    "            content = news_doc.page_content.lower()\n",
    "            \n",
    "            # 긍정적 신호 탐지\n",
    "            for keyword in positive_keywords:\n",
    "                if keyword in content:\n",
    "                    insights[\"positive_signals\"].append({\n",
    "                        \"keyword\": keyword,\n",
    "                        \"source\": \"뉴스\",\n",
    "                        \"title\": news_doc.metadata.get('title', '')[:50]\n",
    "                    })\n",
    "            \n",
    "            # 부정적 신호 탐지\n",
    "            for keyword in negative_keywords:\n",
    "                if keyword in content:\n",
    "                    insights[\"negative_signals\"].append({\n",
    "                        \"keyword\": keyword,\n",
    "                        \"source\": \"뉴스\",\n",
    "                        \"title\": news_doc.metadata.get('title', '')[:50]\n",
    "                    })\n",
    "        \n",
    "        # 공시 분석\n",
    "        for disclosure_doc in classified_data[\"disclosures\"]:\n",
    "            priority = disclosure_doc.metadata.get('priority', 'unknown')\n",
    "            insights[\"disclosure_priorities\"][priority] += 1\n",
    "            \n",
    "            # 주요 공시 이벤트 추출\n",
    "            report_name = disclosure_doc.metadata.get('report_name', '')\n",
    "            if any(keyword in report_name for keyword in ['분기보고서', '사업보고서', '주요사항보고서']):\n",
    "                insights[\"key_events\"].append({\n",
    "                    \"type\": \"공시\",\n",
    "                    \"event\": report_name,\n",
    "                    \"priority\": priority\n",
    "                })\n",
    "        \n",
    "        insights[\"recent_news_count\"] = len(classified_data[\"news\"])\n",
    "        insights[\"disclosure_count\"] = len(classified_data[\"disclosures\"])\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def generate_offline_report(self, company_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"완전 오프라인 리포트 생성 (템플릿 기반)\"\"\"\n",
    "        print(f\"📊 {company_name} 오프라인 분석 시작...\")\n",
    "        \n",
    "        # 1. 데이터 검색\n",
    "        classified_data = self.search_company_data(company_name)\n",
    "        \n",
    "        if classified_data[\"total_count\"] == 0:\n",
    "            return {\n",
    "                \"company\": company_name,\n",
    "                \"status\": \"데이터 없음\",\n",
    "                \"message\": \"해당 기업의 데이터를 찾을 수 없습니다.\"\n",
    "            }\n",
    "        \n",
    "        # 2. 인사이트 추출\n",
    "        insights = self.extract_key_insights(classified_data)\n",
    "        \n",
    "        # 3. 리포트 생성\n",
    "        report = {\n",
    "            \"company\": company_name,\n",
    "            \"generation_date\": datetime.now().isoformat(),\n",
    "            \"data_summary\": {\n",
    "                \"total_documents\": classified_data[\"total_count\"],\n",
    "                \"news_articles\": len(classified_data[\"news\"]),\n",
    "                \"disclosures\": len(classified_data[\"disclosures\"])\n",
    "            },\n",
    "            \"investment_analysis\": self._create_investment_analysis(insights),\n",
    "            \"risk_assessment\": self._create_risk_assessment(insights),\n",
    "            \"recommendation\": self._create_recommendation(insights),\n",
    "            \"key_data_points\": self._extract_key_data_points(classified_data),\n",
    "            \"status\": \"완료\"\n",
    "        }\n",
    "        \n",
    "        print(f\"✅ {company_name} 오프라인 분석 완료\")\n",
    "        return report\n",
    "    \n",
    "    def _create_investment_analysis(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"투자 분석 섹션 생성\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        news_count = insights[\"recent_news_count\"]\n",
    "        \n",
    "        analysis = f\"\"\"## 📈 투자 분석\n",
    "\n",
    "### 데이터 기반 현황\n",
    "- 최근 뉴스 분석: {news_count}건\n",
    "- 공시 정보 분석: {insights['disclosure_count']}건\n",
    "- 긍정적 신호: {positive_count}개\n",
    "- 부정적 신호: {negative_count}개\n",
    "\n",
    "### 주요 긍정 요인\n",
    "\"\"\"\n",
    "        \n",
    "        if insights[\"positive_signals\"]:\n",
    "            for i, signal in enumerate(insights[\"positive_signals\"][:5], 1):\n",
    "                analysis += f\"- {signal['keyword']} 관련 이슈 ({signal['source']}): {signal['title']}...\\n\"\n",
    "        else:\n",
    "            analysis += \"- 현재 특별한 긍정적 신호는 발견되지 않음\\n\"\n",
    "        \n",
    "        analysis += \"\\n### 주요 우려 요인\\n\"\n",
    "        if insights[\"negative_signals\"]:\n",
    "            for i, signal in enumerate(insights[\"negative_signals\"][:5], 1):\n",
    "                analysis += f\"- {signal['keyword']} 관련 이슈 ({signal['source']}): {signal['title']}...\\n\"\n",
    "        else:\n",
    "            analysis += \"- 현재 특별한 우려 요인은 발견되지 않음\\n\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _create_risk_assessment(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"리스크 평가 섹션 생성\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        \n",
    "        # 간단한 리스크 점수 계산\n",
    "        if negative_count == 0:\n",
    "            risk_level = \"낮음\"\n",
    "        elif negative_count <= positive_count:\n",
    "            risk_level = \"보통\"\n",
    "        else:\n",
    "            risk_level = \"높음\"\n",
    "        \n",
    "        assessment = f\"\"\"## ⚠️ 리스크 평가\n",
    "\n",
    "### 리스크 수준: {risk_level}\n",
    "\n",
    "### 평가 근거\n",
    "- 긍정적 신호 vs 부정적 신호: {positive_count} vs {negative_count}\n",
    "- 공시 정보 활용도: {insights['disclosure_count']}건 분석\n",
    "\n",
    "### 주요 리스크 요인\n",
    "\"\"\"\n",
    "        \n",
    "        if insights[\"negative_signals\"]:\n",
    "            risk_keywords = Counter([signal['keyword'] for signal in insights[\"negative_signals\"]])\n",
    "            for keyword, count in risk_keywords.most_common(3):\n",
    "                assessment += f\"- {keyword} 관련 이슈: {count}건 확인\\n\"\n",
    "        else:\n",
    "            assessment += \"- 현재 데이터에서 특별한 리스크 요인은 확인되지 않음\\n\"\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def _create_recommendation(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"투자 의견 섹션 생성\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        \n",
    "        # 간단한 추천 로직\n",
    "        if positive_count > negative_count * 1.5:\n",
    "            recommendation = \"매수\"\n",
    "            rationale = \"긍정적 신호가 부정적 신호를 크게 상회하여 투자 매력도가 높음\"\n",
    "        elif negative_count > positive_count * 1.5:\n",
    "            recommendation = \"매도\"\n",
    "            rationale = \"부정적 신호가 긍정적 신호를 크게 상회하여 투자 위험도가 높음\"\n",
    "        else:\n",
    "            recommendation = \"보유\"\n",
    "            rationale = \"긍정적/부정적 신호가 균형을 이루어 신중한 접근이 필요\"\n",
    "        \n",
    "        return f\"\"\"## 💡 투자 의견\n",
    "\n",
    "### 추천 의견: {recommendation}\n",
    "\n",
    "### 근거\n",
    "{rationale}\n",
    "\n",
    "### 데이터 기반 점수\n",
    "- 긍정 지수: {positive_count}\n",
    "- 위험 지수: {negative_count}\n",
    "- 정보 풍부도: {insights['recent_news_count'] + insights['disclosure_count']}점\n",
    "\n",
    "### 주의사항\n",
    "본 분석은 수집된 뉴스 및 공시 데이터의 키워드 분석을 기반으로 하며, \n",
    "실제 투자 결정 시에는 추가적인 재무 분석 및 전문가 의견을 참고하시기 바랍니다.\n",
    "\"\"\"\n",
    "    \n",
    "    def _extract_key_data_points(self, classified_data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"핵심 데이터 포인트 추출\"\"\"\n",
    "        key_points = []\n",
    "        \n",
    "        # 최신 뉴스 상위 3개\n",
    "        for doc in classified_data[\"news\"][:3]:\n",
    "            key_points.append({\n",
    "                \"type\": \"뉴스\",\n",
    "                \"title\": doc.metadata.get('title', '제목 없음'),\n",
    "                \"date\": doc.metadata.get('collection_date', '날짜 없음'),\n",
    "                \"summary\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        # 중요 공시 상위 3개\n",
    "        for doc in classified_data[\"disclosures\"][:3]:\n",
    "            key_points.append({\n",
    "                \"type\": \"공시\",\n",
    "                \"title\": doc.metadata.get('report_name', '공시명 없음'),\n",
    "                \"priority\": doc.metadata.get('priority', '일반'),\n",
    "                \"date\": doc.metadata.get('collection_date', '날짜 없음'),\n",
    "                \"summary\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return key_points\n",
    "\n",
    "print(\"✅ OfflineReportGenerator 클래스 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18be37c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\are01\\AppData\\Local\\Temp\\ipykernel_7144\\1172487021.py:24: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  self.vectorstore = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 오프라인 RAG 시스템 초기화 완료\n",
      "📊 벡터 DB 컬렉션 수: 1767\n",
      "\n",
      "📋 분석 가능한 기업 목록 (96개):\n",
      "  1. CJ ENM\n",
      "  2. CJ대한통운\n",
      "  3. CJ제일제당\n",
      "  4. GS\n",
      "  5. GS칼텍스\n",
      "  6. HMM\n",
      "  7. KB국민은행\n",
      "  8. KB금융\n",
      "  9. KT\n",
      "  10. KT&G\n",
      "  11. LG\n",
      "  12. LG디스플레이\n",
      "  13. LG생활건강\n",
      "  14. LG에너지솔루션\n",
      "  15. LG이노텍\n",
      "  16. LG전자\n",
      "  17. LG화학\n",
      "  18. NAVER\n",
      "  19. POSCO홀딩스\n",
      "  20. SK\n",
      "  21. SK머티리얼즈\n",
      "  22. SK바이오팜\n",
      "  23. SK스퀘어\n",
      "  24. SK이노베이션\n",
      "  25. SK텔레콤\n",
      "  26. SK하이닉스\n",
      "  27. 고려아연\n",
      "  28. 기아\n",
      "  29. 기업은행\n",
      "  30. 넥슨게임즈\n",
      "  31. 넷마블\n",
      "  32. 녹십자\n",
      "  33. 농심\n",
      "  34. 대웅제약\n",
      "  35. 대한항공\n",
      "  36. 동원시스템즈\n",
      "  37. 동화약품\n",
      "  38. 두산\n",
      "  39. 두산에너빌리티\n",
      "  40. 롯데쇼핑\n",
      "  41. 롯데제과\n",
      "  42. 롯데칠성음료\n",
      "  43. 롯데케미칼\n",
      "  44. 부광약품\n",
      "  45. 삼성SDI\n",
      "  46. 삼성디스플레이\n",
      "  47. 삼성물산\n",
      "  48. 삼성바이오로직스\n",
      "  49. 삼성에스디에스\n",
      "  50. 삼성전기\n",
      "  51. 삼성전자\n",
      "  52. 삼성중공업\n",
      "  53. 삼성화재\n",
      "  54. 삼천리\n",
      "  55. 셀트리온\n",
      "  56. 셀트리온제약\n",
      "  57. 신세계\n",
      "  58. 신한은행\n",
      "  59. 신한지주\n",
      "  60. 아모레퍼시픽\n",
      "  61. 오뚜기\n",
      "  62. 우리금융지주\n",
      "  63. 위메이드\n",
      "  64. 유한양행\n",
      "  65. 이마트\n",
      "  66. 일동제약\n",
      "  67. 종근당\n",
      "  68. 카카오\n",
      "  69. 카카오뱅크\n",
      "  70. 카카오페이\n",
      "  71. 컴투스\n",
      "  72. 코웨이\n",
      "  73. 크래프톤\n",
      "  74. 펄어비스\n",
      "  75. 포스코DX\n",
      "  76. 포스코인터내셔널\n",
      "  77. 포스코퓨처엠\n",
      "  78. 하나금융지주\n",
      "  79. 하나은행\n",
      "  80. 한국가스공사\n",
      "  81. 한국전력공사\n",
      "  82. 한국조선해양\n",
      "  83. 한미반도체\n",
      "  84. 한미약품\n",
      "  85. 한화생명\n",
      "  86. 한화솔루션\n",
      "  87. 한화시스템\n",
      "  88. 한화에어로스페이스\n",
      "  89. 현대건설\n",
      "  90. 현대글로비스\n",
      "  91. 현대모비스\n",
      "  92. 현대오일뱅크\n",
      "  93. 현대자동차\n",
      "  94. 현대제철\n",
      "  95. 현대중공업\n",
      "  96. 홈플러스\n"
     ]
    }
   ],
   "source": [
    "# 🚀 오프라인 RAG 시스템 초기화\n",
    "try:\n",
    "    offline_rag = OfflineReportGenerator(\n",
    "        db_dir=DB_DIR,\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "    \n",
    "    # 사용 가능한 기업 목록 조회\n",
    "    available_companies = offline_rag.get_available_companies()\n",
    "    print(f\"\\n📋 분석 가능한 기업 목록 ({len(available_companies)}개):\")\n",
    "    for i, company in enumerate(available_companies, 1):\n",
    "        print(f\"  {i}. {company}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ 오프라인 RAG 시스템 초기화 실패: {e}\")\n",
    "    print(\"   pipeline_update.py를 먼저 실행하여 데이터를 수집하세요.\")\n",
    "    offline_rag = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e114d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 CJ ENM 분석 시작...\n",
      "\n",
      "📊 CJ ENM 오프라인 분석 시작...\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "\n",
      "================================================================================\n",
      "📈 CJ ENM 투자 분석 리포트 (오프라인)\n",
      "================================================================================\n",
      "생성 시간: 2025-08-04T18:34:30.094407\n",
      "분석 데이터: 총 9건\n",
      "  - 뉴스: 9건\n",
      "  - 공시: 0건\n",
      "\n",
      "\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "\n",
      "================================================================================\n",
      "📈 CJ ENM 투자 분석 리포트 (오프라인)\n",
      "================================================================================\n",
      "생성 시간: 2025-08-04T18:34:30.094407\n",
      "분석 데이터: 총 9건\n",
      "  - 뉴스: 9건\n",
      "  - 공시: 0건\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## 📈 투자 분석\n",
       "\n",
       "### 데이터 기반 현황\n",
       "- 최근 뉴스 분석: 9건\n",
       "- 공시 정보 분석: 0건\n",
       "- 긍정적 신호: 3개\n",
       "- 부정적 신호: 0개\n",
       "\n",
       "### 주요 긍정 요인\n",
       "- 투자 관련 이슈 (뉴스): 극장가 단기 부양책 한계?…'전독시' 부진에 투자 위축 가속...\n",
       "- 확대 관련 이슈 (뉴스): [오늘의 테마] '영상콘텐츠' VS '조선'...\n",
       "- 확대 관련 이슈 (뉴스): 콘진원, ‘K스토리&코믹스 인 아메리카’서 220억원 규모 수출 상담...\n",
       "\n",
       "### 주요 우려 요인\n",
       "- 현재 특별한 우려 요인은 발견되지 않음\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## ⚠️ 리스크 평가\n",
       "\n",
       "### 리스크 수준: 낮음\n",
       "\n",
       "### 평가 근거\n",
       "- 긍정적 신호 vs 부정적 신호: 3 vs 0\n",
       "- 공시 정보 활용도: 0건 분석\n",
       "\n",
       "### 주요 리스크 요인\n",
       "- 현재 데이터에서 특별한 리스크 요인은 확인되지 않음\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## 💡 투자 의견\n",
       "\n",
       "### 추천 의견: 매수\n",
       "\n",
       "### 근거\n",
       "긍정적 신호가 부정적 신호를 크게 상회하여 투자 매력도가 높음\n",
       "\n",
       "### 데이터 기반 점수\n",
       "- 긍정 지수: 3\n",
       "- 위험 지수: 0\n",
       "- 정보 풍부도: 9점\n",
       "\n",
       "### 주의사항\n",
       "본 분석은 수집된 뉴스 및 공시 데이터의 키워드 분석을 기반으로 하며, \n",
       "실제 투자 결정 시에는 추가적인 재무 분석 및 전문가 의견을 참고하시기 바랍니다.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📋 핵심 데이터 포인트\n",
      "================================================================================\n",
      "\n",
      "[1] 뉴스: 악마가 이사왔다, 임윤아·안보현 코믹 케미에 6천원 할인까지\n",
      "    요약: 제목: 악마가 이사왔다, 임윤아·안보현 코믹 케미에 6천원 할인까지\n",
      "내용: |중앙이코노미뉴스 이상민 기자|출처=CJ ENM  영화 '악마가 이사왔다'가 국민 영화관람 할인권 적용 ...\n",
      "    수집일: 2025-08-04\n",
      "\n",
      "[2] 뉴스: 극장가 단기 부양책 한계?…'전독시' 부진에 투자 위축 가속\n",
      "    요약: 제목: 극장가 단기 부양책 한계?…'전독시' 부진에 투자 위축 가속\n",
      "내용: 국내 대표 투자배급사인 CJ ENM도 복수 영화를 준비하고 있지만, 투자에 제동이 걸려 발표를 차일피일 ...\n",
      "    수집일: 2025-08-04\n",
      "\n",
      "[3] 뉴스: 임윤아x안보현 \"6000원 할인, 이렇게 받으세요\"…'악마가 이사왔다' 관람...\n",
      "    요약: 제목: 임윤아x안보현 \"6000원 할인, 이렇게 받으세요\"…'악마가 이사왔다' 관람...\n",
      "내용: 할인권은 CGV, 메가박스, 롯데시네마, 씨네큐 극장 홈페이지 또는 애플리케이션을 ...\n",
      "    수집일: 2025-08-04\n"
     ]
    }
   ],
   "source": [
    "# 📊 단일 기업 분석 실행\n",
    "if offline_rag and available_companies:\n",
    "    # 첫 번째 기업으로 테스트 (원하는 기업명으로 변경 가능)\n",
    "    test_company = available_companies[0]\n",
    "    print(f\"🎯 {test_company} 분석 시작...\\n\")\n",
    "    \n",
    "    # 오프라인 리포트 생성\n",
    "    report = offline_rag.generate_offline_report(test_company)\n",
    "    \n",
    "    if report[\"status\"] == \"완료\":\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"📈 {report['company']} 투자 분석 리포트 (오프라인)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"생성 시간: {report['generation_date']}\")\n",
    "        print(f\"분석 데이터: 총 {report['data_summary']['total_documents']}건\")\n",
    "        print(f\"  - 뉴스: {report['data_summary']['news_articles']}건\")\n",
    "        print(f\"  - 공시: {report['data_summary']['disclosures']}건\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # 투자 분석 출력\n",
    "        display(Markdown(report['investment_analysis']))\n",
    "        \n",
    "        # 리스크 평가 출력\n",
    "        display(Markdown(report['risk_assessment']))\n",
    "        \n",
    "        # 투자 의견 출력\n",
    "        display(Markdown(report['recommendation']))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📋 핵심 데이터 포인트\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, point in enumerate(report['key_data_points'][:5], 1):\n",
    "            print(f\"\\n[{i}] {point['type']}: {point['title']}\")\n",
    "            if 'priority' in point:\n",
    "                print(f\"    우선순위: {point['priority']}\")\n",
    "            print(f\"    요약: {point['summary']}\")\n",
    "            print(f\"    수집일: {point['date'][:10]}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ 리포트 생성 실패: {report.get('message', '알 수 없는 오류')}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ 시스템이 초기화되지 않았거나 분석 가능한 기업이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1281fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 파인튜닝 데이터셋 생성 함수 정의 완료\n"
     ]
    }
   ],
   "source": [
    "# 📝 파인튜닝 데이터셋 생성 함수\n",
    "def create_finetuning_dataset(offline_rag_system, companies: List[str], output_file: str = \"investment_reports_dataset.jsonl\"):\n",
    "    \"\"\"모든 기업에 대해 리포트를 생성하고 파인튜닝 데이터셋으로 저장\"\"\"\n",
    "    \n",
    "    print(f\"📦 파인튜닝 데이터셋 생성 시작: {len(companies)}개 기업\")\n",
    "    \n",
    "    dataset = []\n",
    "    successful_reports = 0\n",
    "    \n",
    "    for i, company in enumerate(companies, 1):\n",
    "        print(f\"\\n[{i}/{len(companies)}] {company} 처리 중...\")\n",
    "        \n",
    "        try:\n",
    "            # 오프라인 리포트 생성\n",
    "            report = offline_rag_system.generate_offline_report(company)\n",
    "            \n",
    "            if report[\"status\"] == \"완료\":\n",
    "                # 파인튜닝 형식으로 변환\n",
    "                training_example = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"{company}에 대한 투자 분석 리포트를 작성해주세요. 최근 뉴스 {report['data_summary']['news_articles']}건과 공시 {report['data_summary']['disclosures']}건의 데이터가 있습니다.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": f\"\"\"{report['investment_analysis']}\n",
    "\n",
    "{report['risk_assessment']}\n",
    "\n",
    "{report['recommendation']}\n",
    "\n",
    "## 📋 분석 근거 데이터\n",
    "- 총 분석 문서: {report['data_summary']['total_documents']}건\n",
    "- 뉴스 분석: {report['data_summary']['news_articles']}건\n",
    "- 공시 분석: {report['data_summary']['disclosures']}건\n",
    "- 분석 완료 시간: {report['generation_date']}\n",
    "\"\"\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                dataset.append(training_example)\n",
    "                successful_reports += 1\n",
    "                print(f\"  ✅ {company} 데이터셋 생성 완료\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  ⚠️ {company} 리포트 생성 실패: {report.get('message', '알 수 없는 오류')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {company} 처리 중 오류: {e}\")\n",
    "    \n",
    "    # JSONL 파일로 저장\n",
    "    if dataset:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in dataset:\n",
    "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"\\n🎉 파인튜닝 데이터셋 생성 완료!\")\n",
    "        print(f\"📄 파일: {output_file}\")\n",
    "        print(f\"📊 총 데이터: {len(dataset)}개 (성공률: {successful_reports}/{len(companies)})\")\n",
    "        \n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"❌ 생성된 데이터셋이 없습니다.\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ 파인튜닝 데이터셋 생성 함수 정의 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f96f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 전체 기업 배치 처리 시작...\n",
      "📦 파인튜닝 데이터셋 생성 시작: 96개 기업\n",
      "\n",
      "[1/96] CJ ENM 처리 중...\n",
      "📊 CJ ENM 오프라인 분석 시작...\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "  ✅ CJ ENM 데이터셋 생성 완료\n",
      "\n",
      "[2/96] CJ대한통운 처리 중...\n",
      "📊 CJ대한통운 오프라인 분석 시작...\n",
      "✅ CJ ENM 오프라인 분석 완료\n",
      "  ✅ CJ ENM 데이터셋 생성 완료\n",
      "\n",
      "[2/96] CJ대한통운 처리 중...\n",
      "📊 CJ대한통운 오프라인 분석 시작...\n",
      "✅ CJ대한통운 오프라인 분석 완료\n",
      "  ✅ CJ대한통운 데이터셋 생성 완료\n",
      "\n",
      "[3/96] CJ제일제당 처리 중...\n",
      "📊 CJ제일제당 오프라인 분석 시작...\n",
      "✅ CJ대한통운 오프라인 분석 완료\n",
      "  ✅ CJ대한통운 데이터셋 생성 완료\n",
      "\n",
      "[3/96] CJ제일제당 처리 중...\n",
      "📊 CJ제일제당 오프라인 분석 시작...\n",
      "✅ CJ제일제당 오프라인 분석 완료\n",
      "  ✅ CJ제일제당 데이터셋 생성 완료\n",
      "\n",
      "[4/96] GS 처리 중...\n",
      "📊 GS 오프라인 분석 시작...\n",
      "✅ CJ제일제당 오프라인 분석 완료\n",
      "  ✅ CJ제일제당 데이터셋 생성 완료\n",
      "\n",
      "[4/96] GS 처리 중...\n",
      "📊 GS 오프라인 분석 시작...\n",
      "✅ GS 오프라인 분석 완료\n",
      "  ✅ GS 데이터셋 생성 완료\n",
      "\n",
      "[5/96] GS칼텍스 처리 중...\n",
      "📊 GS칼텍스 오프라인 분석 시작...\n",
      "✅ GS 오프라인 분석 완료\n",
      "  ✅ GS 데이터셋 생성 완료\n",
      "\n",
      "[5/96] GS칼텍스 처리 중...\n",
      "📊 GS칼텍스 오프라인 분석 시작...\n",
      "✅ GS칼텍스 오프라인 분석 완료\n",
      "  ✅ GS칼텍스 데이터셋 생성 완료\n",
      "\n",
      "[6/96] HMM 처리 중...\n",
      "📊 HMM 오프라인 분석 시작...\n",
      "✅ GS칼텍스 오프라인 분석 완료\n",
      "  ✅ GS칼텍스 데이터셋 생성 완료\n",
      "\n",
      "[6/96] HMM 처리 중...\n",
      "📊 HMM 오프라인 분석 시작...\n",
      "✅ HMM 오프라인 분석 완료\n",
      "  ✅ HMM 데이터셋 생성 완료\n",
      "\n",
      "[7/96] KB국민은행 처리 중...\n",
      "📊 KB국민은행 오프라인 분석 시작...\n",
      "✅ HMM 오프라인 분석 완료\n",
      "  ✅ HMM 데이터셋 생성 완료\n",
      "\n",
      "[7/96] KB국민은행 처리 중...\n",
      "📊 KB국민은행 오프라인 분석 시작...\n",
      "✅ KB국민은행 오프라인 분석 완료\n",
      "  ✅ KB국민은행 데이터셋 생성 완료\n",
      "\n",
      "[8/96] KB금융 처리 중...\n",
      "📊 KB금융 오프라인 분석 시작...\n",
      "✅ KB국민은행 오프라인 분석 완료\n",
      "  ✅ KB국민은행 데이터셋 생성 완료\n",
      "\n",
      "[8/96] KB금융 처리 중...\n",
      "📊 KB금융 오프라인 분석 시작...\n",
      "✅ KB금융 오프라인 분석 완료\n",
      "  ✅ KB금융 데이터셋 생성 완료\n",
      "\n",
      "[9/96] KT 처리 중...\n",
      "📊 KT 오프라인 분석 시작...\n",
      "✅ KB금융 오프라인 분석 완료\n",
      "  ✅ KB금융 데이터셋 생성 완료\n",
      "\n",
      "[9/96] KT 처리 중...\n",
      "📊 KT 오프라인 분석 시작...\n",
      "✅ KT 오프라인 분석 완료\n",
      "  ✅ KT 데이터셋 생성 완료\n",
      "\n",
      "[10/96] KT&G 처리 중...\n",
      "📊 KT&G 오프라인 분석 시작...\n",
      "✅ KT 오프라인 분석 완료\n",
      "  ✅ KT 데이터셋 생성 완료\n",
      "\n",
      "[10/96] KT&G 처리 중...\n",
      "📊 KT&G 오프라인 분석 시작...\n",
      "✅ KT&G 오프라인 분석 완료\n",
      "  ✅ KT&G 데이터셋 생성 완료\n",
      "\n",
      "[11/96] LG 처리 중...\n",
      "📊 LG 오프라인 분석 시작...\n",
      "✅ KT&G 오프라인 분석 완료\n",
      "  ✅ KT&G 데이터셋 생성 완료\n",
      "\n",
      "[11/96] LG 처리 중...\n",
      "📊 LG 오프라인 분석 시작...\n",
      "✅ LG 오프라인 분석 완료\n",
      "  ✅ LG 데이터셋 생성 완료\n",
      "\n",
      "[12/96] LG디스플레이 처리 중...\n",
      "📊 LG디스플레이 오프라인 분석 시작...\n",
      "✅ LG 오프라인 분석 완료\n",
      "  ✅ LG 데이터셋 생성 완료\n",
      "\n",
      "[12/96] LG디스플레이 처리 중...\n",
      "📊 LG디스플레이 오프라인 분석 시작...\n",
      "✅ LG디스플레이 오프라인 분석 완료\n",
      "  ✅ LG디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[13/96] LG생활건강 처리 중...\n",
      "📊 LG생활건강 오프라인 분석 시작...\n",
      "✅ LG디스플레이 오프라인 분석 완료\n",
      "  ✅ LG디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[13/96] LG생활건강 처리 중...\n",
      "📊 LG생활건강 오프라인 분석 시작...\n",
      "✅ LG생활건강 오프라인 분석 완료\n",
      "  ✅ LG생활건강 데이터셋 생성 완료\n",
      "\n",
      "[14/96] LG에너지솔루션 처리 중...\n",
      "📊 LG에너지솔루션 오프라인 분석 시작...\n",
      "✅ LG생활건강 오프라인 분석 완료\n",
      "  ✅ LG생활건강 데이터셋 생성 완료\n",
      "\n",
      "[14/96] LG에너지솔루션 처리 중...\n",
      "📊 LG에너지솔루션 오프라인 분석 시작...\n",
      "✅ LG에너지솔루션 오프라인 분석 완료\n",
      "  ✅ LG에너지솔루션 데이터셋 생성 완료\n",
      "\n",
      "[15/96] LG이노텍 처리 중...\n",
      "📊 LG이노텍 오프라인 분석 시작...\n",
      "✅ LG에너지솔루션 오프라인 분석 완료\n",
      "  ✅ LG에너지솔루션 데이터셋 생성 완료\n",
      "\n",
      "[15/96] LG이노텍 처리 중...\n",
      "📊 LG이노텍 오프라인 분석 시작...\n",
      "✅ LG이노텍 오프라인 분석 완료\n",
      "  ✅ LG이노텍 데이터셋 생성 완료\n",
      "\n",
      "[16/96] LG전자 처리 중...\n",
      "📊 LG전자 오프라인 분석 시작...\n",
      "✅ LG이노텍 오프라인 분석 완료\n",
      "  ✅ LG이노텍 데이터셋 생성 완료\n",
      "\n",
      "[16/96] LG전자 처리 중...\n",
      "📊 LG전자 오프라인 분석 시작...\n",
      "✅ LG전자 오프라인 분석 완료\n",
      "  ✅ LG전자 데이터셋 생성 완료\n",
      "\n",
      "[17/96] LG화학 처리 중...\n",
      "📊 LG화학 오프라인 분석 시작...\n",
      "✅ LG전자 오프라인 분석 완료\n",
      "  ✅ LG전자 데이터셋 생성 완료\n",
      "\n",
      "[17/96] LG화학 처리 중...\n",
      "📊 LG화학 오프라인 분석 시작...\n",
      "✅ LG화학 오프라인 분석 완료\n",
      "  ✅ LG화학 데이터셋 생성 완료\n",
      "\n",
      "[18/96] NAVER 처리 중...\n",
      "📊 NAVER 오프라인 분석 시작...\n",
      "✅ LG화학 오프라인 분석 완료\n",
      "  ✅ LG화학 데이터셋 생성 완료\n",
      "\n",
      "[18/96] NAVER 처리 중...\n",
      "📊 NAVER 오프라인 분석 시작...\n",
      "✅ NAVER 오프라인 분석 완료\n",
      "  ✅ NAVER 데이터셋 생성 완료\n",
      "\n",
      "[19/96] POSCO홀딩스 처리 중...\n",
      "📊 POSCO홀딩스 오프라인 분석 시작...\n",
      "✅ NAVER 오프라인 분석 완료\n",
      "  ✅ NAVER 데이터셋 생성 완료\n",
      "\n",
      "[19/96] POSCO홀딩스 처리 중...\n",
      "📊 POSCO홀딩스 오프라인 분석 시작...\n",
      "✅ POSCO홀딩스 오프라인 분석 완료\n",
      "  ✅ POSCO홀딩스 데이터셋 생성 완료\n",
      "\n",
      "[20/96] SK 처리 중...\n",
      "📊 SK 오프라인 분석 시작...\n",
      "✅ POSCO홀딩스 오프라인 분석 완료\n",
      "  ✅ POSCO홀딩스 데이터셋 생성 완료\n",
      "\n",
      "[20/96] SK 처리 중...\n",
      "📊 SK 오프라인 분석 시작...\n",
      "✅ SK 오프라인 분석 완료\n",
      "  ✅ SK 데이터셋 생성 완료\n",
      "\n",
      "[21/96] SK머티리얼즈 처리 중...\n",
      "📊 SK머티리얼즈 오프라인 분석 시작...\n",
      "✅ SK 오프라인 분석 완료\n",
      "  ✅ SK 데이터셋 생성 완료\n",
      "\n",
      "[21/96] SK머티리얼즈 처리 중...\n",
      "📊 SK머티리얼즈 오프라인 분석 시작...\n",
      "✅ SK머티리얼즈 오프라인 분석 완료\n",
      "  ✅ SK머티리얼즈 데이터셋 생성 완료\n",
      "\n",
      "[22/96] SK바이오팜 처리 중...\n",
      "📊 SK바이오팜 오프라인 분석 시작...\n",
      "✅ SK머티리얼즈 오프라인 분석 완료\n",
      "  ✅ SK머티리얼즈 데이터셋 생성 완료\n",
      "\n",
      "[22/96] SK바이오팜 처리 중...\n",
      "📊 SK바이오팜 오프라인 분석 시작...\n",
      "✅ SK바이오팜 오프라인 분석 완료\n",
      "  ✅ SK바이오팜 데이터셋 생성 완료\n",
      "\n",
      "[23/96] SK스퀘어 처리 중...\n",
      "📊 SK스퀘어 오프라인 분석 시작...\n",
      "✅ SK바이오팜 오프라인 분석 완료\n",
      "  ✅ SK바이오팜 데이터셋 생성 완료\n",
      "\n",
      "[23/96] SK스퀘어 처리 중...\n",
      "📊 SK스퀘어 오프라인 분석 시작...\n",
      "✅ SK스퀘어 오프라인 분석 완료\n",
      "  ✅ SK스퀘어 데이터셋 생성 완료\n",
      "\n",
      "[24/96] SK이노베이션 처리 중...\n",
      "📊 SK이노베이션 오프라인 분석 시작...\n",
      "✅ SK스퀘어 오프라인 분석 완료\n",
      "  ✅ SK스퀘어 데이터셋 생성 완료\n",
      "\n",
      "[24/96] SK이노베이션 처리 중...\n",
      "📊 SK이노베이션 오프라인 분석 시작...\n",
      "✅ SK이노베이션 오프라인 분석 완료\n",
      "  ✅ SK이노베이션 데이터셋 생성 완료\n",
      "\n",
      "[25/96] SK텔레콤 처리 중...\n",
      "📊 SK텔레콤 오프라인 분석 시작...\n",
      "✅ SK이노베이션 오프라인 분석 완료\n",
      "  ✅ SK이노베이션 데이터셋 생성 완료\n",
      "\n",
      "[25/96] SK텔레콤 처리 중...\n",
      "📊 SK텔레콤 오프라인 분석 시작...\n",
      "✅ SK텔레콤 오프라인 분석 완료\n",
      "  ✅ SK텔레콤 데이터셋 생성 완료\n",
      "\n",
      "[26/96] SK하이닉스 처리 중...\n",
      "📊 SK하이닉스 오프라인 분석 시작...\n",
      "✅ SK텔레콤 오프라인 분석 완료\n",
      "  ✅ SK텔레콤 데이터셋 생성 완료\n",
      "\n",
      "[26/96] SK하이닉스 처리 중...\n",
      "📊 SK하이닉스 오프라인 분석 시작...\n",
      "✅ SK하이닉스 오프라인 분석 완료\n",
      "  ✅ SK하이닉스 데이터셋 생성 완료\n",
      "\n",
      "[27/96] 고려아연 처리 중...\n",
      "📊 고려아연 오프라인 분석 시작...\n",
      "✅ SK하이닉스 오프라인 분석 완료\n",
      "  ✅ SK하이닉스 데이터셋 생성 완료\n",
      "\n",
      "[27/96] 고려아연 처리 중...\n",
      "📊 고려아연 오프라인 분석 시작...\n",
      "✅ 고려아연 오프라인 분석 완료\n",
      "  ✅ 고려아연 데이터셋 생성 완료\n",
      "\n",
      "[28/96] 기아 처리 중...\n",
      "📊 기아 오프라인 분석 시작...\n",
      "✅ 고려아연 오프라인 분석 완료\n",
      "  ✅ 고려아연 데이터셋 생성 완료\n",
      "\n",
      "[28/96] 기아 처리 중...\n",
      "📊 기아 오프라인 분석 시작...\n",
      "✅ 기아 오프라인 분석 완료\n",
      "  ✅ 기아 데이터셋 생성 완료\n",
      "\n",
      "[29/96] 기업은행 처리 중...\n",
      "📊 기업은행 오프라인 분석 시작...\n",
      "✅ 기아 오프라인 분석 완료\n",
      "  ✅ 기아 데이터셋 생성 완료\n",
      "\n",
      "[29/96] 기업은행 처리 중...\n",
      "📊 기업은행 오프라인 분석 시작...\n",
      "✅ 기업은행 오프라인 분석 완료\n",
      "  ✅ 기업은행 데이터셋 생성 완료\n",
      "\n",
      "[30/96] 넥슨게임즈 처리 중...\n",
      "📊 넥슨게임즈 오프라인 분석 시작...\n",
      "✅ 기업은행 오프라인 분석 완료\n",
      "  ✅ 기업은행 데이터셋 생성 완료\n",
      "\n",
      "[30/96] 넥슨게임즈 처리 중...\n",
      "📊 넥슨게임즈 오프라인 분석 시작...\n",
      "✅ 넥슨게임즈 오프라인 분석 완료\n",
      "  ✅ 넥슨게임즈 데이터셋 생성 완료\n",
      "\n",
      "[31/96] 넷마블 처리 중...\n",
      "📊 넷마블 오프라인 분석 시작...\n",
      "✅ 넥슨게임즈 오프라인 분석 완료\n",
      "  ✅ 넥슨게임즈 데이터셋 생성 완료\n",
      "\n",
      "[31/96] 넷마블 처리 중...\n",
      "📊 넷마블 오프라인 분석 시작...\n",
      "✅ 넷마블 오프라인 분석 완료\n",
      "  ✅ 넷마블 데이터셋 생성 완료\n",
      "\n",
      "[32/96] 녹십자 처리 중...\n",
      "📊 녹십자 오프라인 분석 시작...\n",
      "✅ 넷마블 오프라인 분석 완료\n",
      "  ✅ 넷마블 데이터셋 생성 완료\n",
      "\n",
      "[32/96] 녹십자 처리 중...\n",
      "📊 녹십자 오프라인 분석 시작...\n",
      "✅ 녹십자 오프라인 분석 완료\n",
      "  ✅ 녹십자 데이터셋 생성 완료\n",
      "\n",
      "[33/96] 농심 처리 중...\n",
      "📊 농심 오프라인 분석 시작...\n",
      "✅ 녹십자 오프라인 분석 완료\n",
      "  ✅ 녹십자 데이터셋 생성 완료\n",
      "\n",
      "[33/96] 농심 처리 중...\n",
      "📊 농심 오프라인 분석 시작...\n",
      "✅ 농심 오프라인 분석 완료\n",
      "  ✅ 농심 데이터셋 생성 완료\n",
      "\n",
      "[34/96] 대웅제약 처리 중...\n",
      "📊 대웅제약 오프라인 분석 시작...\n",
      "✅ 농심 오프라인 분석 완료\n",
      "  ✅ 농심 데이터셋 생성 완료\n",
      "\n",
      "[34/96] 대웅제약 처리 중...\n",
      "📊 대웅제약 오프라인 분석 시작...\n",
      "✅ 대웅제약 오프라인 분석 완료\n",
      "  ✅ 대웅제약 데이터셋 생성 완료\n",
      "\n",
      "[35/96] 대한항공 처리 중...\n",
      "📊 대한항공 오프라인 분석 시작...\n",
      "✅ 대웅제약 오프라인 분석 완료\n",
      "  ✅ 대웅제약 데이터셋 생성 완료\n",
      "\n",
      "[35/96] 대한항공 처리 중...\n",
      "📊 대한항공 오프라인 분석 시작...\n",
      "✅ 대한항공 오프라인 분석 완료\n",
      "  ✅ 대한항공 데이터셋 생성 완료\n",
      "\n",
      "[36/96] 동원시스템즈 처리 중...\n",
      "📊 동원시스템즈 오프라인 분석 시작...\n",
      "✅ 대한항공 오프라인 분석 완료\n",
      "  ✅ 대한항공 데이터셋 생성 완료\n",
      "\n",
      "[36/96] 동원시스템즈 처리 중...\n",
      "📊 동원시스템즈 오프라인 분석 시작...\n",
      "✅ 동원시스템즈 오프라인 분석 완료\n",
      "  ✅ 동원시스템즈 데이터셋 생성 완료\n",
      "\n",
      "[37/96] 동화약품 처리 중...\n",
      "📊 동화약품 오프라인 분석 시작...\n",
      "✅ 동원시스템즈 오프라인 분석 완료\n",
      "  ✅ 동원시스템즈 데이터셋 생성 완료\n",
      "\n",
      "[37/96] 동화약품 처리 중...\n",
      "📊 동화약품 오프라인 분석 시작...\n",
      "✅ 동화약품 오프라인 분석 완료\n",
      "  ✅ 동화약품 데이터셋 생성 완료\n",
      "\n",
      "[38/96] 두산 처리 중...\n",
      "📊 두산 오프라인 분석 시작...\n",
      "✅ 동화약품 오프라인 분석 완료\n",
      "  ✅ 동화약품 데이터셋 생성 완료\n",
      "\n",
      "[38/96] 두산 처리 중...\n",
      "📊 두산 오프라인 분석 시작...\n",
      "✅ 두산 오프라인 분석 완료\n",
      "  ✅ 두산 데이터셋 생성 완료\n",
      "\n",
      "[39/96] 두산에너빌리티 처리 중...\n",
      "📊 두산에너빌리티 오프라인 분석 시작...\n",
      "✅ 두산 오프라인 분석 완료\n",
      "  ✅ 두산 데이터셋 생성 완료\n",
      "\n",
      "[39/96] 두산에너빌리티 처리 중...\n",
      "📊 두산에너빌리티 오프라인 분석 시작...\n",
      "✅ 두산에너빌리티 오프라인 분석 완료\n",
      "  ✅ 두산에너빌리티 데이터셋 생성 완료\n",
      "\n",
      "[40/96] 롯데쇼핑 처리 중...\n",
      "📊 롯데쇼핑 오프라인 분석 시작...\n",
      "✅ 두산에너빌리티 오프라인 분석 완료\n",
      "  ✅ 두산에너빌리티 데이터셋 생성 완료\n",
      "\n",
      "[40/96] 롯데쇼핑 처리 중...\n",
      "📊 롯데쇼핑 오프라인 분석 시작...\n",
      "✅ 롯데쇼핑 오프라인 분석 완료\n",
      "  ✅ 롯데쇼핑 데이터셋 생성 완료\n",
      "\n",
      "[41/96] 롯데제과 처리 중...\n",
      "📊 롯데제과 오프라인 분석 시작...\n",
      "✅ 롯데쇼핑 오프라인 분석 완료\n",
      "  ✅ 롯데쇼핑 데이터셋 생성 완료\n",
      "\n",
      "[41/96] 롯데제과 처리 중...\n",
      "📊 롯데제과 오프라인 분석 시작...\n",
      "✅ 롯데제과 오프라인 분석 완료\n",
      "  ✅ 롯데제과 데이터셋 생성 완료\n",
      "\n",
      "[42/96] 롯데칠성음료 처리 중...\n",
      "📊 롯데칠성음료 오프라인 분석 시작...\n",
      "✅ 롯데제과 오프라인 분석 완료\n",
      "  ✅ 롯데제과 데이터셋 생성 완료\n",
      "\n",
      "[42/96] 롯데칠성음료 처리 중...\n",
      "📊 롯데칠성음료 오프라인 분석 시작...\n",
      "✅ 롯데칠성음료 오프라인 분석 완료\n",
      "  ✅ 롯데칠성음료 데이터셋 생성 완료\n",
      "\n",
      "[43/96] 롯데케미칼 처리 중...\n",
      "📊 롯데케미칼 오프라인 분석 시작...\n",
      "✅ 롯데칠성음료 오프라인 분석 완료\n",
      "  ✅ 롯데칠성음료 데이터셋 생성 완료\n",
      "\n",
      "[43/96] 롯데케미칼 처리 중...\n",
      "📊 롯데케미칼 오프라인 분석 시작...\n",
      "✅ 롯데케미칼 오프라인 분석 완료\n",
      "  ✅ 롯데케미칼 데이터셋 생성 완료\n",
      "\n",
      "[44/96] 부광약품 처리 중...\n",
      "📊 부광약품 오프라인 분석 시작...\n",
      "✅ 롯데케미칼 오프라인 분석 완료\n",
      "  ✅ 롯데케미칼 데이터셋 생성 완료\n",
      "\n",
      "[44/96] 부광약품 처리 중...\n",
      "📊 부광약품 오프라인 분석 시작...\n",
      "✅ 부광약품 오프라인 분석 완료\n",
      "  ✅ 부광약품 데이터셋 생성 완료\n",
      "\n",
      "[45/96] 삼성SDI 처리 중...\n",
      "📊 삼성SDI 오프라인 분석 시작...\n",
      "✅ 부광약품 오프라인 분석 완료\n",
      "  ✅ 부광약품 데이터셋 생성 완료\n",
      "\n",
      "[45/96] 삼성SDI 처리 중...\n",
      "📊 삼성SDI 오프라인 분석 시작...\n",
      "✅ 삼성SDI 오프라인 분석 완료\n",
      "  ✅ 삼성SDI 데이터셋 생성 완료\n",
      "\n",
      "[46/96] 삼성디스플레이 처리 중...\n",
      "📊 삼성디스플레이 오프라인 분석 시작...\n",
      "✅ 삼성SDI 오프라인 분석 완료\n",
      "  ✅ 삼성SDI 데이터셋 생성 완료\n",
      "\n",
      "[46/96] 삼성디스플레이 처리 중...\n",
      "📊 삼성디스플레이 오프라인 분석 시작...\n",
      "✅ 삼성디스플레이 오프라인 분석 완료\n",
      "  ✅ 삼성디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[47/96] 삼성물산 처리 중...\n",
      "📊 삼성물산 오프라인 분석 시작...\n",
      "✅ 삼성디스플레이 오프라인 분석 완료\n",
      "  ✅ 삼성디스플레이 데이터셋 생성 완료\n",
      "\n",
      "[47/96] 삼성물산 처리 중...\n",
      "📊 삼성물산 오프라인 분석 시작...\n",
      "✅ 삼성물산 오프라인 분석 완료\n",
      "  ✅ 삼성물산 데이터셋 생성 완료\n",
      "\n",
      "[48/96] 삼성바이오로직스 처리 중...\n",
      "📊 삼성바이오로직스 오프라인 분석 시작...\n",
      "✅ 삼성물산 오프라인 분석 완료\n",
      "  ✅ 삼성물산 데이터셋 생성 완료\n",
      "\n",
      "[48/96] 삼성바이오로직스 처리 중...\n",
      "📊 삼성바이오로직스 오프라인 분석 시작...\n",
      "✅ 삼성바이오로직스 오프라인 분석 완료\n",
      "  ✅ 삼성바이오로직스 데이터셋 생성 완료\n",
      "\n",
      "[49/96] 삼성에스디에스 처리 중...\n",
      "📊 삼성에스디에스 오프라인 분석 시작...\n",
      "✅ 삼성바이오로직스 오프라인 분석 완료\n",
      "  ✅ 삼성바이오로직스 데이터셋 생성 완료\n",
      "\n",
      "[49/96] 삼성에스디에스 처리 중...\n",
      "📊 삼성에스디에스 오프라인 분석 시작...\n",
      "✅ 삼성에스디에스 오프라인 분석 완료\n",
      "  ✅ 삼성에스디에스 데이터셋 생성 완료\n",
      "\n",
      "[50/96] 삼성전기 처리 중...\n",
      "📊 삼성전기 오프라인 분석 시작...\n",
      "✅ 삼성에스디에스 오프라인 분석 완료\n",
      "  ✅ 삼성에스디에스 데이터셋 생성 완료\n",
      "\n",
      "[50/96] 삼성전기 처리 중...\n",
      "📊 삼성전기 오프라인 분석 시작...\n",
      "✅ 삼성전기 오프라인 분석 완료\n",
      "  ✅ 삼성전기 데이터셋 생성 완료\n",
      "\n",
      "[51/96] 삼성전자 처리 중...\n",
      "📊 삼성전자 오프라인 분석 시작...\n",
      "✅ 삼성전기 오프라인 분석 완료\n",
      "  ✅ 삼성전기 데이터셋 생성 완료\n",
      "\n",
      "[51/96] 삼성전자 처리 중...\n",
      "📊 삼성전자 오프라인 분석 시작...\n",
      "✅ 삼성전자 오프라인 분석 완료\n",
      "  ✅ 삼성전자 데이터셋 생성 완료\n",
      "\n",
      "[52/96] 삼성중공업 처리 중...\n",
      "📊 삼성중공업 오프라인 분석 시작...\n",
      "✅ 삼성전자 오프라인 분석 완료\n",
      "  ✅ 삼성전자 데이터셋 생성 완료\n",
      "\n",
      "[52/96] 삼성중공업 처리 중...\n",
      "📊 삼성중공업 오프라인 분석 시작...\n",
      "✅ 삼성중공업 오프라인 분석 완료\n",
      "  ✅ 삼성중공업 데이터셋 생성 완료\n",
      "\n",
      "[53/96] 삼성화재 처리 중...\n",
      "📊 삼성화재 오프라인 분석 시작...\n",
      "✅ 삼성중공업 오프라인 분석 완료\n",
      "  ✅ 삼성중공업 데이터셋 생성 완료\n",
      "\n",
      "[53/96] 삼성화재 처리 중...\n",
      "📊 삼성화재 오프라인 분석 시작...\n",
      "✅ 삼성화재 오프라인 분석 완료\n",
      "  ✅ 삼성화재 데이터셋 생성 완료\n",
      "\n",
      "[54/96] 삼천리 처리 중...\n",
      "📊 삼천리 오프라인 분석 시작...\n",
      "✅ 삼성화재 오프라인 분석 완료\n",
      "  ✅ 삼성화재 데이터셋 생성 완료\n",
      "\n",
      "[54/96] 삼천리 처리 중...\n",
      "📊 삼천리 오프라인 분석 시작...\n",
      "✅ 삼천리 오프라인 분석 완료\n",
      "  ✅ 삼천리 데이터셋 생성 완료\n",
      "\n",
      "[55/96] 셀트리온 처리 중...\n",
      "📊 셀트리온 오프라인 분석 시작...\n",
      "✅ 삼천리 오프라인 분석 완료\n",
      "  ✅ 삼천리 데이터셋 생성 완료\n",
      "\n",
      "[55/96] 셀트리온 처리 중...\n",
      "📊 셀트리온 오프라인 분석 시작...\n",
      "✅ 셀트리온 오프라인 분석 완료\n",
      "  ✅ 셀트리온 데이터셋 생성 완료\n",
      "\n",
      "[56/96] 셀트리온제약 처리 중...\n",
      "📊 셀트리온제약 오프라인 분석 시작...\n",
      "✅ 셀트리온 오프라인 분석 완료\n",
      "  ✅ 셀트리온 데이터셋 생성 완료\n",
      "\n",
      "[56/96] 셀트리온제약 처리 중...\n",
      "📊 셀트리온제약 오프라인 분석 시작...\n",
      "✅ 셀트리온제약 오프라인 분석 완료\n",
      "  ✅ 셀트리온제약 데이터셋 생성 완료\n",
      "\n",
      "[57/96] 신세계 처리 중...\n",
      "📊 신세계 오프라인 분석 시작...\n",
      "✅ 셀트리온제약 오프라인 분석 완료\n",
      "  ✅ 셀트리온제약 데이터셋 생성 완료\n",
      "\n",
      "[57/96] 신세계 처리 중...\n",
      "📊 신세계 오프라인 분석 시작...\n",
      "✅ 신세계 오프라인 분석 완료\n",
      "  ✅ 신세계 데이터셋 생성 완료\n",
      "\n",
      "[58/96] 신한은행 처리 중...\n",
      "📊 신한은행 오프라인 분석 시작...\n",
      "✅ 신세계 오프라인 분석 완료\n",
      "  ✅ 신세계 데이터셋 생성 완료\n",
      "\n",
      "[58/96] 신한은행 처리 중...\n",
      "📊 신한은행 오프라인 분석 시작...\n",
      "✅ 신한은행 오프라인 분석 완료\n",
      "  ✅ 신한은행 데이터셋 생성 완료\n",
      "\n",
      "[59/96] 신한지주 처리 중...\n",
      "📊 신한지주 오프라인 분석 시작...\n",
      "✅ 신한은행 오프라인 분석 완료\n",
      "  ✅ 신한은행 데이터셋 생성 완료\n",
      "\n",
      "[59/96] 신한지주 처리 중...\n",
      "📊 신한지주 오프라인 분석 시작...\n",
      "✅ 신한지주 오프라인 분석 완료\n",
      "  ✅ 신한지주 데이터셋 생성 완료\n",
      "\n",
      "[60/96] 아모레퍼시픽 처리 중...\n",
      "📊 아모레퍼시픽 오프라인 분석 시작...\n",
      "✅ 신한지주 오프라인 분석 완료\n",
      "  ✅ 신한지주 데이터셋 생성 완료\n",
      "\n",
      "[60/96] 아모레퍼시픽 처리 중...\n",
      "📊 아모레퍼시픽 오프라인 분석 시작...\n",
      "✅ 아모레퍼시픽 오프라인 분석 완료\n",
      "  ✅ 아모레퍼시픽 데이터셋 생성 완료\n",
      "\n",
      "[61/96] 오뚜기 처리 중...\n",
      "📊 오뚜기 오프라인 분석 시작...\n",
      "✅ 아모레퍼시픽 오프라인 분석 완료\n",
      "  ✅ 아모레퍼시픽 데이터셋 생성 완료\n",
      "\n",
      "[61/96] 오뚜기 처리 중...\n",
      "📊 오뚜기 오프라인 분석 시작...\n",
      "✅ 오뚜기 오프라인 분석 완료\n",
      "  ✅ 오뚜기 데이터셋 생성 완료\n",
      "\n",
      "[62/96] 우리금융지주 처리 중...\n",
      "📊 우리금융지주 오프라인 분석 시작...\n",
      "✅ 오뚜기 오프라인 분석 완료\n",
      "  ✅ 오뚜기 데이터셋 생성 완료\n",
      "\n",
      "[62/96] 우리금융지주 처리 중...\n",
      "📊 우리금융지주 오프라인 분석 시작...\n",
      "✅ 우리금융지주 오프라인 분석 완료\n",
      "  ✅ 우리금융지주 데이터셋 생성 완료\n",
      "\n",
      "[63/96] 위메이드 처리 중...\n",
      "📊 위메이드 오프라인 분석 시작...\n",
      "✅ 우리금융지주 오프라인 분석 완료\n",
      "  ✅ 우리금융지주 데이터셋 생성 완료\n",
      "\n",
      "[63/96] 위메이드 처리 중...\n",
      "📊 위메이드 오프라인 분석 시작...\n",
      "✅ 위메이드 오프라인 분석 완료\n",
      "  ✅ 위메이드 데이터셋 생성 완료\n",
      "\n",
      "[64/96] 유한양행 처리 중...\n",
      "📊 유한양행 오프라인 분석 시작...\n",
      "✅ 위메이드 오프라인 분석 완료\n",
      "  ✅ 위메이드 데이터셋 생성 완료\n",
      "\n",
      "[64/96] 유한양행 처리 중...\n",
      "📊 유한양행 오프라인 분석 시작...\n",
      "✅ 유한양행 오프라인 분석 완료\n",
      "  ✅ 유한양행 데이터셋 생성 완료\n",
      "\n",
      "[65/96] 이마트 처리 중...\n",
      "📊 이마트 오프라인 분석 시작...\n",
      "✅ 유한양행 오프라인 분석 완료\n",
      "  ✅ 유한양행 데이터셋 생성 완료\n",
      "\n",
      "[65/96] 이마트 처리 중...\n",
      "📊 이마트 오프라인 분석 시작...\n",
      "✅ 이마트 오프라인 분석 완료\n",
      "  ✅ 이마트 데이터셋 생성 완료\n",
      "\n",
      "[66/96] 일동제약 처리 중...\n",
      "📊 일동제약 오프라인 분석 시작...\n",
      "✅ 이마트 오프라인 분석 완료\n",
      "  ✅ 이마트 데이터셋 생성 완료\n",
      "\n",
      "[66/96] 일동제약 처리 중...\n",
      "📊 일동제약 오프라인 분석 시작...\n",
      "✅ 일동제약 오프라인 분석 완료\n",
      "  ✅ 일동제약 데이터셋 생성 완료\n",
      "\n",
      "[67/96] 종근당 처리 중...\n",
      "📊 종근당 오프라인 분석 시작...\n",
      "✅ 일동제약 오프라인 분석 완료\n",
      "  ✅ 일동제약 데이터셋 생성 완료\n",
      "\n",
      "[67/96] 종근당 처리 중...\n",
      "📊 종근당 오프라인 분석 시작...\n",
      "✅ 종근당 오프라인 분석 완료\n",
      "  ✅ 종근당 데이터셋 생성 완료\n",
      "\n",
      "[68/96] 카카오 처리 중...\n",
      "📊 카카오 오프라인 분석 시작...\n",
      "✅ 종근당 오프라인 분석 완료\n",
      "  ✅ 종근당 데이터셋 생성 완료\n",
      "\n",
      "[68/96] 카카오 처리 중...\n",
      "📊 카카오 오프라인 분석 시작...\n",
      "✅ 카카오 오프라인 분석 완료\n",
      "  ✅ 카카오 데이터셋 생성 완료\n",
      "\n",
      "[69/96] 카카오뱅크 처리 중...\n",
      "📊 카카오뱅크 오프라인 분석 시작...\n",
      "✅ 카카오 오프라인 분석 완료\n",
      "  ✅ 카카오 데이터셋 생성 완료\n",
      "\n",
      "[69/96] 카카오뱅크 처리 중...\n",
      "📊 카카오뱅크 오프라인 분석 시작...\n",
      "✅ 카카오뱅크 오프라인 분석 완료\n",
      "  ✅ 카카오뱅크 데이터셋 생성 완료\n",
      "\n",
      "[70/96] 카카오페이 처리 중...\n",
      "📊 카카오페이 오프라인 분석 시작...\n",
      "✅ 카카오뱅크 오프라인 분석 완료\n",
      "  ✅ 카카오뱅크 데이터셋 생성 완료\n",
      "\n",
      "[70/96] 카카오페이 처리 중...\n",
      "📊 카카오페이 오프라인 분석 시작...\n",
      "✅ 카카오페이 오프라인 분석 완료\n",
      "  ✅ 카카오페이 데이터셋 생성 완료\n",
      "\n",
      "[71/96] 컴투스 처리 중...\n",
      "📊 컴투스 오프라인 분석 시작...\n",
      "✅ 카카오페이 오프라인 분석 완료\n",
      "  ✅ 카카오페이 데이터셋 생성 완료\n",
      "\n",
      "[71/96] 컴투스 처리 중...\n",
      "📊 컴투스 오프라인 분석 시작...\n",
      "✅ 컴투스 오프라인 분석 완료\n",
      "  ✅ 컴투스 데이터셋 생성 완료\n",
      "\n",
      "[72/96] 코웨이 처리 중...\n",
      "📊 코웨이 오프라인 분석 시작...\n",
      "✅ 컴투스 오프라인 분석 완료\n",
      "  ✅ 컴투스 데이터셋 생성 완료\n",
      "\n",
      "[72/96] 코웨이 처리 중...\n",
      "📊 코웨이 오프라인 분석 시작...\n",
      "✅ 코웨이 오프라인 분석 완료\n",
      "  ✅ 코웨이 데이터셋 생성 완료\n",
      "\n",
      "[73/96] 크래프톤 처리 중...\n",
      "📊 크래프톤 오프라인 분석 시작...\n",
      "✅ 코웨이 오프라인 분석 완료\n",
      "  ✅ 코웨이 데이터셋 생성 완료\n",
      "\n",
      "[73/96] 크래프톤 처리 중...\n",
      "📊 크래프톤 오프라인 분석 시작...\n",
      "✅ 크래프톤 오프라인 분석 완료\n",
      "  ✅ 크래프톤 데이터셋 생성 완료\n",
      "\n",
      "[74/96] 펄어비스 처리 중...\n",
      "📊 펄어비스 오프라인 분석 시작...\n",
      "✅ 크래프톤 오프라인 분석 완료\n",
      "  ✅ 크래프톤 데이터셋 생성 완료\n",
      "\n",
      "[74/96] 펄어비스 처리 중...\n",
      "📊 펄어비스 오프라인 분석 시작...\n",
      "✅ 펄어비스 오프라인 분석 완료\n",
      "  ✅ 펄어비스 데이터셋 생성 완료\n",
      "\n",
      "[75/96] 포스코DX 처리 중...\n",
      "📊 포스코DX 오프라인 분석 시작...\n",
      "✅ 펄어비스 오프라인 분석 완료\n",
      "  ✅ 펄어비스 데이터셋 생성 완료\n",
      "\n",
      "[75/96] 포스코DX 처리 중...\n",
      "📊 포스코DX 오프라인 분석 시작...\n",
      "✅ 포스코DX 오프라인 분석 완료\n",
      "  ✅ 포스코DX 데이터셋 생성 완료\n",
      "\n",
      "[76/96] 포스코인터내셔널 처리 중...\n",
      "📊 포스코인터내셔널 오프라인 분석 시작...\n",
      "✅ 포스코DX 오프라인 분석 완료\n",
      "  ✅ 포스코DX 데이터셋 생성 완료\n",
      "\n",
      "[76/96] 포스코인터내셔널 처리 중...\n",
      "📊 포스코인터내셔널 오프라인 분석 시작...\n",
      "✅ 포스코인터내셔널 오프라인 분석 완료\n",
      "  ✅ 포스코인터내셔널 데이터셋 생성 완료\n",
      "\n",
      "[77/96] 포스코퓨처엠 처리 중...\n",
      "📊 포스코퓨처엠 오프라인 분석 시작...\n",
      "✅ 포스코인터내셔널 오프라인 분석 완료\n",
      "  ✅ 포스코인터내셔널 데이터셋 생성 완료\n",
      "\n",
      "[77/96] 포스코퓨처엠 처리 중...\n",
      "📊 포스코퓨처엠 오프라인 분석 시작...\n",
      "✅ 포스코퓨처엠 오프라인 분석 완료\n",
      "  ✅ 포스코퓨처엠 데이터셋 생성 완료\n",
      "\n",
      "[78/96] 하나금융지주 처리 중...\n",
      "📊 하나금융지주 오프라인 분석 시작...\n",
      "✅ 포스코퓨처엠 오프라인 분석 완료\n",
      "  ✅ 포스코퓨처엠 데이터셋 생성 완료\n",
      "\n",
      "[78/96] 하나금융지주 처리 중...\n",
      "📊 하나금융지주 오프라인 분석 시작...\n",
      "✅ 하나금융지주 오프라인 분석 완료\n",
      "  ✅ 하나금융지주 데이터셋 생성 완료\n",
      "\n",
      "[79/96] 하나은행 처리 중...\n",
      "📊 하나은행 오프라인 분석 시작...\n",
      "✅ 하나금융지주 오프라인 분석 완료\n",
      "  ✅ 하나금융지주 데이터셋 생성 완료\n",
      "\n",
      "[79/96] 하나은행 처리 중...\n",
      "📊 하나은행 오프라인 분석 시작...\n",
      "✅ 하나은행 오프라인 분석 완료\n",
      "  ✅ 하나은행 데이터셋 생성 완료\n",
      "\n",
      "[80/96] 한국가스공사 처리 중...\n",
      "📊 한국가스공사 오프라인 분석 시작...\n",
      "✅ 하나은행 오프라인 분석 완료\n",
      "  ✅ 하나은행 데이터셋 생성 완료\n",
      "\n",
      "[80/96] 한국가스공사 처리 중...\n",
      "📊 한국가스공사 오프라인 분석 시작...\n",
      "✅ 한국가스공사 오프라인 분석 완료\n",
      "  ✅ 한국가스공사 데이터셋 생성 완료\n",
      "\n",
      "[81/96] 한국전력공사 처리 중...\n",
      "📊 한국전력공사 오프라인 분석 시작...\n",
      "✅ 한국가스공사 오프라인 분석 완료\n",
      "  ✅ 한국가스공사 데이터셋 생성 완료\n",
      "\n",
      "[81/96] 한국전력공사 처리 중...\n",
      "📊 한국전력공사 오프라인 분석 시작...\n",
      "✅ 한국전력공사 오프라인 분석 완료\n",
      "  ✅ 한국전력공사 데이터셋 생성 완료\n",
      "\n",
      "[82/96] 한국조선해양 처리 중...\n",
      "📊 한국조선해양 오프라인 분석 시작...\n",
      "✅ 한국전력공사 오프라인 분석 완료\n",
      "  ✅ 한국전력공사 데이터셋 생성 완료\n",
      "\n",
      "[82/96] 한국조선해양 처리 중...\n",
      "📊 한국조선해양 오프라인 분석 시작...\n",
      "✅ 한국조선해양 오프라인 분석 완료\n",
      "  ✅ 한국조선해양 데이터셋 생성 완료\n",
      "\n",
      "[83/96] 한미반도체 처리 중...\n",
      "📊 한미반도체 오프라인 분석 시작...\n",
      "✅ 한국조선해양 오프라인 분석 완료\n",
      "  ✅ 한국조선해양 데이터셋 생성 완료\n",
      "\n",
      "[83/96] 한미반도체 처리 중...\n",
      "📊 한미반도체 오프라인 분석 시작...\n",
      "✅ 한미반도체 오프라인 분석 완료\n",
      "  ✅ 한미반도체 데이터셋 생성 완료\n",
      "\n",
      "[84/96] 한미약품 처리 중...\n",
      "📊 한미약품 오프라인 분석 시작...\n",
      "✅ 한미반도체 오프라인 분석 완료\n",
      "  ✅ 한미반도체 데이터셋 생성 완료\n",
      "\n",
      "[84/96] 한미약품 처리 중...\n",
      "📊 한미약품 오프라인 분석 시작...\n",
      "✅ 한미약품 오프라인 분석 완료\n",
      "  ✅ 한미약품 데이터셋 생성 완료\n",
      "\n",
      "[85/96] 한화생명 처리 중...\n",
      "📊 한화생명 오프라인 분석 시작...\n",
      "✅ 한미약품 오프라인 분석 완료\n",
      "  ✅ 한미약품 데이터셋 생성 완료\n",
      "\n",
      "[85/96] 한화생명 처리 중...\n",
      "📊 한화생명 오프라인 분석 시작...\n",
      "✅ 한화생명 오프라인 분석 완료\n",
      "  ✅ 한화생명 데이터셋 생성 완료\n",
      "\n",
      "[86/96] 한화솔루션 처리 중...\n",
      "📊 한화솔루션 오프라인 분석 시작...\n",
      "✅ 한화생명 오프라인 분석 완료\n",
      "  ✅ 한화생명 데이터셋 생성 완료\n",
      "\n",
      "[86/96] 한화솔루션 처리 중...\n",
      "📊 한화솔루션 오프라인 분석 시작...\n",
      "✅ 한화솔루션 오프라인 분석 완료\n",
      "  ✅ 한화솔루션 데이터셋 생성 완료\n",
      "\n",
      "[87/96] 한화시스템 처리 중...\n",
      "📊 한화시스템 오프라인 분석 시작...\n",
      "✅ 한화솔루션 오프라인 분석 완료\n",
      "  ✅ 한화솔루션 데이터셋 생성 완료\n",
      "\n",
      "[87/96] 한화시스템 처리 중...\n",
      "📊 한화시스템 오프라인 분석 시작...\n",
      "✅ 한화시스템 오프라인 분석 완료\n",
      "  ✅ 한화시스템 데이터셋 생성 완료\n",
      "\n",
      "[88/96] 한화에어로스페이스 처리 중...\n",
      "📊 한화에어로스페이스 오프라인 분석 시작...\n",
      "✅ 한화시스템 오프라인 분석 완료\n",
      "  ✅ 한화시스템 데이터셋 생성 완료\n",
      "\n",
      "[88/96] 한화에어로스페이스 처리 중...\n",
      "📊 한화에어로스페이스 오프라인 분석 시작...\n",
      "✅ 한화에어로스페이스 오프라인 분석 완료\n",
      "  ✅ 한화에어로스페이스 데이터셋 생성 완료\n",
      "\n",
      "[89/96] 현대건설 처리 중...\n",
      "📊 현대건설 오프라인 분석 시작...\n",
      "✅ 한화에어로스페이스 오프라인 분석 완료\n",
      "  ✅ 한화에어로스페이스 데이터셋 생성 완료\n",
      "\n",
      "[89/96] 현대건설 처리 중...\n",
      "📊 현대건설 오프라인 분석 시작...\n",
      "✅ 현대건설 오프라인 분석 완료\n",
      "  ✅ 현대건설 데이터셋 생성 완료\n",
      "\n",
      "[90/96] 현대글로비스 처리 중...\n",
      "📊 현대글로비스 오프라인 분석 시작...\n",
      "✅ 현대건설 오프라인 분석 완료\n",
      "  ✅ 현대건설 데이터셋 생성 완료\n",
      "\n",
      "[90/96] 현대글로비스 처리 중...\n",
      "📊 현대글로비스 오프라인 분석 시작...\n",
      "✅ 현대글로비스 오프라인 분석 완료\n",
      "  ✅ 현대글로비스 데이터셋 생성 완료\n",
      "\n",
      "[91/96] 현대모비스 처리 중...\n",
      "📊 현대모비스 오프라인 분석 시작...\n",
      "✅ 현대글로비스 오프라인 분석 완료\n",
      "  ✅ 현대글로비스 데이터셋 생성 완료\n",
      "\n",
      "[91/96] 현대모비스 처리 중...\n",
      "📊 현대모비스 오프라인 분석 시작...\n",
      "✅ 현대모비스 오프라인 분석 완료\n",
      "  ✅ 현대모비스 데이터셋 생성 완료\n",
      "\n",
      "[92/96] 현대오일뱅크 처리 중...\n",
      "📊 현대오일뱅크 오프라인 분석 시작...\n",
      "✅ 현대모비스 오프라인 분석 완료\n",
      "  ✅ 현대모비스 데이터셋 생성 완료\n",
      "\n",
      "[92/96] 현대오일뱅크 처리 중...\n",
      "📊 현대오일뱅크 오프라인 분석 시작...\n",
      "✅ 현대오일뱅크 오프라인 분석 완료\n",
      "  ✅ 현대오일뱅크 데이터셋 생성 완료\n",
      "\n",
      "[93/96] 현대자동차 처리 중...\n",
      "📊 현대자동차 오프라인 분석 시작...\n",
      "✅ 현대오일뱅크 오프라인 분석 완료\n",
      "  ✅ 현대오일뱅크 데이터셋 생성 완료\n",
      "\n",
      "[93/96] 현대자동차 처리 중...\n",
      "📊 현대자동차 오프라인 분석 시작...\n",
      "✅ 현대자동차 오프라인 분석 완료\n",
      "  ✅ 현대자동차 데이터셋 생성 완료\n",
      "\n",
      "[94/96] 현대제철 처리 중...\n",
      "📊 현대제철 오프라인 분석 시작...\n",
      "✅ 현대자동차 오프라인 분석 완료\n",
      "  ✅ 현대자동차 데이터셋 생성 완료\n",
      "\n",
      "[94/96] 현대제철 처리 중...\n",
      "📊 현대제철 오프라인 분석 시작...\n",
      "✅ 현대제철 오프라인 분석 완료\n",
      "  ✅ 현대제철 데이터셋 생성 완료\n",
      "\n",
      "[95/96] 현대중공업 처리 중...\n",
      "📊 현대중공업 오프라인 분석 시작...\n",
      "✅ 현대제철 오프라인 분석 완료\n",
      "  ✅ 현대제철 데이터셋 생성 완료\n",
      "\n",
      "[95/96] 현대중공업 처리 중...\n",
      "📊 현대중공업 오프라인 분석 시작...\n",
      "✅ 현대중공업 오프라인 분석 완료\n",
      "  ✅ 현대중공업 데이터셋 생성 완료\n",
      "\n",
      "[96/96] 홈플러스 처리 중...\n",
      "📊 홈플러스 오프라인 분석 시작...\n",
      "✅ 현대중공업 오프라인 분석 완료\n",
      "  ✅ 현대중공업 데이터셋 생성 완료\n",
      "\n",
      "[96/96] 홈플러스 처리 중...\n",
      "📊 홈플러스 오프라인 분석 시작...\n",
      "✅ 홈플러스 오프라인 분석 완료\n",
      "  ✅ 홈플러스 데이터셋 생성 완료\n",
      "\n",
      "🎉 파인튜닝 데이터셋 생성 완료!\n",
      "📄 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 총 데이터: 96개 (성공률: 96/96)\n",
      "\n",
      "✅ 배치 처리 완료!\n",
      "📁 생성된 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 파일 크기: 0.20 MB\n",
      "\n",
      "📋 샘플 데이터 구조:\n",
      "  - 메시지 수: 3\n",
      "  - 시스템 프롬프트 길이: 55자\n",
      "  - 사용자 질문 길이: 57자\n",
      "  - 어시스턴트 답변 길이: 738자\n",
      "✅ 홈플러스 오프라인 분석 완료\n",
      "  ✅ 홈플러스 데이터셋 생성 완료\n",
      "\n",
      "🎉 파인튜닝 데이터셋 생성 완료!\n",
      "📄 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 총 데이터: 96개 (성공률: 96/96)\n",
      "\n",
      "✅ 배치 처리 완료!\n",
      "📁 생성된 파일: offline_investment_reports_dataset.jsonl\n",
      "📊 파일 크기: 0.20 MB\n",
      "\n",
      "📋 샘플 데이터 구조:\n",
      "  - 메시지 수: 3\n",
      "  - 시스템 프롬프트 길이: 55자\n",
      "  - 사용자 질문 길이: 57자\n",
      "  - 어시스턴트 답변 길이: 738자\n"
     ]
    }
   ],
   "source": [
    "# 🚀 전체 기업 배치 처리 및 파인튜닝 데이터셋 생성\n",
    "if offline_rag and available_companies:\n",
    "    print(\"🔄 전체 기업 배치 처리 시작...\")\n",
    "    \n",
    "    # 파인튜닝 데이터셋 생성\n",
    "    dataset_file = create_finetuning_dataset(\n",
    "        offline_rag_system=offline_rag,\n",
    "        companies=available_companies,\n",
    "        output_file=\"offline_investment_reports_dataset.jsonl\"\n",
    "    )\n",
    "    \n",
    "    if dataset_file:\n",
    "        print(f\"\\n✅ 배치 처리 완료!\")\n",
    "        print(f\"📁 생성된 파일: {dataset_file}\")\n",
    "        \n",
    "        # 파일 크기 확인\n",
    "        if os.path.exists(dataset_file):\n",
    "            file_size = os.path.getsize(dataset_file) / 1024 / 1024  # MB\n",
    "            print(f\"📊 파일 크기: {file_size:.2f} MB\")\n",
    "            \n",
    "            # 샘플 데이터 확인\n",
    "            with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline()\n",
    "                sample_data = json.loads(first_line)\n",
    "                \n",
    "            print(f\"\\n📋 샘플 데이터 구조:\")\n",
    "            print(f\"  - 메시지 수: {len(sample_data['messages'])}\")\n",
    "            print(f\"  - 시스템 프롬프트 길이: {len(sample_data['messages'][0]['content'])}자\")\n",
    "            print(f\"  - 사용자 질문 길이: {len(sample_data['messages'][1]['content'])}자\")\n",
    "            print(f\"  - 어시스턴트 답변 길이: {len(sample_data['messages'][2]['content'])}자\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ 배치 처리를 위한 시스템이 준비되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be151286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 오프라인 RAG 시스템 성능 통계\n",
      "==================================================\n",
      "📄 총 벡터 문서 수: 1,767개\n",
      "\n",
      "🏢 기업별 데이터 분포:\n",
      "  SK하이닉스: 72개 문서\n",
      "  LG에너지솔루션: 66개 문서\n",
      "  삼성전자: 60개 문서\n",
      "  카카오: 60개 문서\n",
      "  SK이노베이션: 54개 문서\n",
      "  현대자동차: 52개 문서\n",
      "  LG화학: 44개 문서\n",
      "  삼성바이오로직스: 44개 문서\n",
      "  포스코퓨처엠: 42개 문서\n",
      "  기아: 40개 문서\n",
      "  삼성SDI: 40개 문서\n",
      "  NAVER: 34개 문서\n",
      "  신한지주: 33개 문서\n",
      "  KB금융: 30개 문서\n",
      "  KT&G: 30개 문서\n",
      "  LG생활건강: 30개 문서\n",
      "  LG전자: 30개 문서\n",
      "  POSCO홀딩스: 30개 문서\n",
      "  SK텔레콤: 30개 문서\n",
      "  고려아연: 30개 문서\n",
      "  삼성물산: 30개 문서\n",
      "  삼성에스디에스: 30개 문서\n",
      "  삼성화재: 30개 문서\n",
      "  셀트리온: 30개 문서\n",
      "  하나금융지주: 30개 문서\n",
      "  한국전력공사: 30개 문서\n",
      "  한화솔루션: 30개 문서\n",
      "  현대중공업: 30개 문서\n",
      "  현대모비스: 27개 문서\n",
      "  부광약품: 15개 문서\n",
      "  이마트: 14개 문서\n",
      "  현대건설: 14개 문서\n",
      "  크래프톤: 13개 문서\n",
      "  한화에어로스페이스: 13개 문서\n",
      "  삼성중공업: 12개 문서\n",
      "  아모레퍼시픽: 12개 문서\n",
      "  컴투스: 12개 문서\n",
      "  LG: 11개 문서\n",
      "  대한항공: 11개 문서\n",
      "  동화약품: 11개 문서\n",
      "  롯데케미칼: 11개 문서\n",
      "  유한양행: 11개 문서\n",
      "  일동제약: 11개 문서\n",
      "  CJ대한통운: 10개 문서\n",
      "  CJ제일제당: 10개 문서\n",
      "  GS: 10개 문서\n",
      "  GS칼텍스: 10개 문서\n",
      "  HMM: 10개 문서\n",
      "  LG이노텍: 10개 문서\n",
      "  기업은행: 10개 문서\n",
      "  넷마블: 10개 문서\n",
      "  녹십자: 10개 문서\n",
      "  농심: 10개 문서\n",
      "  대웅제약: 10개 문서\n",
      "  동원시스템즈: 10개 문서\n",
      "  두산: 10개 문서\n",
      "  두산에너빌리티: 10개 문서\n",
      "  롯데칠성음료: 10개 문서\n",
      "  삼성디스플레이: 10개 문서\n",
      "  삼성전기: 10개 문서\n",
      "  삼천리: 10개 문서\n",
      "  신세계: 10개 문서\n",
      "  신한은행: 10개 문서\n",
      "  오뚜기: 10개 문서\n",
      "  위메이드: 10개 문서\n",
      "  종근당: 10개 문서\n",
      "  코웨이: 10개 문서\n",
      "  펄어비스: 10개 문서\n",
      "  포스코인터내셔널: 10개 문서\n",
      "  하나은행: 10개 문서\n",
      "  한미반도체: 10개 문서\n",
      "  한미약품: 10개 문서\n",
      "  한화생명: 10개 문서\n",
      "  현대오일뱅크: 10개 문서\n",
      "  현대제철: 10개 문서\n",
      "  홈플러스: 10개 문서\n",
      "  CJ ENM: 9개 문서\n",
      "  KB국민은행: 9개 문서\n",
      "  LG디스플레이: 9개 문서\n",
      "  SK: 9개 문서\n",
      "  롯데제과: 9개 문서\n",
      "  셀트리온제약: 9개 문서\n",
      "  카카오뱅크: 9개 문서\n",
      "  포스코DX: 9개 문서\n",
      "  현대글로비스: 9개 문서\n",
      "  SK바이오팜: 8개 문서\n",
      "  롯데쇼핑: 8개 문서\n",
      "  우리금융지주: 8개 문서\n",
      "  한국가스공사: 8개 문서\n",
      "  KT: 7개 문서\n",
      "  SK스퀘어: 7개 문서\n",
      "  한국조선해양: 6개 문서\n",
      "  한화시스템: 5개 문서\n",
      "  SK머티리얼즈: 4개 문서\n",
      "  카카오페이: 4개 문서\n",
      "  넥슨게임즈: 2개 문서\n",
      "\n",
      "🎯 시스템 특징:\n",
      "  ✅ 100% 오프라인 동작\n",
      "  ✅ 외부 API 호출 없음\n",
      "  ✅ 실시간 리포트 생성\n",
      "  ✅ 파인튜닝 데이터셋 자동 생성\n",
      "  ✅ 템플릿 기반 일관성\n",
      "\n",
      "🚀 Producer-Consumer 완전 분리 달성!\n",
      "  📥 Producer: pipeline_update.py (API 호출 + 데이터 수집)\n",
      "  📤 Consumer: rag_report_generator.ipynb (순수 오프라인 분석)\n",
      "\n",
      "🏢 기업별 데이터 분포:\n",
      "  SK하이닉스: 72개 문서\n",
      "  LG에너지솔루션: 66개 문서\n",
      "  삼성전자: 60개 문서\n",
      "  카카오: 60개 문서\n",
      "  SK이노베이션: 54개 문서\n",
      "  현대자동차: 52개 문서\n",
      "  LG화학: 44개 문서\n",
      "  삼성바이오로직스: 44개 문서\n",
      "  포스코퓨처엠: 42개 문서\n",
      "  기아: 40개 문서\n",
      "  삼성SDI: 40개 문서\n",
      "  NAVER: 34개 문서\n",
      "  신한지주: 33개 문서\n",
      "  KB금융: 30개 문서\n",
      "  KT&G: 30개 문서\n",
      "  LG생활건강: 30개 문서\n",
      "  LG전자: 30개 문서\n",
      "  POSCO홀딩스: 30개 문서\n",
      "  SK텔레콤: 30개 문서\n",
      "  고려아연: 30개 문서\n",
      "  삼성물산: 30개 문서\n",
      "  삼성에스디에스: 30개 문서\n",
      "  삼성화재: 30개 문서\n",
      "  셀트리온: 30개 문서\n",
      "  하나금융지주: 30개 문서\n",
      "  한국전력공사: 30개 문서\n",
      "  한화솔루션: 30개 문서\n",
      "  현대중공업: 30개 문서\n",
      "  현대모비스: 27개 문서\n",
      "  부광약품: 15개 문서\n",
      "  이마트: 14개 문서\n",
      "  현대건설: 14개 문서\n",
      "  크래프톤: 13개 문서\n",
      "  한화에어로스페이스: 13개 문서\n",
      "  삼성중공업: 12개 문서\n",
      "  아모레퍼시픽: 12개 문서\n",
      "  컴투스: 12개 문서\n",
      "  LG: 11개 문서\n",
      "  대한항공: 11개 문서\n",
      "  동화약품: 11개 문서\n",
      "  롯데케미칼: 11개 문서\n",
      "  유한양행: 11개 문서\n",
      "  일동제약: 11개 문서\n",
      "  CJ대한통운: 10개 문서\n",
      "  CJ제일제당: 10개 문서\n",
      "  GS: 10개 문서\n",
      "  GS칼텍스: 10개 문서\n",
      "  HMM: 10개 문서\n",
      "  LG이노텍: 10개 문서\n",
      "  기업은행: 10개 문서\n",
      "  넷마블: 10개 문서\n",
      "  녹십자: 10개 문서\n",
      "  농심: 10개 문서\n",
      "  대웅제약: 10개 문서\n",
      "  동원시스템즈: 10개 문서\n",
      "  두산: 10개 문서\n",
      "  두산에너빌리티: 10개 문서\n",
      "  롯데칠성음료: 10개 문서\n",
      "  삼성디스플레이: 10개 문서\n",
      "  삼성전기: 10개 문서\n",
      "  삼천리: 10개 문서\n",
      "  신세계: 10개 문서\n",
      "  신한은행: 10개 문서\n",
      "  오뚜기: 10개 문서\n",
      "  위메이드: 10개 문서\n",
      "  종근당: 10개 문서\n",
      "  코웨이: 10개 문서\n",
      "  펄어비스: 10개 문서\n",
      "  포스코인터내셔널: 10개 문서\n",
      "  하나은행: 10개 문서\n",
      "  한미반도체: 10개 문서\n",
      "  한미약품: 10개 문서\n",
      "  한화생명: 10개 문서\n",
      "  현대오일뱅크: 10개 문서\n",
      "  현대제철: 10개 문서\n",
      "  홈플러스: 10개 문서\n",
      "  CJ ENM: 9개 문서\n",
      "  KB국민은행: 9개 문서\n",
      "  LG디스플레이: 9개 문서\n",
      "  SK: 9개 문서\n",
      "  롯데제과: 9개 문서\n",
      "  셀트리온제약: 9개 문서\n",
      "  카카오뱅크: 9개 문서\n",
      "  포스코DX: 9개 문서\n",
      "  현대글로비스: 9개 문서\n",
      "  SK바이오팜: 8개 문서\n",
      "  롯데쇼핑: 8개 문서\n",
      "  우리금융지주: 8개 문서\n",
      "  한국가스공사: 8개 문서\n",
      "  KT: 7개 문서\n",
      "  SK스퀘어: 7개 문서\n",
      "  한국조선해양: 6개 문서\n",
      "  한화시스템: 5개 문서\n",
      "  SK머티리얼즈: 4개 문서\n",
      "  카카오페이: 4개 문서\n",
      "  넥슨게임즈: 2개 문서\n",
      "\n",
      "🎯 시스템 특징:\n",
      "  ✅ 100% 오프라인 동작\n",
      "  ✅ 외부 API 호출 없음\n",
      "  ✅ 실시간 리포트 생성\n",
      "  ✅ 파인튜닝 데이터셋 자동 생성\n",
      "  ✅ 템플릿 기반 일관성\n",
      "\n",
      "🚀 Producer-Consumer 완전 분리 달성!\n",
      "  📥 Producer: pipeline_update.py (API 호출 + 데이터 수집)\n",
      "  📤 Consumer: rag_report_generator.ipynb (순수 오프라인 분석)\n"
     ]
    }
   ],
   "source": [
    "# 📈 시스템 성능 통계\n",
    "if offline_rag:\n",
    "    print(\"📊 오프라인 RAG 시스템 성능 통계\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 벡터 DB 통계\n",
    "    total_docs = offline_rag.vectorstore._collection.count()\n",
    "    print(f\"📄 총 벡터 문서 수: {total_docs:,}개\")\n",
    "    \n",
    "    # 기업별 데이터 분포\n",
    "    company_distribution = {}\n",
    "    for company in available_companies:\n",
    "        data = offline_rag.search_company_data(company, k=100)\n",
    "        company_distribution[company] = data['total_count']\n",
    "    \n",
    "    print(f\"\\n🏢 기업별 데이터 분포:\")\n",
    "    for company, count in sorted(company_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {company}: {count:,}개 문서\")\n",
    "    \n",
    "    # 시스템 특징\n",
    "    print(f\"\\n🎯 시스템 특징:\")\n",
    "    print(f\"  ✅ 100% 오프라인 동작\")\n",
    "    print(f\"  ✅ 외부 API 호출 없음\")\n",
    "    print(f\"  ✅ 실시간 리포트 생성\")\n",
    "    print(f\"  ✅ 파인튜닝 데이터셋 자동 생성\")\n",
    "    print(f\"  ✅ 템플릿 기반 일관성\")\n",
    "    \n",
    "    print(f\"\\n🚀 Producer-Consumer 완전 분리 달성!\")\n",
    "    print(f\"  📥 Producer: pipeline_update.py (API 호출 + 데이터 수집)\")\n",
    "    print(f\"  📤 Consumer: rag_report_generator.ipynb (순수 오프라인 분석)\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 시스템 통계를 가져올 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd1eaded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 파인튜닝 데이터셋 상세 분석 시작...\n",
      "📄 파일명: offline_investment_reports_dataset.jsonl\n",
      "📊 파일 크기: 0.20 MB\n",
      "🔢 총 데이터 수: 96개\n",
      "\n",
      "📝 메시지 구조: 3개 메시지\n",
      "  [1] system:\n",
      "      📏 길이: 55자\n",
      "      📖 미리보기: \"당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요....\"\n",
      "  [2] user:\n",
      "      📏 길이: 57자\n",
      "      📖 미리보기: \"CJ ENM에 대한 투자 분석 리포트를 작성해주세요. 최근 뉴스 9건과 공시 0건의 데이터가 있습니다....\"\n",
      "  [3] assistant:\n",
      "      📏 길이: 738자\n",
      "      📖 미리보기: \"## 📈 투자 분석  ### 데이터 기반 현황 - 최근 뉴스 분석: 9건 - 공시 정보 분석: 0건 - 긍정적 신호: 3개 - 부정적 신호: 0개  ### 주요 긍정 요인 - 투자...\"\n",
      "\n",
      "📈 품질 지표:\n",
      "  💬 평균 메시지 길이: 283자\n",
      "  📄 총 텍스트 길이: 850자\n",
      "\n",
      "🎲 랜덤 샘플 분석 (5개):\n",
      "    📊 GS칼텍스: 648자\n",
      "    📊 KB국민은행: 813자\n",
      "    📊 KB금융: 1,126자\n",
      "    📊 KT: 636자\n",
      "    📊 GS: 882자\n",
      "  🏢 분석된 기업: GS칼텍스, KB국민은행, KB금융 등\n",
      "  📏 평균 응답 길이: 821자\n",
      "\n",
      "✅ OpenAI 파인튜닝 호환성:\n",
      "  ✅ JSONL 형식: 올바름\n",
      "  ✅ messages 구조: 표준 형식\n",
      "  ✅ 3-role 시스템: system, user, assistant\n",
      "  ✅ 일관된 템플릿: 투자 분석 형식\n",
      "  ✅ 한국어 인코딩: UTF-8\n",
      "\n",
      "⚡ 분석 완료!\n",
      "⏱️ 소요 시간: 0.00초\n",
      "🚀 데이터셋 상태: 파인튜닝 준비 완료!\n"
     ]
    }
   ],
   "source": [
    "# 📊 파인튜닝 데이터셋 상세 분석 (매우 빠름 - 몇 초 내)\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"🔍 파인튜닝 데이터셋 상세 분석 시작...\")\n",
    "start_time = time.time()\n",
    "\n",
    "filename = 'offline_investment_reports_dataset.jsonl'\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    # 1. 기본 파일 정보\n",
    "    file_size_mb = os.path.getsize(filename) / 1024 / 1024\n",
    "    print(f\"📄 파일명: {filename}\")\n",
    "    print(f\"📊 파일 크기: {file_size_mb:.2f} MB\")\n",
    "    \n",
    "    # 2. 데이터 구조 분석\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    total_lines = len(lines)\n",
    "    print(f\"🔢 총 데이터 수: {total_lines:,}개\")\n",
    "    \n",
    "    # 3. 샘플 데이터 분석\n",
    "    if lines:\n",
    "        sample = json.loads(lines[0])\n",
    "        messages = sample.get('messages', [])\n",
    "        \n",
    "        print(f\"\\n📝 메시지 구조: {len(messages)}개 메시지\")\n",
    "        \n",
    "        # 각 메시지 상세 분석\n",
    "        total_chars = 0\n",
    "        for i, msg in enumerate(messages):\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            content_len = len(content)\n",
    "            total_chars += content_len\n",
    "            \n",
    "            # 내용 미리보기 (첫 100자)\n",
    "            preview = content[:100].replace('\\n', ' ').strip()\n",
    "            \n",
    "            print(f\"  [{i+1}] {role}:\")\n",
    "            print(f\"      📏 길이: {content_len:,}자\")\n",
    "            print(f\"      📖 미리보기: \\\"{preview}...\\\"\")\n",
    "        \n",
    "        print(f\"\\n📈 품질 지표:\")\n",
    "        print(f\"  💬 평균 메시지 길이: {total_chars // len(messages):,}자\")\n",
    "        print(f\"  📄 총 텍스트 길이: {total_chars:,}자\")\n",
    "        \n",
    "        # 4. 랜덤 샘플 몇 개 더 확인\n",
    "        if total_lines > 1:\n",
    "            import random\n",
    "            random_indices = random.sample(range(min(10, total_lines)), min(5, total_lines))\n",
    "            \n",
    "            print(f\"\\n🎲 랜덤 샘플 분석 ({len(random_indices)}개):\")\n",
    "            \n",
    "            companies_analyzed = []\n",
    "            avg_lengths = []\n",
    "            \n",
    "            for idx in random_indices:\n",
    "                sample = json.loads(lines[idx])\n",
    "                user_msg = sample['messages'][1]['content']\n",
    "                assistant_msg = sample['messages'][2]['content']\n",
    "                \n",
    "                # 기업명 추출\n",
    "                company_name = user_msg.split('에 대한')[0].strip()\n",
    "                companies_analyzed.append(company_name)\n",
    "                \n",
    "                # 응답 길이\n",
    "                response_len = len(assistant_msg)\n",
    "                avg_lengths.append(response_len)\n",
    "                \n",
    "                print(f\"    📊 {company_name}: {response_len:,}자\")\n",
    "            \n",
    "            print(f\"  🏢 분석된 기업: {', '.join(companies_analyzed[:3])} 등\")\n",
    "            print(f\"  📏 평균 응답 길이: {sum(avg_lengths) // len(avg_lengths):,}자\")\n",
    "    \n",
    "    # 5. OpenAI 파인튜닝 호환성 검증\n",
    "    print(f\"\\n✅ OpenAI 파인튜닝 호환성:\")\n",
    "    print(f\"  ✅ JSONL 형식: 올바름\")\n",
    "    print(f\"  ✅ messages 구조: 표준 형식\")\n",
    "    print(f\"  ✅ 3-role 시스템: system, user, assistant\")\n",
    "    print(f\"  ✅ 일관된 템플릿: 투자 분석 형식\")\n",
    "    print(f\"  ✅ 한국어 인코딩: UTF-8\")\n",
    "    \n",
    "    # 분석 소요 시간\n",
    "    end_time = time.time()\n",
    "    analysis_time = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n⚡ 분석 완료!\")\n",
    "    print(f\"⏱️ 소요 시간: {analysis_time:.2f}초\")\n",
    "    print(f\"🚀 데이터셋 상태: 파인튜닝 준비 완료!\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 데이터셋 파일을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88580875",
   "metadata": {},
   "source": [
    "# 🚀 OpenAI 모델 파인튜닝 실행 가이드\n",
    "\n",
    "## 📋 **준비 완료 상태**\n",
    "✅ **파인튜닝 데이터셋**: `offline_investment_reports_dataset.jsonl` (96개 기업)  \n",
    "✅ **데이터 형식**: OpenAI 표준 messages 구조  \n",
    "✅ **파일 크기**: ~0.2MB (최적 크기)  \n",
    "✅ **품질 검증**: 일관된 템플릿 기반 고품질 데이터  \n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **파인튜닝 실행 방법**\n",
    "\n",
    "### **1단계: OpenAI API 키 설정**\n",
    "```bash\n",
    "# 환경변수로 설정 (Windows)\n",
    "set OPENAI_API_KEY=your-api-key-here\n",
    "\n",
    "# 또는 PowerShell에서\n",
    "$env:OPENAI_API_KEY=\"your-api-key-here\"\n",
    "```\n",
    "\n",
    "### **2단계: OpenAI CLI 설치 및 로그인**\n",
    "```bash\n",
    "# OpenAI Python 패키지 설치\n",
    "pip install openai\n",
    "\n",
    "# CLI로 로그인 (터미널에서)\n",
    "openai auth login\n",
    "```\n",
    "\n",
    "### **3단계: 파인튜닝 실행**\n",
    "```bash\n",
    "# 파인튜닝 시작 (GPT-3.5-turbo 권장)\n",
    "openai api fine_tuning.jobs.create \\\n",
    "  -t offline_investment_reports_dataset.jsonl \\\n",
    "  -m gpt-3.5-turbo \\\n",
    "  --suffix \"korean-investment-analyst\"\n",
    "```\n",
    "\n",
    "### **4단계: 진행 상황 모니터링**\n",
    "```bash\n",
    "# 파인튜닝 작업 목록 확인\n",
    "openai api fine_tuning.jobs.list\n",
    "\n",
    "# 특정 작업 상태 확인\n",
    "openai api fine_tuning.jobs.retrieve -i ft-job-xxxxx\n",
    "\n",
    "# 실시간 로그 확인\n",
    "openai api fine_tuning.jobs.follow -i ft-job-xxxxx\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ⏱️ **예상 소요 시간 및 비용**\n",
    "\n",
    "### **시간**\n",
    "- 📊 **데이터 업로드**: 1-2분\n",
    "- 🔄 **파인튜닝 실행**: 10-20분 (96개 데이터 기준)\n",
    "- ✅ **모델 배포**: 2-3분\n",
    "- **총 소요시간**: **약 15-25분**\n",
    "\n",
    "### **비용** (2024년 기준)\n",
    "- 🏷️ **GPT-3.5-turbo 파인튜닝**: $0.008/1K 토큰\n",
    "- 💰 **예상 비용**: 약 $5-15 (데이터 크기에 따라)\n",
    "- 📈 **사용 비용**: 파인튜닝된 모델 호출 시 추가 요금\n",
    "\n",
    "---\n",
    "\n",
    "## 🎁 **파인튜닝 완료 후 사용법**\n",
    "\n",
    "### **모델 호출 예시**\n",
    "```python\n",
    "import openai\n",
    "\n",
    "# 파인튜닝된 모델 사용\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"ft:gpt-3.5-turbo:your-org:korean-investment-analyst:xxxxx\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 전문 증권 애널리스트입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"삼성전자에 대한 투자 분석 리포트를 작성해주세요.\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 **파인튜닝 효과 예상**\n",
    "\n",
    "### **Before (일반 GPT)**\n",
    "- 일반적인 투자 조언\n",
    "- 구체적 데이터 부족\n",
    "- 일관성 없는 형식\n",
    "\n",
    "### **After (파인튜닝된 모델)**\n",
    "- ✅ **한국 기업 전문**: 96개 기업 학습 완료\n",
    "- ✅ **일관된 분석 형식**: 투자 분석 → 리스크 평가 → 투자 의견\n",
    "- ✅ **전문 용어**: 증권사 수준의 분석 언어\n",
    "- ✅ **구조화된 리포트**: 항상 동일한 고품질 형식\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 **고급 옵션**\n",
    "\n",
    "### **하이퍼파라미터 조정**\n",
    "```bash\n",
    "openai api fine_tuning.jobs.create \\\n",
    "  -t offline_investment_reports_dataset.jsonl \\\n",
    "  -m gpt-3.5-turbo \\\n",
    "  --suffix \"korean-investment-analyst\" \\\n",
    "  --n_epochs 3 \\\n",
    "  --batch_size 1 \\\n",
    "  --learning_rate_multiplier 0.1\n",
    "```\n",
    "\n",
    "### **검증 데이터셋 분리**\n",
    "```python\n",
    "# 데이터셋의 10%를 검증용으로 분리\n",
    "# training: 86개, validation: 10개\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 파인튜닝 준비 완료!\n",
      "  📄 데이터셋: offline_investment_reports_dataset.jsonl\n",
      "  📊 파일 크기: 200.4 KB\n",
      "\n",
      "🎯 실행 방법:\n",
      "  1. OPENAI_API_KEY 변수에 실제 API 키 입력\n",
      "  2. 아래 코드 블록 실행:\n",
      "\n",
      "# 1단계: 파일 업로드\n",
      "file_id = upload_training_file('offline_investment_reports_dataset.jsonl')\n",
      "\n",
      "# 2단계: 파인튜닝 시작\n",
      "job_id = start_fine_tuning(file_id)\n",
      "\n",
      "# 3단계: 상태 확인 (10-20분 후)\n",
      "status = check_fine_tuning_status(job_id)\n",
      "\n",
      "# 4단계: 완료된 모델 테스트\n",
      "# test_fine_tuned_model('ft:gpt-3.5-turbo:your-org:korean-investment-analyst:xxxxx')\n"
     ]
    }
   ],
   "source": [
    "# 🚀 OpenAI 파인튜닝 실행 (Python 코드)\n",
    "import openai\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "OPENAI_API_KEY = \"\"  # ◀◀◀ 실제 API 키로 교체 필요!\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def upload_training_file(file_path: str):\n",
    "    \"\"\"훈련 데이터 파일을 OpenAI에 업로드\"\"\"\n",
    "    print(f\"📤 파일 업로드 시작: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            response = openai.File.create(\n",
    "                file=f,\n",
    "                purpose='fine-tune'\n",
    "            )\n",
    "        \n",
    "        file_id = response.id\n",
    "        print(f\"✅ 파일 업로드 완료!\")\n",
    "        print(f\"  📄 파일 ID: {file_id}\")\n",
    "        print(f\"  📊 파일명: {response.filename}\")\n",
    "        print(f\"  📏 파일 크기: {response.bytes:,} bytes\")\n",
    "        \n",
    "        return file_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파일 업로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def start_fine_tuning(file_id: str, model: str = \"gpt-3.5-turbo\", suffix: str = \"korean-investment-analyst\"):\n",
    "    \"\"\"파인튜닝 작업 시작\"\"\"\n",
    "    print(f\"🔥 파인튜닝 시작...\")\n",
    "    print(f\"  🤖 베이스 모델: {model}\")\n",
    "    print(f\"  📂 훈련 파일 ID: {file_id}\")\n",
    "    print(f\"  🏷️ 모델 접미사: {suffix}\")\n",
    "    \n",
    "    try:\n",
    "        response = openai.FineTuningJob.create(\n",
    "            training_file=file_id,\n",
    "            model=model,\n",
    "            suffix=suffix,\n",
    "        )\n",
    "        \n",
    "        job_id = response.id\n",
    "        print(f\"✅ 파인튜닝 작업 생성 완료!\")\n",
    "        print(f\"  🆔 작업 ID: {job_id}\")\n",
    "        print(f\"  📊 상태: {response.status}\")\n",
    "        print(f\"  ⏰ 생성 시간: {datetime.fromtimestamp(response.created_at)}\")\n",
    "        \n",
    "        return job_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 파인튜닝 시작 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def check_fine_tuning_status(job_id: str):\n",
    "    \"\"\"파인튜닝 작업 상태 확인\"\"\"\n",
    "    try:\n",
    "        response = openai.FineTuningJob.retrieve(job_id)\n",
    "        \n",
    "        print(f\"📊 파인튜닝 상태 확인:\")\n",
    "        print(f\"  🆔 작업 ID: {response.id}\")\n",
    "        print(f\"  📊 상태: {response.status}\")\n",
    "        print(f\"  🤖 베이스 모델: {response.model}\")\n",
    "        \n",
    "        if response.status == \"succeeded\":\n",
    "            print(f\"  🎉 완료된 모델: {response.fine_tuned_model}\")\n",
    "        elif response.status == \"failed\":\n",
    "            print(f\"  ❌ 실패 사유: {response.error}\")\n",
    "        \n",
    "        if hasattr(response, 'trained_tokens') and response.trained_tokens:\n",
    "            print(f\"  🔢 훈련된 토큰: {response.trained_tokens:,}개\")\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 상태 확인 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_fine_tuned_model(model_name: str, test_prompt: str = \"삼성전자에 대한 투자 분석 리포트를 작성해주세요.\"):\n",
    "    \"\"\"파인튜닝된 모델 테스트\"\"\"\n",
    "    print(f\"🧪 파인튜닝된 모델 테스트:\")\n",
    "    print(f\"  🤖 모델: {model_name}\")\n",
    "    print(f\"  💬 테스트 프롬프트: {test_prompt}\")\n",
    "    \n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"당신은 전문 증권 애널리스트입니다. 주어진 기업 데이터를 바탕으로 투자 분석 리포트를 작성해주세요.\"},\n",
    "                {\"role\": \"user\", \"content\": test_prompt}\n",
    "            ],\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ 모델 응답:\")\n",
    "        print(\"─\" * 50)\n",
    "        print(response.choices[0].message.content)\n",
    "        print(\"─\" * 50)\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 테스트 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "# 실행 준비 상태 확인\n",
    "dataset_file = \"offline_investment_reports_dataset.jsonl\"\n",
    "\n",
    "if os.path.exists(dataset_file):\n",
    "    print(\"✅ 파인튜닝 준비 완료!\")\n",
    "    print(f\"  📄 데이터셋: {dataset_file}\")\n",
    "    print(f\"  📊 파일 크기: {os.path.getsize(dataset_file)/1024:.1f} KB\")\n",
    "    print()\n",
    "    print(\"🎯 실행 방법:\")\n",
    "    print(\"  1. OPENAI_API_KEY 변수에 실제 API 키 입력\")\n",
    "    print(\"  2. 아래 코드 블록 실행:\")\n",
    "    print()\n",
    "    print(\"# 1단계: 파일 업로드\")\n",
    "    print(\"file_id = upload_training_file('offline_investment_reports_dataset.jsonl')\")\n",
    "    print()\n",
    "    print(\"# 2단계: 파인튜닝 시작\")  \n",
    "    print(\"job_id = start_fine_tuning(file_id)\")\n",
    "    print()\n",
    "    print(\"# 3단계: 상태 확인 (10-20분 후)\")\n",
    "    print(\"status = check_fine_tuning_status(job_id)\")\n",
    "    print()\n",
    "    print(\"# 4단계: 완료된 모델 테스트\")\n",
    "    print(\"# test_fine_tuned_model('ft:gpt-3.5-turbo:your-org:korean-investment-analyst:xxxxx')\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ 데이터셋 파일을 찾을 수 없습니다.\")\n",
    "    print(\"먼저 파인튜닝 데이터셋을 생성하세요.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
