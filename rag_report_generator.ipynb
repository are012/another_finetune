{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5097bc8",
   "metadata": {},
   "source": [
    "# ğŸ“Š ì™„ì „ ì˜¤í”„ë¼ì¸ RAG ë¦¬í¬íŠ¸ ìƒì„±ê¸° (Pure Consumer)\n",
    "\n",
    "## ğŸ¯ ì‹œìŠ¤í…œ ê°œìš” \n",
    "ì´ ë…¸íŠ¸ë¶ì€ **100% ì˜¤í”„ë¼ì¸**ìœ¼ë¡œ ì‘ë™í•˜ëŠ” Consumerì…ë‹ˆë‹¤.\n",
    "ì™¸ë¶€ API í˜¸ì¶œ ì—†ì´ ë¡œì»¬ ë²¡í„° DBì—ì„œë§Œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ“‹ í•µì‹¬ íŠ¹ì§•\n",
    "- **ì™„ì „ ì˜¤í”„ë¼ì¸**: ì™¸ë¶€ API í˜¸ì¶œ ì „í˜€ ì—†ìŒ\n",
    "- **ìˆœìˆ˜ RAG**: ë¡œì»¬ ë²¡í„° DBì—ì„œë§Œ ì •ë³´ ê²€ìƒ‰\n",
    "- **í…œí”Œë¦¿ ê¸°ë°˜**: ê·œì¹™ ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "- **íŒŒì¸íŠœë‹ ì¹œí™”**: ì¼ê´€ëœ êµ¬ì¡°ì˜ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ ìƒì„±\n",
    "- **ë¹ ë¥¸ ì²˜ë¦¬**: API ëŒ€ê¸°ì‹œê°„ ì—†ìŒ\n",
    "\n",
    "### ğŸ”„ Producer-Consumer ë¶„ë¦¬\n",
    "- **Producer** (`pipeline_update.py`): ë°ì´í„° ìˆ˜ì§‘ + API í˜¸ì¶œ + ë²¡í„° DB ì €ì¥\n",
    "- **Consumer** (ì´ ë…¸íŠ¸ë¶): ë²¡í„° DB ê²€ìƒ‰ + í…œí”Œë¦¿ ê¸°ë°˜ ë¦¬í¬íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a143aafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (API ê´€ë ¨ ì œì™¸)\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# ë²¡í„° DB ê´€ë ¨ (ì„ë² ë”©ì€ ì €ì¥ëœ ê²ƒë§Œ ì‚¬ìš©)\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# ë…¸íŠ¸ë¶ ì¶œë ¥ìš©\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ (ìˆœìˆ˜ ì˜¤í”„ë¼ì¸ ëª¨ë“œ)\")\n",
    "print(\"ğŸš« ì™¸ë¶€ API ì˜ì¡´ì„± ì—†ìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0f733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ì˜¤í”„ë¼ì¸ ì„¤ì •\n",
    "DB_DIR = \"rag_db\"  # ë²¡í„° DB ë””ë ‰í† ë¦¬\n",
    "GOOGLE_API_KEY = \"AIzaSyBwcmK-DKRCI2r8xhHygSfu2GQ-oqK6t_4\"  # ì„ë² ë”© ë¡œë“œìš©ë§Œ\n",
    "\n",
    "print(\"âœ… ì˜¤í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ\")\n",
    "print(f\"ğŸ“ ë²¡í„° DB ë””ë ‰í† ë¦¬: {DB_DIR}\")\n",
    "\n",
    "# DB ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "if os.path.exists(DB_DIR):\n",
    "    print(\"âœ… ë²¡í„° DB ë°œê²¬\")\n",
    "else:\n",
    "    print(\"âŒ ë²¡í„° DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pipeline_update.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OfflineReportGenerator:\n",
    "    \"\"\"ì™„ì „ ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±ê¸° - ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ\"\"\"\n",
    "    \n",
    "    def __init__(self, db_dir: str = \"rag_db\", google_api_key: str = None):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™” - ì €ì¥ëœ ë²¡í„° DBë§Œ ë¡œë“œ\n",
    "        \n",
    "        Args:\n",
    "            db_dir (str): ë²¡í„° DB ë””ë ‰í† ë¦¬\n",
    "            google_api_key (str): ì„ë² ë”© ë¡œë“œìš© (ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± ì•ˆí•¨)\n",
    "        \"\"\"\n",
    "        self.db_dir = db_dir\n",
    "        \n",
    "        if not os.path.exists(db_dir):\n",
    "            raise FileNotFoundError(f\"ë²¡í„° DB ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {db_dir}\")\n",
    "        \n",
    "        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ê¸°ì¡´ ì„ë² ë”© ë¡œë“œìš©ë§Œ)\n",
    "        self.embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=google_api_key\n",
    "        )\n",
    "        \n",
    "        # ë²¡í„° DB ë¡œë“œ (ì €ì¥ëœ ë°ì´í„°ë§Œ ì‚¬ìš©)\n",
    "        self.vectorstore = Chroma(\n",
    "            persist_directory=db_dir,\n",
    "            embedding_function=self.embeddings\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… ì˜¤í”„ë¼ì¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "        print(f\"ğŸ“Š ë²¡í„° DB ì»¬ë ‰ì…˜ ìˆ˜: {self.vectorstore._collection.count()}\")\n",
    "    \n",
    "    def get_available_companies(self) -> List[str]:\n",
    "        \"\"\"DBì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ì—… ëª©ë¡ ì¡°íšŒ\"\"\"\n",
    "        try:\n",
    "            results = self.vectorstore.get()\n",
    "            companies = set()\n",
    "            \n",
    "            for metadata in results['metadatas']:\n",
    "                if 'company' in metadata:\n",
    "                    companies.add(metadata['company'])\n",
    "            \n",
    "            return sorted(list(companies))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê¸°ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def search_company_data(self, company_name: str, k: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"íŠ¹ì • ê¸°ì—…ì˜ ëª¨ë“  ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰ ë° ë¶„ë¥˜\"\"\"\n",
    "        try:\n",
    "            # ê¸°ì—…ëª…ìœ¼ë¡œ ê²€ìƒ‰\n",
    "            results = self.vectorstore.similarity_search(\n",
    "                query=f\"{company_name} ë‰´ìŠ¤ ê³µì‹œ ë¶„ì„\",\n",
    "                k=k,\n",
    "                filter={\"company\": company_name}\n",
    "            )\n",
    "            \n",
    "            # ë°ì´í„° íƒ€ì…ë³„ë¡œ ë¶„ë¥˜\n",
    "            classified_data = {\n",
    "                \"news\": [],\n",
    "                \"disclosures\": [],\n",
    "                \"other\": [],\n",
    "                \"total_count\": len(results)\n",
    "            }\n",
    "            \n",
    "            for doc in results:\n",
    "                source = doc.metadata.get('source', 'other')\n",
    "                if source == 'news':\n",
    "                    classified_data[\"news\"].append(doc)\n",
    "                elif source == 'disclosure':\n",
    "                    classified_data[\"disclosures\"].append(doc)\n",
    "                else:\n",
    "                    classified_data[\"other\"].append(doc)\n",
    "            \n",
    "            return classified_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ {company_name} ë°ì´í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "            return {\"news\": [], \"disclosures\": [], \"other\": [], \"total_count\": 0}\n",
    "    \n",
    "    def extract_key_insights(self, classified_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"ê²€ìƒ‰ëœ ë°ì´í„°ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ (ê·œì¹™ ê¸°ë°˜)\"\"\"\n",
    "        insights = {\n",
    "            \"positive_signals\": [],\n",
    "            \"negative_signals\": [],\n",
    "            \"key_events\": [],\n",
    "            \"disclosure_priorities\": defaultdict(int),\n",
    "            \"recent_news_count\": 0,\n",
    "            \"disclosure_count\": 0\n",
    "        }\n",
    "        \n",
    "        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì •ì˜\n",
    "        positive_keywords = ['ì¦ê°€', 'ìƒìŠ¹', 'ì„±ì¥', 'í™•ëŒ€', 'íˆ¬ì', 'ê°œë°œ', 'í˜‘ë ¥', 'ê³„ì•½', 'ìˆ˜ì£¼', 'ë§¤ì¶œ']\n",
    "        negative_keywords = ['ê°ì†Œ', 'í•˜ë½', 'ì¶•ì†Œ', 'ì†ì‹¤', 'ë¦¬ìŠ¤í¬', 'ìš°ë ¤', 'ì·¨ì†Œ', 'ì§€ì—°', 'ë¬¸ì œ']\n",
    "        \n",
    "        # ë‰´ìŠ¤ ë¶„ì„\n",
    "        for news_doc in classified_data[\"news\"]:\n",
    "            content = news_doc.page_content.lower()\n",
    "            \n",
    "            # ê¸ì •ì  ì‹ í˜¸ íƒì§€\n",
    "            for keyword in positive_keywords:\n",
    "                if keyword in content:\n",
    "                    insights[\"positive_signals\"].append({\n",
    "                        \"keyword\": keyword,\n",
    "                        \"source\": \"ë‰´ìŠ¤\",\n",
    "                        \"title\": news_doc.metadata.get('title', '')[:50]\n",
    "                    })\n",
    "            \n",
    "            # ë¶€ì •ì  ì‹ í˜¸ íƒì§€\n",
    "            for keyword in negative_keywords:\n",
    "                if keyword in content:\n",
    "                    insights[\"negative_signals\"].append({\n",
    "                        \"keyword\": keyword,\n",
    "                        \"source\": \"ë‰´ìŠ¤\",\n",
    "                        \"title\": news_doc.metadata.get('title', '')[:50]\n",
    "                    })\n",
    "        \n",
    "        # ê³µì‹œ ë¶„ì„\n",
    "        for disclosure_doc in classified_data[\"disclosures\"]:\n",
    "            priority = disclosure_doc.metadata.get('priority', 'unknown')\n",
    "            insights[\"disclosure_priorities\"][priority] += 1\n",
    "            \n",
    "            # ì£¼ìš” ê³µì‹œ ì´ë²¤íŠ¸ ì¶”ì¶œ\n",
    "            report_name = disclosure_doc.metadata.get('report_name', '')\n",
    "            if any(keyword in report_name for keyword in ['ë¶„ê¸°ë³´ê³ ì„œ', 'ì‚¬ì—…ë³´ê³ ì„œ', 'ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ']):\n",
    "                insights[\"key_events\"].append({\n",
    "                    \"type\": \"ê³µì‹œ\",\n",
    "                    \"event\": report_name,\n",
    "                    \"priority\": priority\n",
    "                })\n",
    "        \n",
    "        insights[\"recent_news_count\"] = len(classified_data[\"news\"])\n",
    "        insights[\"disclosure_count\"] = len(classified_data[\"disclosures\"])\n",
    "        \n",
    "        return insights\n",
    "    \n",
    "    def generate_offline_report(self, company_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"ì™„ì „ ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„± (í…œí”Œë¦¿ ê¸°ë°˜)\"\"\"\n",
    "        print(f\"ğŸ“Š {company_name} ì˜¤í”„ë¼ì¸ ë¶„ì„ ì‹œì‘...\")\n",
    "        \n",
    "        # 1. ë°ì´í„° ê²€ìƒ‰\n",
    "        classified_data = self.search_company_data(company_name)\n",
    "        \n",
    "        if classified_data[\"total_count\"] == 0:\n",
    "            return {\n",
    "                \"company\": company_name,\n",
    "                \"status\": \"ë°ì´í„° ì—†ìŒ\",\n",
    "                \"message\": \"í•´ë‹¹ ê¸°ì—…ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "            }\n",
    "        \n",
    "        # 2. ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ\n",
    "        insights = self.extract_key_insights(classified_data)\n",
    "        \n",
    "        # 3. ë¦¬í¬íŠ¸ ìƒì„±\n",
    "        report = {\n",
    "            \"company\": company_name,\n",
    "            \"generation_date\": datetime.now().isoformat(),\n",
    "            \"data_summary\": {\n",
    "                \"total_documents\": classified_data[\"total_count\"],\n",
    "                \"news_articles\": len(classified_data[\"news\"]),\n",
    "                \"disclosures\": len(classified_data[\"disclosures\"])\n",
    "            },\n",
    "            \"investment_analysis\": self._create_investment_analysis(insights),\n",
    "            \"risk_assessment\": self._create_risk_assessment(insights),\n",
    "            \"recommendation\": self._create_recommendation(insights),\n",
    "            \"key_data_points\": self._extract_key_data_points(classified_data),\n",
    "            \"status\": \"ì™„ë£Œ\"\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… {company_name} ì˜¤í”„ë¼ì¸ ë¶„ì„ ì™„ë£Œ\")\n",
    "        return report\n",
    "    \n",
    "    def _create_investment_analysis(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"íˆ¬ì ë¶„ì„ ì„¹ì…˜ ìƒì„±\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        news_count = insights[\"recent_news_count\"]\n",
    "        \n",
    "        analysis = f\"\"\"## ğŸ“ˆ íˆ¬ì ë¶„ì„\n",
    "\n",
    "### ë°ì´í„° ê¸°ë°˜ í˜„í™©\n",
    "- ìµœê·¼ ë‰´ìŠ¤ ë¶„ì„: {news_count}ê±´\n",
    "- ê³µì‹œ ì •ë³´ ë¶„ì„: {insights['disclosure_count']}ê±´\n",
    "- ê¸ì •ì  ì‹ í˜¸: {positive_count}ê°œ\n",
    "- ë¶€ì •ì  ì‹ í˜¸: {negative_count}ê°œ\n",
    "\n",
    "### ì£¼ìš” ê¸ì • ìš”ì¸\n",
    "\"\"\"\n",
    "        \n",
    "        if insights[\"positive_signals\"]:\n",
    "            for i, signal in enumerate(insights[\"positive_signals\"][:5], 1):\n",
    "                analysis += f\"- {signal['keyword']} ê´€ë ¨ ì´ìŠˆ ({signal['source']}): {signal['title']}...\\n\"\n",
    "        else:\n",
    "            analysis += \"- í˜„ì¬ íŠ¹ë³„í•œ ê¸ì •ì  ì‹ í˜¸ëŠ” ë°œê²¬ë˜ì§€ ì•ŠìŒ\\n\"\n",
    "        \n",
    "        analysis += \"\\n### ì£¼ìš” ìš°ë ¤ ìš”ì¸\\n\"\n",
    "        if insights[\"negative_signals\"]:\n",
    "            for i, signal in enumerate(insights[\"negative_signals\"][:5], 1):\n",
    "                analysis += f\"- {signal['keyword']} ê´€ë ¨ ì´ìŠˆ ({signal['source']}): {signal['title']}...\\n\"\n",
    "        else:\n",
    "            analysis += \"- í˜„ì¬ íŠ¹ë³„í•œ ìš°ë ¤ ìš”ì¸ì€ ë°œê²¬ë˜ì§€ ì•ŠìŒ\\n\"\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _create_risk_assessment(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"ë¦¬ìŠ¤í¬ í‰ê°€ ì„¹ì…˜ ìƒì„±\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ë¦¬ìŠ¤í¬ ì ìˆ˜ ê³„ì‚°\n",
    "        if negative_count == 0:\n",
    "            risk_level = \"ë‚®ìŒ\"\n",
    "        elif negative_count <= positive_count:\n",
    "            risk_level = \"ë³´í†µ\"\n",
    "        else:\n",
    "            risk_level = \"ë†’ìŒ\"\n",
    "        \n",
    "        assessment = f\"\"\"## âš ï¸ ë¦¬ìŠ¤í¬ í‰ê°€\n",
    "\n",
    "### ë¦¬ìŠ¤í¬ ìˆ˜ì¤€: {risk_level}\n",
    "\n",
    "### í‰ê°€ ê·¼ê±°\n",
    "- ê¸ì •ì  ì‹ í˜¸ vs ë¶€ì •ì  ì‹ í˜¸: {positive_count} vs {negative_count}\n",
    "- ê³µì‹œ ì •ë³´ í™œìš©ë„: {insights['disclosure_count']}ê±´ ë¶„ì„\n",
    "\n",
    "### ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸\n",
    "\"\"\"\n",
    "        \n",
    "        if insights[\"negative_signals\"]:\n",
    "            risk_keywords = Counter([signal['keyword'] for signal in insights[\"negative_signals\"]])\n",
    "            for keyword, count in risk_keywords.most_common(3):\n",
    "                assessment += f\"- {keyword} ê´€ë ¨ ì´ìŠˆ: {count}ê±´ í™•ì¸\\n\"\n",
    "        else:\n",
    "            assessment += \"- í˜„ì¬ ë°ì´í„°ì—ì„œ íŠ¹ë³„í•œ ë¦¬ìŠ¤í¬ ìš”ì¸ì€ í™•ì¸ë˜ì§€ ì•ŠìŒ\\n\"\n",
    "        \n",
    "        return assessment\n",
    "    \n",
    "    def _create_recommendation(self, insights: Dict[str, Any]) -> str:\n",
    "        \"\"\"íˆ¬ì ì˜ê²¬ ì„¹ì…˜ ìƒì„±\"\"\"\n",
    "        positive_count = len(insights[\"positive_signals\"])\n",
    "        negative_count = len(insights[\"negative_signals\"])\n",
    "        \n",
    "        # ê°„ë‹¨í•œ ì¶”ì²œ ë¡œì§\n",
    "        if positive_count > negative_count * 1.5:\n",
    "            recommendation = \"ë§¤ìˆ˜\"\n",
    "            rationale = \"ê¸ì •ì  ì‹ í˜¸ê°€ ë¶€ì •ì  ì‹ í˜¸ë¥¼ í¬ê²Œ ìƒíšŒí•˜ì—¬ íˆ¬ì ë§¤ë ¥ë„ê°€ ë†’ìŒ\"\n",
    "        elif negative_count > positive_count * 1.5:\n",
    "            recommendation = \"ë§¤ë„\"\n",
    "            rationale = \"ë¶€ì •ì  ì‹ í˜¸ê°€ ê¸ì •ì  ì‹ í˜¸ë¥¼ í¬ê²Œ ìƒíšŒí•˜ì—¬ íˆ¬ì ìœ„í—˜ë„ê°€ ë†’ìŒ\"\n",
    "        else:\n",
    "            recommendation = \"ë³´ìœ \"\n",
    "            rationale = \"ê¸ì •ì /ë¶€ì •ì  ì‹ í˜¸ê°€ ê· í˜•ì„ ì´ë£¨ì–´ ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”\"\n",
    "        \n",
    "        return f\"\"\"## ğŸ’¡ íˆ¬ì ì˜ê²¬\n",
    "\n",
    "### ì¶”ì²œ ì˜ê²¬: {recommendation}\n",
    "\n",
    "### ê·¼ê±°\n",
    "{rationale}\n",
    "\n",
    "### ë°ì´í„° ê¸°ë°˜ ì ìˆ˜\n",
    "- ê¸ì • ì§€ìˆ˜: {positive_count}\n",
    "- ìœ„í—˜ ì§€ìˆ˜: {negative_count}\n",
    "- ì •ë³´ í’ë¶€ë„: {insights['recent_news_count'] + insights['disclosure_count']}ì \n",
    "\n",
    "### ì£¼ì˜ì‚¬í•­\n",
    "ë³¸ ë¶„ì„ì€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë° ê³µì‹œ ë°ì´í„°ì˜ í‚¤ì›Œë“œ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, \n",
    "ì‹¤ì œ íˆ¬ì ê²°ì • ì‹œì—ëŠ” ì¶”ê°€ì ì¸ ì¬ë¬´ ë¶„ì„ ë° ì „ë¬¸ê°€ ì˜ê²¬ì„ ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\n",
    "\"\"\"\n",
    "    \n",
    "    def _extract_key_data_points(self, classified_data: Dict[str, Any]) -> List[Dict[str, str]]:\n",
    "        \"\"\"í•µì‹¬ ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "        key_points = []\n",
    "        \n",
    "        # ìµœì‹  ë‰´ìŠ¤ ìƒìœ„ 3ê°œ\n",
    "        for doc in classified_data[\"news\"][:3]:\n",
    "            key_points.append({\n",
    "                \"type\": \"ë‰´ìŠ¤\",\n",
    "                \"title\": doc.metadata.get('title', 'ì œëª© ì—†ìŒ'),\n",
    "                \"date\": doc.metadata.get('collection_date', 'ë‚ ì§œ ì—†ìŒ'),\n",
    "                \"summary\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        # ì¤‘ìš” ê³µì‹œ ìƒìœ„ 3ê°œ\n",
    "        for doc in classified_data[\"disclosures\"][:3]:\n",
    "            key_points.append({\n",
    "                \"type\": \"ê³µì‹œ\",\n",
    "                \"title\": doc.metadata.get('report_name', 'ê³µì‹œëª… ì—†ìŒ'),\n",
    "                \"priority\": doc.metadata.get('priority', 'ì¼ë°˜'),\n",
    "                \"date\": doc.metadata.get('collection_date', 'ë‚ ì§œ ì—†ìŒ'),\n",
    "                \"summary\": doc.page_content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        return key_points\n",
    "\n",
    "print(\"âœ… OfflineReportGenerator í´ë˜ìŠ¤ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be37c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ ì˜¤í”„ë¼ì¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "try:\n",
    "    offline_rag = OfflineReportGenerator(\n",
    "        db_dir=DB_DIR,\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "    \n",
    "    # ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ì—… ëª©ë¡ ì¡°íšŒ\n",
    "    available_companies = offline_rag.get_available_companies()\n",
    "    print(f\"\\nğŸ“‹ ë¶„ì„ ê°€ëŠ¥í•œ ê¸°ì—… ëª©ë¡ ({len(available_companies)}ê°œ):\")\n",
    "    for i, company in enumerate(available_companies, 1):\n",
    "        print(f\"  {i}. {company}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì˜¤í”„ë¼ì¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   pipeline_update.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.\")\n",
    "    offline_rag = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114d953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š ë‹¨ì¼ ê¸°ì—… ë¶„ì„ ì‹¤í–‰\n",
    "if offline_rag and available_companies:\n",
    "    # ì²« ë²ˆì§¸ ê¸°ì—…ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ (ì›í•˜ëŠ” ê¸°ì—…ëª…ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "    test_company = available_companies[0]\n",
    "    print(f\"ğŸ¯ {test_company} ë¶„ì„ ì‹œì‘...\\n\")\n",
    "    \n",
    "    # ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "    report = offline_rag.generate_offline_report(test_company)\n",
    "    \n",
    "    if report[\"status\"] == \"ì™„ë£Œ\":\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"ğŸ“ˆ {report['company']} íˆ¬ì ë¶„ì„ ë¦¬í¬íŠ¸ (ì˜¤í”„ë¼ì¸)\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"ìƒì„± ì‹œê°„: {report['generation_date']}\")\n",
    "        print(f\"ë¶„ì„ ë°ì´í„°: ì´ {report['data_summary']['total_documents']}ê±´\")\n",
    "        print(f\"  - ë‰´ìŠ¤: {report['data_summary']['news_articles']}ê±´\")\n",
    "        print(f\"  - ê³µì‹œ: {report['data_summary']['disclosures']}ê±´\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # íˆ¬ì ë¶„ì„ ì¶œë ¥\n",
    "        display(Markdown(report['investment_analysis']))\n",
    "        \n",
    "        # ë¦¬ìŠ¤í¬ í‰ê°€ ì¶œë ¥\n",
    "        display(Markdown(report['risk_assessment']))\n",
    "        \n",
    "        # íˆ¬ì ì˜ê²¬ ì¶œë ¥\n",
    "        display(Markdown(report['recommendation']))\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"ğŸ“‹ í•µì‹¬ ë°ì´í„° í¬ì¸íŠ¸\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, point in enumerate(report['key_data_points'][:5], 1):\n",
    "            print(f\"\\n[{i}] {point['type']}: {point['title']}\")\n",
    "            if 'priority' in point:\n",
    "                print(f\"    ìš°ì„ ìˆœìœ„: {point['priority']}\")\n",
    "            print(f\"    ìš”ì•½: {point['summary']}\")\n",
    "            print(f\"    ìˆ˜ì§‘ì¼: {point['date'][:10]}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {report.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\")\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ ì‹œìŠ¤í…œì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¶„ì„ ê°€ëŠ¥í•œ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281fec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜\n",
    "def create_finetuning_dataset(offline_rag_system, companies: List[str], output_file: str = \"investment_reports_dataset.jsonl\"):\n",
    "    \"\"\"ëª¨ë“  ê¸°ì—…ì— ëŒ€í•´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•˜ê³  íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ìœ¼ë¡œ ì €ì¥\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“¦ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘: {len(companies)}ê°œ ê¸°ì—…\")\n",
    "    \n",
    "    dataset = []\n",
    "    successful_reports = 0\n",
    "    \n",
    "    for i, company in enumerate(companies, 1):\n",
    "        print(f\"\\n[{i}/{len(companies)}] {company} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        try:\n",
    "            # ì˜¤í”„ë¼ì¸ ë¦¬í¬íŠ¸ ìƒì„±\n",
    "            report = offline_rag_system.generate_offline_report(company)\n",
    "            \n",
    "            if report[\"status\"] == \"ì™„ë£Œ\":\n",
    "                # íŒŒì¸íŠœë‹ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "                training_example = {\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": \"ë‹¹ì‹ ì€ ì „ë¬¸ ì¦ê¶Œ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ê¸°ì—… ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": f\"{company}ì— ëŒ€í•œ íˆ¬ì ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ìµœê·¼ ë‰´ìŠ¤ {report['data_summary']['news_articles']}ê±´ê³¼ ê³µì‹œ {report['data_summary']['disclosures']}ê±´ì˜ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"assistant\",\n",
    "                            \"content\": f\"\"\"{report['investment_analysis']}\n",
    "\n",
    "{report['risk_assessment']}\n",
    "\n",
    "{report['recommendation']}\n",
    "\n",
    "## ğŸ“‹ ë¶„ì„ ê·¼ê±° ë°ì´í„°\n",
    "- ì´ ë¶„ì„ ë¬¸ì„œ: {report['data_summary']['total_documents']}ê±´\n",
    "- ë‰´ìŠ¤ ë¶„ì„: {report['data_summary']['news_articles']}ê±´\n",
    "- ê³µì‹œ ë¶„ì„: {report['data_summary']['disclosures']}ê±´\n",
    "- ë¶„ì„ ì™„ë£Œ ì‹œê°„: {report['generation_date']}\n",
    "\"\"\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "                \n",
    "                dataset.append(training_example)\n",
    "                successful_reports += 1\n",
    "                print(f\"  âœ… {company} ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  âš ï¸ {company} ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {report.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {company} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    # JSONL íŒŒì¼ë¡œ ì €ì¥\n",
    "    if dataset:\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            for example in dataset:\n",
    "                f.write(json.dumps(example, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        print(f\"\\nğŸ‰ íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“„ íŒŒì¼: {output_file}\")\n",
    "        print(f\"ğŸ“Š ì´ ë°ì´í„°: {len(dataset)}ê°œ (ì„±ê³µë¥ : {successful_reports}/{len(companies)})\")\n",
    "        \n",
    "        return output_file\n",
    "    else:\n",
    "        print(\"âŒ ìƒì„±ëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ… íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f96f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ ì „ì²´ ê¸°ì—… ë°°ì¹˜ ì²˜ë¦¬ ë° íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„±\n",
    "if offline_rag and available_companies:\n",
    "    print(\"ğŸ”„ ì „ì²´ ê¸°ì—… ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...\")\n",
    "    \n",
    "    # íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset_file = create_finetuning_dataset(\n",
    "        offline_rag_system=offline_rag,\n",
    "        companies=available_companies,\n",
    "        output_file=\"offline_investment_reports_dataset.jsonl\"\n",
    "    )\n",
    "    \n",
    "    if dataset_file:\n",
    "        print(f\"\\nâœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
    "        print(f\"ğŸ“ ìƒì„±ëœ íŒŒì¼: {dataset_file}\")\n",
    "        \n",
    "        # íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "        if os.path.exists(dataset_file):\n",
    "            file_size = os.path.getsize(dataset_file) / 1024 / 1024  # MB\n",
    "            print(f\"ğŸ“Š íŒŒì¼ í¬ê¸°: {file_size:.2f} MB\")\n",
    "            \n",
    "            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸\n",
    "            with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "                first_line = f.readline()\n",
    "                sample_data = json.loads(first_line)\n",
    "                \n",
    "            print(f\"\\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡°:\")\n",
    "            print(f\"  - ë©”ì‹œì§€ ìˆ˜: {len(sample_data['messages'])}\")\n",
    "            print(f\"  - ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(sample_data['messages'][0]['content'])}ì\")\n",
    "            print(f\"  - ì‚¬ìš©ì ì§ˆë¬¸ ê¸¸ì´: {len(sample_data['messages'][1]['content'])}ì\")\n",
    "            print(f\"  - ì–´ì‹œìŠ¤í„´íŠ¸ ë‹µë³€ ê¸¸ì´: {len(sample_data['messages'][2]['content'])}ì\")\n",
    "            \n",
    "else:\n",
    "    print(\"âŒ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be151286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µê³„\n",
    "if offline_rag:\n",
    "    print(\"ğŸ“Š ì˜¤í”„ë¼ì¸ RAG ì‹œìŠ¤í…œ ì„±ëŠ¥ í†µê³„\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # ë²¡í„° DB í†µê³„\n",
    "    total_docs = offline_rag.vectorstore._collection.count()\n",
    "    print(f\"ğŸ“„ ì´ ë²¡í„° ë¬¸ì„œ ìˆ˜: {total_docs:,}ê°œ\")\n",
    "    \n",
    "    # ê¸°ì—…ë³„ ë°ì´í„° ë¶„í¬\n",
    "    company_distribution = {}\n",
    "    for company in available_companies:\n",
    "        data = offline_rag.search_company_data(company, k=100)\n",
    "        company_distribution[company] = data['total_count']\n",
    "    \n",
    "    print(f\"\\nğŸ¢ ê¸°ì—…ë³„ ë°ì´í„° ë¶„í¬:\")\n",
    "    for company, count in sorted(company_distribution.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {company}: {count:,}ê°œ ë¬¸ì„œ\")\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ íŠ¹ì§•\n",
    "    print(f\"\\nğŸ¯ ì‹œìŠ¤í…œ íŠ¹ì§•:\")\n",
    "    print(f\"  âœ… 100% ì˜¤í”„ë¼ì¸ ë™ì‘\")\n",
    "    print(f\"  âœ… ì™¸ë¶€ API í˜¸ì¶œ ì—†ìŒ\")\n",
    "    print(f\"  âœ… ì‹¤ì‹œê°„ ë¦¬í¬íŠ¸ ìƒì„±\")\n",
    "    print(f\"  âœ… íŒŒì¸íŠœë‹ ë°ì´í„°ì…‹ ìë™ ìƒì„±\")\n",
    "    print(f\"  âœ… í…œí”Œë¦¿ ê¸°ë°˜ ì¼ê´€ì„±\")\n",
    "    \n",
    "    print(f\"\\nğŸš€ Producer-Consumer ì™„ì „ ë¶„ë¦¬ ë‹¬ì„±!\")\n",
    "    print(f\"  ğŸ“¥ Producer: pipeline_update.py (API í˜¸ì¶œ + ë°ì´í„° ìˆ˜ì§‘)\")\n",
    "    print(f\"  ğŸ“¤ Consumer: rag_report_generator.ipynb (ìˆœìˆ˜ ì˜¤í”„ë¼ì¸ ë¶„ì„)\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ ì‹œìŠ¤í…œ í†µê³„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
